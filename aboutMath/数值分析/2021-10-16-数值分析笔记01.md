# 误差

## 若干定义

1. 误差的来源：
    1. 模型误差：由计算方法或计算模型的不严格而引入
    2. 观测误差：生产实践中由于测量精度的不足而引入的测量误差
    3. **截断误差**：对于将连续问题/无限问题近似为离散问题/有限问题而进行的“差分”或“舍去高阶无穷小项”等操作而引入的计算误差
    4. 舍入误差：对于利用计算机求解的问题，计算机最长字长限制了我们的有效数字位数，故我们进行舍入而引入的误差
2. **绝对误差和绝对误差限**：约定物理量的真实值记为$x$，其观测值记为$x^*$. 那么，我们将值$x - x^*$称为绝对误差$e(x^*)$，将绝对误差之绝对值的**上界**称为绝对误差限$\epsilon(x^*)$(实际上在授课的PPT中，绝对误差限没有被表达为一个对映关系，但为了数学上的对称，将其修改为了函数形式). 
   
   这样我们就可以将真实值表达为：

$$x = x^* \pm \epsilon(x^*)$$

3. **相对误差和相对误差限**：约定物理量的真实值记为$x$，其观测值记为$x^*$. 那么，我们将值$\frac{e(x^*)}{x}$称为相对误差$e_r(x^*)$. 实际上，绝大多数情况下，物理量的真实值$x$是不可知的，因此我们可以将相对误差近似地计算为$\frac{e(x^*)}{x^*}$. 同理，相对误差之绝对值的**上界**称为相对误差限$\epsilon_r(x^*)$

给出定义之后，我们可以发现绝对误差限和相对误差限之间的关系：

$$\epsilon_r = \frac{\epsilon}{\vert x^* \vert} \tag{1.1}$$


*请注意，在上述的描述中使用了“上界”的措辞，这表示了误差限可以远远大于相应误差. 作为基本的数学分析内容，**上界**的概念应与**上确界**区分.* **请务必将此概念彻底理解，以便进行下述的数学推导.**


## 有效数字

### 定义

**兹定义**：若$\epsilon(x^*)$ 的绝对误差限可以是 $x^*$ 某一位上数字的半个单位，且该位直到 $x^*$ 的首个非零数字一共有 $n$ 位数字，则称近似值 $x^*$ 有 $n$ 位有效数字. 

分析地描述：设置测量值$x^*$被$c$进制表示，我们将其从小数点后的第$m$位称为“最后一位有效数字”**当且仅当**$m$满足如下条件：

$$\vert x - x^* \vert \leqslant \frac{1}{2} \times c^{-m} \tag{1.2}$$

此时$x^*$具有$n$位有效数字**当且仅当**其首个非零数字到小数点之后第$m$位数字中共有$n$个数字.

对于常用的十进制情况，我们将式(1.2)转写为

$$\vert x - x^* \vert \leqslant \frac{1}{2} \times 10^{-m} \tag{1.2'}$$

### 另一等价定义

将以$c$进制表示的**测量值**$x^*$按照数位转写为如下的标准形式：

$$x^* = \pm\overline{0.a_1a_2a_3...a_n...} \times c^m\ (a_1 \neq 0) \tag{1.3}$$

称$x^*$具有$n$位有效数字当且仅当：

$$\vert x - x^*\vert \leqslant \frac{1}{2} \times c^{m - n} \tag{1.4}$$

容易证明，上述两种定义是等价的.

### 有效数字位数与首位数字的关系

**定理1.1-1**. 若以$c$进制表示的估计值$x^*$有$n$位有效数字，则

$$\epsilon_r(x^*) = \frac{1}{2a_1}\times c^{-n+1}\tag{1.5}$$

可以成立.(注：相对误差限是一个任意大的上界，因此所谓"可以成立"是指其相对误差的上确界不大于式1.5的右值)

**定理1.1-1，证明**.

由式1.1，

$$\epsilon_r = \frac{\epsilon}{\vert x^* \vert} \leqslant \frac{1/2\times c^{m - n}}{\vert x^* \vert} = \frac{1/2\times c^{m - n}}{\overline{0.a_1a_2a_3...a_n...}\times c^m} \leqslant \frac{1/2\times c^{m - n}}{\overline{0.a_1000...}\times c^m} \leqslant \frac{1}{2a_1}\times c^{-n+1}$$ 

$\blacksquare$

**定理1.1-2**

若以$c$进制表示的估计值$x^*$的相对误差限可以取到：

$$\epsilon_r(x^*) = \frac{1}{2(a_1+1)}\times c^{-n+1}\tag{1.6}$$

那么$x^*$至少具有$n$位有效数字

**定理1.1-2，证明**

首先注意，上述的"可以取到"意味着

$$\epsilon_r(x^*) \leqslant \frac{1}{2(a_1+1)}\times c^{-n+1}\tag{1.6'}$$

由式1.1，

$$\epsilon = \epsilon_r \vert x^* \vert \leqslant (\frac{1}{2(a_1+1)}\times c^{-n+1}) \vert x^* \vert \tag{1.7}$$

注意到

$$\vert x^* \vert = \overline{0.a_1a_2a_3...a_n...} \times c^m\ \leqslant (a_1+1) \times c^{m-1} \tag{1.8}$$

将式1.8代入至式1.7中，解出

$$\epsilon \leqslant \frac{1}{2} \times c^{-n+m} \tag{1.4}$$

$\blacksquare$

在课间听到了类似于"为什么分母上一个是$a_1$一个是$(a_1+1)$呢？"的问题，其实这是一个初等数学内容：我们知道，对于正分数，分母缩小分数则变大，分母变大分数则变小，对于两个定理的证明过程，我们均需保留一个不大于号$\leqslant$，且我们都做了将$x^*$的小数形式"截断"的操作，因此为保持推导过程中的不等号方向不变(否则推导将难以收敛)，我们就将其近似为了不同的形式.

### 四舍五入近似法，有效数位的简单理解

先来看一个例子：

**例1.1** 以$x^*_1 = 3.1415$和$x^*_2 = 3.1416$近似$\pi = 3.141592653..$，试分别计算其有效数字.

**解** 由$\pi - x^*_1 = 0.00009265...$得知：

$$1/2 \times 10^{-4} \leqslant \vert \pi - x^*_1 \vert \leqslant 1/2 \times 10^{-3}$$

故$x^*_1 = 3.1415$具有4位有效数字.

同理，

$$1/2 \times 10^{-5} \leqslant \vert \pi - x^*_2 \vert \leqslant 1/2 \times 10^{-4}$$

故$x^*_2 = 3.1416$具有5位有效数字.$\blacksquare$

直观地来看，上例中$x_1$进行了错误的四舍五入近似操作，而$x_2$进行了正确的四舍五入近似操作，这似乎导致了$x_2$在上述的定义中具有更多的有效数字. 这种看法不失为一种对于有效数字定义的理解. 结合我们在中学物理中学习到的"估读位数也算有效数字"的结论，以科学哲学的眼光来看，我们可以这样理解有效数字的定义：在实际生产实践的测量操作中，由于人眼的某种固有属性或者人脑的某种固有价值，我们在判断"测量值是否超过了最小刻度的1/2"有着较强的能力. 因此在数学中，我们将上述的价值从生产中抽离出来，将"是否准确估读至1/2"理性衍生为"误差是否大于某一位上数字的半个单位"，作为了有效数字的判断依据.

## 误差的传播

兹定义误差传递模式：对于一个对映关系$y = f(X)$，$y$的误差表示为：

$$e(y^*) = y - y^* = f(X) - f(X^*),\ X\in	\mathbb{R^n}\tag{1.9}$$

若$f$在点$X^*$附近可微，且我们假定$X\approx X^*$，即可提出误差传递的近似计算方式：

$$e(y^*) = f(X) - f(X^*) \approx df(X^*)\tag{1.10-1}$$

由中值定理：

$$e(y^*) \approx f'(\xi)(X - X^*),\ \xi \in (X, X^*)\tag{1.10-2}$$

我们已经假定$X\approx X^*$，因此：

$$e(y^*) \approx f'(X^*)e(X^*)\tag{1.10-3}$$

若将$n$维向量$X$展开为实数形式，则

$$e(y^*) = \sum_{i = 1}^n\frac{\partial f(x_1^*, x_2^*, ..., x_n^*)}{\partial x_i}e(x_i)\tag{1.10}$$

同理，我们推导相对误差的传递：

$$e_r(y^*) = \frac{e(y^*)}{y^*}\approx \sum_{i = 1}^n\frac{\partial f(x_1^*, x_2^*, ..., x_n^*)}{\partial x_i}\frac{e(x_i)}{y^*} \tag{1.11-1}$$

为了形式上的对称，我们将$e(x_i)$转写为$x^*_ie_r(x_i^*)$，有

$$e_r(y^*) \approx \sum_{i = 1}^n\frac{\partial f(x_1^*, x_2^*, ..., x_n^*)}{\partial x_i}\frac{x_i^*}{y^*} e_r(x_i) \tag{1.11-1}$$

### 计算方法的数值稳定性，误差传播对工程的启发

若在计算过程中，数据误差不增长，则称算法是数值稳定的.

在实际计算中应注意如下可能会使误差急剧增长的操作：

1. 避免两个相近的数相减

   $$\vert e_r(x - y) \vert = \frac{\vert e(x) - e(y) \vert}{\vert x - y \vert}$$

   在$x \approx y$时，$x$与$y$的相减操作可能会得到相对误差非常大的结果.

   因此，在当$x$相当大时，计算如下等式时应进行变换：

   $$\sqrt{x+1} - \sqrt{x} = \frac{1}{\sqrt{x+1} + \sqrt{x}}$$

   $$\frac{1}{x} - \frac{1}{x + 1} = \frac{1}{x(x + 1)}$$

2. 优先计算较小数
   
   在利用计算机浮点数时，应优先计算较小数. 例：

   ```cpp
   double res1 = 0; 
   double res2 = 0;
   res1 += 1e20;
   for (int i = 0; i < INT_MAX; i++) {
       res1 += 1;
       res2 += 1;
   }
   res2 += 1e20;
   ```
   
   在运行上述代码段后，上述两个变量`res1`和`res2`会得到一个不同的数值，而在数学上，它们理应是相等的.

3. 避免小数除大数

   $$\epsilon(\frac{x}{y}) = \frac{ye(x)- xe(y)}{y^2}$$

4. 简化运算次数

2021.10.16
Hautbois


---
title: 数值分析02：恰定方程组的解
tags: 
  - 数值分析
---

恰定方程组指系数矩阵秩与其阶数相同，且与增广矩阵秩相同的方程组. 它有且仅有一个解向量. 

它的一般形式是

$$Ax = b$$

我们需要通过某种手段解出符合上述条件的$x$.

# Cramer法则

我们将上述列向量$b$替换$A$中的第$i$个列向量，把产生的新矩阵记作$A_i$，那么解向量的第$i$项：

$$x_i = \frac{det(A_i)}{det(A)} \tag{2.1}$$

式(2.1)被称为Cramer法则. 它要求系数矩阵$A$是满秩的.

## 证明 

对于满秩的系数矩阵$A$，我们可以有如下推导

$$Ax = b\tag{2.2-1}$$

$$x = A^{-1}b\tag{2.2-2}$$

由伴随矩阵的定义：

$$x = \frac{A^{*}b}{det(A)}\tag{2.2}$$

而由矩阵运算法则，解向量$x$的第$i$个分量可以描述为：

$$x_i = \sum^n_{j=0}A_{ij}^*b_j\tag{2.3-1}$$

而$A^*_{ij}$是$a_{ji}$(请注意$i$和$j$的顺序)的代数余子式.

故由行列式的Laplace展开运算，不难得到

$$x_i = \sum^n_{j=0}A_{ij}^*b_j = A_i \tag{2.3}$$

$\blacksquare$

## 复杂度

一个$n$阶行列式具有$n!$个Laplace展开项，每一个展开项由$n$项相乘，故计算单个$n$阶行列式需要$n!(n-1)$次乘法运算. 而对于$n$阶系数矩阵，Cramer法则要求计算$(n+1)$个矩阵. 故最终的计算复杂度为$(n+1)!(n-1)$，过大，不可接受.

*是什么深层次的原因导致了Cramer法则具有如此的计算复杂度呢？*

# Gauss消元法

Gauss消元法的核心任务是，对于增广矩阵$[A,\ b]\in \mathbb{R}^{n}$，执行$n$次消元，第$i$次消元通过基本矩阵变换将$a_{ii}$下方的所有元素归零.

## 简单Gauss消元法

数学语言描述：

**算法2.1:**

对于$A = [A,\ b]\in \mathbb{R}^{n}$的第$1$至$(n-1)$行向量$A_i(i < n)$，重复执行下述步骤：

$for\ j = i+1 , \ j <= n$，令

$$[A_j,b_j] \leftarrow [A_j,b_j] - \frac{a_{ji}}{a_{ii}}[A_i,b_i] \tag{2.4}$$

我们不需要对最后一列元素进行消元，因此使用了“第$1$至$(n-1)$行”的提法. 经过这样的消元，系数矩阵化为了上三角矩阵，可以立即解出$x_n$，从而逐个回代求解出解向量. 最终的解公式太过于啰嗦丑陋，且推导难度不大，故不在此重复.

$\blacksquare$

### 复杂度分析

根据上述算法，对于第$i$次执行来讲，赋值表达式(2.4)中首先执行了1次$\frac{a_{ji}}{a_{ii}}$，而行向量中共有$(n-i+1)$个非零元素，故每次执行赋值表达式(2.4)均要求$(n-i+2)$次乘法运算. 对于每一个行向量$i$，赋值表达式(2.4)均执行$(n-i)$次，故Gauss消元法的复杂度可以表述为：

$$\sum_{i =1}^{n}(n-1)(n-1+2) = \frac{n^3}{3}+n^2-\frac{n}{3}$$

### 失效情况

我们在之前提到了，实际计算中应尽量避免小数除大数，而上述的算法实际上无法避免这种情况，可能在某些矩阵的解决过程中会有某一极小的数作主元的情况发生. 故为了避免这样的问题，我们提出了如下的改进算法：

## (列)主元素法

(列)主元素法在算法2.1执行过程中，每面对一个新的$i$值，它都会遍历矩阵的第$i$列，将拥有绝对值最大的第$i$列元素的行向量替换至第$i$行. 这样的算法附加一个$O(n)$开销，但可以很大程度上保证算法的数值稳定.

### 全主元素法

继续改进，在在算法2.1执行过程中，每面对一个新的$i$值，它都会遍历整个矩阵，将拥有绝对值最大的元素的行向量替换至第$i$行，列向量替换至第$i$列，同时记录这个列变换，便于修正解向量的顺序. 这样的算法附加一个$O(n^2)$开销，但可以极大程度上保证算法的数值稳定，是求解中小型稠密恰定方程组的最优方法之一.

在不少应用场景中，系数矩阵$A$是保持不变的，而方程右值$b$来源于输入值，是时刻变化的. 因此在这样的情境下，使用简单Gauss消元法及其衍生方法就会产生复用性低的问题：面对每一个新的$b$，我们都需要重新执行一遍完全一样的Gauss消元步骤，这是愚蠢的. 故我们试图找到某一种方式来记录Gauss消元法的步骤，我们可以将步骤记录在某种数据结构中，但这种方式太过于随意，也太过于工程化，故难以与理论的误差分析等兼容，故我们提出了结构化描述Gauss消元步骤的方式，这种方式将实践的具体情况祓除，将上述的"记录"抽象为理论.

## 分解法

### 用基本初等矩阵刻画线性变换

在高等代数中我们知道，任何一个基本矩阵变换均可以被一个基本初等矩阵刻画. 假使我们有基本初等矩阵$L\in \mathbb{R}^n$，矩阵$A\in \mathbb{R}^n$，那么$LA$代表对$A$执行一系列初等行变换，而$AL$代表对$A$执行一系列初等列变换. 

在上述的情况下，$L$对$A$执行的变换操作相当于将单位矩阵$E$变换为$L$所需的行或列操作.

**例2.1** 设矩阵
$L = \begin{pmatrix}
1&0&0\\
0&1&0\\
0&1&1\\
\end{pmatrix}$ ，$A=\begin{pmatrix}
1&1&1\\
1&1&1\\
1&1&1\\
\end{pmatrix}$，求$LA$和$AL$.

解答：从单位矩阵$E$变换为$L$，若是行变换，需“将第二行的1倍加到第三行上”；若是列变换，需执行“将第三列的1倍加到第二列上”. 故$LA$可以视为“将$A$的第二行的1倍加到第三行上”，故

$$LA = \begin{pmatrix}
1&1&1\\
1&1&1\\
2&2&2\\
\end{pmatrix}$$

同理，$AL$可以视为“将$A$的第三列的1倍加到第二列上”，故

$$AL = \begin{pmatrix}
1&2&1\\
1&2&1\\
1&2&1\\
\end{pmatrix}$$

### Doolittle分解

我们在之后的讨论中，为了方便，将彻底不考虑增广矩阵，只考虑系数矩阵. 

因此，我们可以通过一个基本初等矩阵$L$，将上述对系数矩阵$A$执行的一系列消元法结构化地描述. 根据算法2.1，面对行向量$A_i$，我们执行的操作可以刻画为：

$$A\leftarrow L_iA$$

其中：

$$L=\begin{pmatrix}
1&0&...&0&0&...&0\\
0&1&...&0&0&...&0\\
\vdots&\vdots& &\vdots&\vdots&\vdots&\vdots\\
0&0&...&1&0&...&0\\
0&0&...&-l_{i+1,\ i}&1&...&0\\
0&0&...&-l_{i+2,\ i}&0&...&0\\
\vdots&\vdots& &\vdots&\vdots&\vdots&\vdots\\
0&0&...&-l_{n,i}&0&...&1
\end{pmatrix}$$

其中，$l_{k,i} = \frac{a_{k,i}}{a{i,i}}$

因此，算法2.1的一系列操作可以记作：

$$A\leftarrow L_{n-1}L_{n-2}...L_{1}A\tag{2.5'}$$

用新的符号重写上式以避免在下述推导中出现歧义：

$$U = L_{n-1}L_{n-2}...L_{1}A\tag{2.5}$$

左乘移项:

$$L_{1}^{-1}L_{2}^{-1}...L_{n-1}^{-1}U = A\tag{2.6}$$

而上述的各个$L_i$有着很好的运算性质(实际上，与线性变换的性质息息相关)：

在$L=\begin{pmatrix}
1&0&...&0&0&...&0\\
0&1&...&0&0&...&0\\
\vdots&\vdots& &\vdots&\vdots&\vdots&\vdots\\
0&0&...&1&0&...&0\\
0&0&...&-l_{i+1,\ i}&1&...&0\\
0&0&...&-l_{i+2,\ i}&0&...&0\\
\vdots&\vdots& &\vdots&\vdots&\vdots&\vdots\\
0&0&...&-l_{n,i}&0&...&1
\end{pmatrix}$时，$L^{-1}=\begin{pmatrix}
1&0&...&0&0&...&0\\
0&1&...&0&0&...&0\\
\vdots&\vdots& &\vdots&\vdots&\vdots&\vdots\\
0&0&...&1&0&...&0\\
0&0&...&l_{i+1,\ i}&1&...&0\\
0&0&...&l_{i+2,\ i}&0&...&0\\
\vdots&\vdots& &\vdots&\vdots&\vdots&\vdots\\
0&0&...&l_{n,i}&0&...&1
\end{pmatrix}$

且在$L_1=\begin{pmatrix}
1&0&...&0&0&...&0\\
0&1&...&0&0&...&0\\
\vdots&\vdots& &\vdots&\vdots&\vdots&\vdots\\
0&0&...&1&0&...&0\\
0&0&...&-l_{i+1,\ i}&1&...&0\\
0&0&...&-l_{i+2,\ i}&0&...&0\\
\vdots&\vdots& &\vdots&\vdots&\vdots&\vdots\\
0&0&...&-l_{n,\ i}&0&...&1
\end{pmatrix}$,$L_2=\begin{pmatrix}
1&0&...&0&0&...&0\\
0&1&...&0&0&...&0\\
\vdots&\vdots& &\vdots&\vdots&\vdots&\vdots\\
0&0&...&1&0&...&0\\
0&0&...&0&1&...&0\\
0&0&...&0&-l_{i+2, i+1}&...&0\\
\vdots&\vdots& &\vdots&\vdots&\vdots&\vdots\\
0&0&...&0&-l_{n, i+1}&...&1
\end{pmatrix}$时，有

$$L_1L_2=L=\begin{pmatrix}
1&0&...&0&0&...&0\\
0&1&...&0&0&...&0\\
\vdots&\vdots& &\vdots&\vdots&\vdots&\vdots\\
0&0&...&1&0&...&0\\
0&0&...&-l_{i+1,\ i}&1&...&0\\
0&0&...&-l_{i+2,\ i}&-l_{i+2, i+1}&...&0\\
\vdots&\vdots& &\vdots&\vdots&\vdots&\vdots\\
0&0&...&-l_{n,i}&-l_{n, i+1}&...&1
\end{pmatrix}$$

故我们可以根据上述的运算性质，将式(2.6)的所有$L_i$项合并为一个$L$项：

$$LU = A\tag{2.7}$$

其中：

$$L = \begin{pmatrix}
1&0&...&0&0&...&0\\
l_{2,\ 1}&1&...&0&0&...&0\\
\vdots&\vdots& &\vdots&\vdots&\vdots&\vdots\\
l_{3,\ 1}&l_{3,\ 2}&...&1&0&...&0\\
l_{4,\ 1}&l_{4,\ 2}&...&l_{i+1,\ i}&1&...&0\\
l_{5,\ 1}&l_{5,\ 2}&...&l_{i+2,\ i}&l_{i+2, i+1}&...&0\\
\vdots&\vdots& &\vdots&\vdots&\vdots&\vdots\\
l_{n,\ 1}&l_{n,\ 2}&...&l_{n,i}&l_{n, i+1}&...&1
\end{pmatrix}$$

是一个单位下三角矩阵. 而Gauss消元的结果$U$是一个上三角矩阵，故我们可以藉由Gauss消元的原理，将矩阵$A$分解为一个单位下三角矩阵和一个一般上三角矩阵的积. 这种分解被称为Doolittle分解.

#### 有效条件

正如我们在所有工程学、自然科学和形式科学中所作的工作一样，在提出一个新的工具之后，我们需要关注一个问题：在什么情况下，这种工具能够正常发挥作用？

我们之前提到，Doolittle分解是使用基本初等矩阵来记录Gauss消元的步骤，因此Gauss消元与Doolittle分解理应是等价的. 因此，适用Gauss消元的矩阵也理应适用Doolittle分解.

**定理2.1**：设$A\in \mathbb{R}^n$，若$A$的$1$至$(n-1)$阶顺序主子式$A_i$均非零，则矩阵$A$存在唯一的Doolittle分解.

这条定理是直观的：在执行消元时，我们需要一个非零的主元$a_{ii}$来执行消元操作. 而易证明，经过先前的消元操作之后，主元$a_{ii}$非零当且仅当主子式矩阵$A_{i}$是满秩的. 分析的证明难度不大，请见讲义.

我们应当注意到，定理2.1中的条件($1$至$(n-1)$阶顺序主子式$A_i$均非零)弱于方程可解条件(系数矩阵$A$满秩). 这一差距的来源在于，我们在执行消元的过程中，不需要对最后一列元素执行消元操作，因此，最后一个主元$a_{nn}$是否非零并不重要. 这表明，可以执行Doolittle分解是矩阵可解的必要非充分条件. 这也表示着，Doolittle分解已将消元法从解方程的实际情景中彻底地抽象了出来.

#### LC的计算方法

至此，我们提出了Doolittle分解的来源、分析定义和有效条件，是时候来讨论其具体的计算和应用方式了.

考察Doolittle分解的定义式:

$$LU = A\tag{2.7}$$

其中，$L$是一个单位下三角矩阵，$U$是一个上三角矩阵. 故将上式表示如下：

$$\begin{pmatrix}
1&0&0&...&0\\
l_{2,\ 1}&1&0&...&0\\
l_{3,\ 1}&l_{3,\ 2}&1&...&0\\
\vdots&\vdots&\vdots& &\vdots\\
l_{n,\ 1}&l_{n,\ 2}&l_{n,3}&...&1
\end{pmatrix}
\begin{pmatrix}
u_{1,1}&u_{1,2}&u_{1,3}&...&u_{1,n}\\
0&u_{2,2}&u_{2,3}&...&u_{2,n}\\
0&0&u_{3,3}&...&u_{3,n}\\
\vdots&\vdots&\vdots& &\vdots\\
0&0&0&...&u_{n,n}
\end{pmatrix} = \begin{pmatrix}
a_{1,1}&a_{1,2}&a_{1,3}&...&a_{1,n}\\
a_{2,1}&u_{2,2}&u_{2,3}&...&a_{2,n}\\
a_{3,1}&a_{3,2}&u_{3,3}&...&a_{3,n}\\
\vdots&\vdots&\vdots& &\vdots\\
a_{n,1}&a_{n,2}&a_{n,3}&...&a_{n,n}
\end{pmatrix}\tag{2.8}$$

由矩阵的乘法法则：

$$a_{ij} = \sum_{k = 1}^{n}l_{ik}u_{kj}\tag{2.9}$$

根据$L$和$U$矩阵中$1$和$0$的分布，我们将式(2.9)的计算简化(实际上，简化之后的形式变得非常丑陋)

$$\left\{
\begin{aligned}
&a_{ij}=u_{ij},\ i = 1 \\
&a_{ij} = \sum_{k=1}^{i-1}l_{ik}u_{kj}+u_{ij},\ j \leqslant i, i > 1 \\
&a_{ij} = \sum_{k = 1}^{n}l_{ik}u_{kj},\ j < i, i > 1
\end{aligned}
\right.\tag{2.10}$$

由此可以反解出

$$ \left\{
\begin{aligned}
&u_{1j}=a_{1j} \\
&u_{ij}=a_{ij} - \sum_{k=1}^{i-1}l_{ik}u_{kj},\ j \leqslant i, i > 1 \\
&l_{ij}=\frac{a_{ij} - \sum_{k=1}^{i-1}l_{ik}u_{kj}}{u_{jj}},\ j < i, i > 1
\end{aligned}
\right.\tag{2.11}$$

实际的计算顺序可以简单描述为"逐层计算，先行后列". 实际的计算步骤见讲义，看上去吓人但难度偏低(张宇纸老虎.gif)，按照上述公式实际操作一遍即可学会.

#### 在解方程中应用Doolittle分解

假设我们通过上述繁杂而无趣的计算方法求出了系数矩阵$A$的Doolittle分解：

$$LU = A\tag{2.7}$$

将其回代到方程组$Ax = b$中：

$$LUx = b$$

将向量$Ux$表示为$y$，有

$$ \left\{
\begin{aligned}
Ly = b\\
Ux = y
\end{aligned}
\right.$$

相当于解两个已经过消元的恰定(如果系数矩阵$A$是满秩的)方程组.

至于课本中描述的紧凑格式解方程组，可以认为是一种类似于小学数学"列竖式计算"的内容，重要性偏低.

而所谓追赶法解三对角方程，即是上述$L$和$U$矩阵中多出了很多个$0$，在式(2.11)中多代入几个$0$即可. 不是很理解为什么还要将其单列一节.

#### 开销分析

由于基本原理相同，Doolittle分解法与Gauss消元法具有相同的运算量，但Doolittle分解法有着复用性的优势.

### Cholesky分解

在实践中，有相当一部分线性求解问题应用到了正定二次型的系数矩阵，而正定二次型具有更优的性质，可能可以简化求解运算. 因此我们有必要针对正定二次型来提出一种新的算法.

#### 正定二次型

若一个矩阵满足$A = A^T$，则将其称为对称矩阵，而术语“二次型”常被用于提及一个对称矩阵.

当一个二次型$A$满足以下四个等价条件之一：

1. $\forall x \neq 0，x^TAx>0$
2. $\forall A的特征值 \lambda, \lambda > 0$
3. $A$的所有顺序主子式均大于$0$
4. $C \in \mathbb{R}^{n}, A = C^TC$

则$A$被称为正定二次型. 上述四个条件均为命题"$A$是正定二次型"的充分必要条件

#### 平方根法

由上述性质4：$C \in \mathbb{R}^{n}, A = C^TC$，我们知道，一个正定二次型可以被分解为一个实矩阵和其转置的积.

**定理2.2**: 若$A$属$n\times n$正定二次型，则存在唯一的可逆下三角矩阵$C$，使得

$$A = CC^T \tag{2.12}$$

其中，$C$的对角元素均为正实数.

**证明**：

由定理2.1，存在$LU = A$，其中$L$是下三角单位矩阵，$U$为上三角矩阵. 我们将$U$的对角线元素抄到矩阵$D$中，即记矩阵$D = diag(u_{11}, u_{22}, ..., u_{nn})$，再记矩阵$P = D^{-1}U$，则$A = LDP$. 请注意，**此时的$P$之对角元素全部为1**. 又$A$属二次型，$D$属对角矩阵，故：

$$LDP = A = A^T = P^TD^TL^T = P^TDL^T$$

而，定理2.1指出，同一个矩阵的Doolittle分解是唯一的. 因此不难得到$L = P^T$，即

$$A = LDP = P^TDP \tag{2.13}$$

接下来，我们试图将对角矩阵$D$拆分为形如$F^TF$的形式. 但这要求着$F$也为对角矩阵，且$D$的对角线元素值为$F$相应对角线元素值的平方，这就要求$D$的对角线元素均为正. 因此证明：

由正定性质：任取一非零列向量$x\in \mathbb{R}^{1\times n}$，有

$$x^TAx = x^TP^TDPx > 0$$

这种形式表示：$D$也是一个正定二次型，而$D$是一个对角阵，对角阵的对角元素就是它的特征值. 因此$D$的对角线元素为正.

故$D = F^TF, F=diag(\sqrt{u_{ii}})$. 即

$$A = P^TDP=P^TF^TFP=(FP)^TFP=CC^T$$

易知$FP$是上三角矩阵，故$C = (FP)^T$是一个下三角矩阵.

$\blacksquare$

唯一性的证明是容易的，请见讲义.

##### 计算方法

Cholesky分解的计算原理与Doolittle分解一致，均是应用矩阵乘法法则. 直接把结果贴在此处.

$$ \left\{
\begin{aligned}
&c_{kk}=(a_{kk} - \sum_{j=1}^{k-1}c^2_{kj})^{1/2} \\
&c_{ik}=(a_{ik}-\sum_{j=1}^{k-1}c_{ij}c_{kj})^{1/2}, i>k \\
\end{aligned}
\right.\tag{2.11}$$

完成Cholesky分解后的解方程步骤与Doolittle分解一致.

容易得知，Cholesky分解应用了对称性，故计算复杂度为Gauss消元法的一半，为$O(n^3/6)$.

但是，Cholesky分解法调用了平方运算，可能会导致运算时间的延长.

#### 改进的平方根法

在Cholesky分解的证明过程中，我们提到：

$$A = LDP = LDL^T \tag{2.13}$$

其中，$L$是单位下三角矩阵，$D$为对角矩阵.

因此我们可以将矩阵$A$作$LDL^T$分解以避免平方根运算.

##### 计算方法

改进的平方根法在求矩阵$L$和$D$的过程中采用了与Doolittle分解完全一致的方式：首先将$LD$组合为一个新的矩阵$U$处理，再通过与Doolittle分解完全一致的计算方式得到如下的计算式：

$$ \left\{
\begin{aligned}
&d_{k}=a_{kk}- \sum_{k=1}^{k-1}l^2_{kj}d_{j}\\
&l_{ij}=(a_{ik} - \sum_{k=1}^{k-1}l_{ij}d_jl_{kj})/d_k,\ k \leqslant i\\
\end{aligned}
\right.\tag{2.14}$$

如果对式子的形式有着充分的观察力，会发现式(2.14)与(2.11)有着完全一致的形式(2.11的第二条由于对称性被简化掉了).

这样的运算式尚未把正定二次型的性质充分发挥，故提出了一个使用辅助量$u_{ik}=l_{ik}d_k$(实际上就是将$LD$的积$U$中的元素显式地记录了下来)的计算式，见课本.

使用改进的平方根法解方程的步骤与上述两方法稍有出入：

$$Ax = b$$

$$LDL^Tx=b$$

$$Ly = b, L^Tx=D^{-1}b$$

# 针对矩阵的误差分析

直观地，误差分析首先要求我们能够确定且统一地描述误差(即测定值与实际值之间的"距离")，而单靠矩阵显然做不到这一点，因此我们借用了范数这一工具.

## 度量方法：范数

范数实际上是实分析中的一条重要概念，是长度概念的推广，我们在此将其简化描述.

假设有一线性空间$\mathbb{D}$，存在某一函数$\Vert ·\Vert$将$C$映射至非负实数空间$\mathbb{R}^*$，且这个函数满足：

1. $\Vert x\Vert=0$当且仅当$x=0$(正定性)
2. $\forall x \in \mathbb{D}, \alpha \in \mathbb{R}, \Vert\alpha x \Vert=\vert\alpha\vert \Vert x \Vert$(绝对齐次性)
3. $\forall x,y \in \mathbb{D}, \Vert x + y \Vert \leqslant \Vert x \Vert + \Vert y \Vert$(三角不等式)

则这一函数被称为$\mathbb{D}$上的一个**范数**，二元体$(\mathbb{D}, \Vert·\Vert)$被称为一个赋范线性空间或Banach空间. 

一句话描述：**范数是某种距离的刻画**.

我们称具有如下性质的范数是等价的：

在一线性空间$\mathbb{D}$上的两个范数$\Vert ·\Vert_1$和$\Vert ·\Vert_2$，如$\forall x \in \mathbb{D}$, $\exist M,m\in\mathbb{R}, M>m>0$，有：

$$m\Vert x \Vert_2 \leqslant \Vert x \Vert_1 \leqslant M\Vert x \Vert_2$$

则称范数$\Vert ·\Vert_1$和$\Vert ·\Vert_2$等价.

这种等价性给出了一种很好的性质：等价范数之间只差一个常数倍，因此，想要得到向量的某种性质，无论用哪种范数来估计，都可以获得相同的结果.

### 向量范数

针对于$1\times n$向量空间的常用范数有：

1. 1-范数：$\Vert x \Vert_1=\sum_{i=1}^{n}x_i$
2. 2-范数：$\Vert x \Vert_2=\sqrt{\sum_{i=1}^{n}x_i^2}$
3. $\infty$-范数：$\Vert x\Vert_\infty = max\ x_i$

所有的向量范数均是等价的(证明请参见任一实分析教材，如陈建功《实函数论》)

### 矩阵范数

一般来讲，在矩阵空间上的某一函数$\Vert ·\Vert$想要被称之为范数，还需要附加一个相容性条件：

$$\Vert AB \Vert \leqslant \Vert A \Vert\Vert B \Vert$$

#### 诱导范数

若$\Vert x \Vert$属一$1\times n$向量空间上的向量范数，$A$是一$n\times n$矩阵，则

$$\Vert A\Vert_m = \max_{x\not ={0}}\frac{\Vert Ax \Vert}{\Vert x\Vert}$$

则$\Vert A\Vert_m$是一矩阵范数，称为诱导范数或算子范数.

证明属实分析内容，略.

由上述向量范数可以诱导出矩阵范数：

1. 1-范数：最大的列总和
2. 2-范数(谱范数)：矩阵的最大奇异值之平方根
3. $\infty$-范数：最大的行总和

所有的矩阵范数均是等价的

## 矩阵的误差定义

提出了范数工具，我们即可定义矩阵的误差：

记测量值为$x^*$，真实值为$x$，则$\Vert x - x^* \Vert$为向量的绝对误差，$\frac{\Vert x - x^* \Vert}{\Vert x^* \Vert}$为向量的绝对误差.

记测量值为$A^*$，真实值为$A$，则$\Vert A - A^* \Vert$为矩阵的绝对误差，$\frac{\Vert A - A^* \Vert}{\Vert A^* \Vert}$为矩阵的绝对误差.

## 方程组的条件数

在某些情况下，方程组$Ax=b$中，若在右端$b$施加一小扰动$\delta b$，其解$x$会获得一个相当大的扰动$\delta x$，这种方程组被称为病态的.

为了定量刻画方程组的病态程度，我们讨论上述情况：

$$A(x+\delta x)=b+\delta b \tag{2.15}$$

试图得到其解的相对变化$\frac{\Vert \delta x \Vert}{\Vert x \Vert}$与右值的相对变化$\frac{\Vert \delta b \Vert}{\Vert b \Vert}$的相关关系，从而找出其病态的根源：

由(2.15)得到$A\delta x = \delta b$，而$A$是可逆矩阵，故：

$$\Vert \delta x \Vert = \Vert A^{-1}\delta b \Vert \leqslant\Vert A^{-1} \Vert\Vert \delta b \Vert$$

又$\Vert Ax \Vert \leqslant \Vert A \Vert \Vert x \Vert$，得到

$$\Vert x \Vert \geqslant \frac{\Vert Ax \Vert}{\Vert x \Vert} = \frac{\Vert b \Vert}{\Vert A \Vert}$$

故：

$$\frac{\Vert \delta x \Vert}{\Vert x \Vert} \leqslant \frac{\Vert A^{-1} \Vert\Vert \delta b \Vert}{\frac{\Vert b \Vert}{\Vert AS \Vert}}=\frac{\Vert A^{-1} \Vert\Vert A \Vert\Vert \delta b \Vert}{\Vert b \Vert}$$

故我们发现$\Vert A^{-1} \Vert\Vert A \Vert$控制着这种扰动的大小，这个量被称为条件数，描述为$cond(A)$. 直观地，条件数的大小与所用范数有关.

### 性质

条件数有如下性质：

1. 任意方阵的条件数大于$1$
2. $\forall \alpha \not ={0} \in \mathbb{R}, cond(\alpha A) = cond(A)$
3. 正交矩阵的2-范数条件数为1

在如下情况下，方阵的条件数会变得很大：

1. 有两行/列向量非常相近时(这种问题通常被称为多重共线性问题)
2. 有某一主元有着较小的绝对值时
3. 各行/列向量元素的数量级差距较大时

2021.10.18
Hautbois

---
title: 数值分析03
tags: 
  - 数值分析
---

## 残量与误差控制

我们继续进行误差分析的讨论：

为表征线性方程组$Ax = b$之解$x^*$的相对误差，记$x^*$的残量$r=b-Ax^*$，则相对误差有如下关系：

$$\frac{\Vert x-x^* \Vert}{\Vert x \Vert} \leqslant cond(A)\frac{\Vert r \Vert}{\Vert b \Vert}\tag{3.1}$$

证明借用了矩阵误差的相容性条件，推导不难，请见讲义.

式(3.1)表明，即使求解出的残量很小，但如果方程的状态数较大，仍可能会导出一个较大的相对误差，这表明$cond(A)$导致的误差增大是一个不可回避的问题.

### 残量方程

根据残量的定义，有第$n$次迭代的残量：$b-Ax_n=r_n$，解方程$Ad_n=r_n$可以解出残量$r_n$反射至$x_n$的数值，迭代：$x_{n+1}=x_n+d_n$即可减小$x_{n+1}$的残量.

# 超定线性方程组的最小二乘解

看一个引例：对物理量$x$执行三次测量，结果分别是$x_1, x_2, x_3$，则这个真实量最有可能是？
记函数$f(x) = \frac{1}{3}[(x-x_1)^2+(x-x_2)^2+(x-x_3)^2]$，称其为方差函数. 我们可以后验地认为，当方差函数取最小值时的$x=(x_1+x_2+x_3)/3$是最有可能取到的. 这样的思想被称为“极大似然”，这种方法被称为“最小二乘”. 我们将上述的情况转化为矩阵形式：

$$\begin{pmatrix}
1\\
1\\
1\\
\end{pmatrix} \begin{pmatrix}
x
\end{pmatrix} = \begin{pmatrix}
x_1\\
x_2\\
x_3\\
\end{pmatrix}$$

上式看作是一个线性方程组，而若$x_1, x_2, x_3$不完全相同，则上述方程组的增广矩阵之秩比系数矩阵秩大，故这样的方程组属"无解"方程组，称为超定线性方程组. 但我们仍可以通过某一优化目标来将这个问题转化为一个最优化问题. 当这一优化目标是"均方误差最小化"，或称为"2-范数最小化"时，这样的解法称为"超定线性方程组的最小二乘解法".

关于超定方程最小二乘解的导函数推导需要用到矩阵分析领域的矩阵求导术，较为复杂且不易理解，详细内容请参见课本或讲义. 下面提出一个较为容易理解的推导方法：

## 解的推导：几何理解

对于线性方程组$Ax =b$，将$A$列向量记为$(a_1, a_2, a_3, ..., a_n)$，其中$a_i\in \mathbb{R}^{n\times1}$，考虑优化条件：

$$\min_{x^*}\Vert b - Ax^* \Vert_2\tag{3.2}$$

它的意义是：$b$到$Ax^*$的距离最小. 其中，$Ax^*$可以由矩阵乘法法则展开为：

$$Ax^* = \sum_{i=0}^na_ix^*_i\tag{3.3}$$

式子(3.3)代表着向量组$(a_1, a_2, a_3, ..., a_n)$的任意线性组合，即向量空间$(a_1, a_2, a_3, ..., a_n)$中的所有向量.

现在，我们需要知道，既然$Ax=b$是一个超定线性方程组，那么矩阵$A$的秩一定比增广矩阵$(A,b)$小，那么向量$b$一定不可以被列向量组$(a_1, a_2, a_3, ..., a_n)$线性表出，这就意味着，**在向量$b$中至少有一个分量在向量空间$(a_1, a_2, a_3, ..., a_n)$之外**，而最小化条件要求$b$到$Ax^*$的距离最小，而$Ax^*$可以是向量空间$(a_1, a_2, a_3, ..., a_n)$中的所有向量，这就必然意味着向量$b - Ax^*$与所有的$a_i,i\in[1,n]$正交. 

如果不理解的话可以在纸上画以下三个向量：

$$a_1 = (1,0,0)$$

$$a_2 = (0,1,0)$$

$$b = (1,1,1)$$

此处，与$b$距离最近且在向量空间$(a_1, a_2)$中的向量必然是$Ax^* = (1,1,0)$，此时$b - Ax^*$与所有的$a_i,i\in[1,n]$正交.

那么，我们就可以使用向量内积将问题转化为：

$$\left\{
\begin{aligned}
&a_1^T(b-Ax^*)=0\\
&a_2^T(b-Ax^*)=0\\
&a_3^T(b-Ax^*)=0\\
&...\\
&a_n^T(b-Ax^*)=0\\
\end{aligned}
\right.$$

上式可以转写为一个线性方程：

$$\begin{pmatrix}
a_1^T\\
a_2^T\\
...\\
a_n^T\\
\end{pmatrix} \begin{pmatrix}
b-Ax^*
\end{pmatrix} = \begin{pmatrix}
0\\
0\\
...\\
0
\end{pmatrix}$$

而上式中左侧的矩阵恰好为$A^T$，即：

$$A^T(b-Ax^*)=0$$

$$A^TAx^*=A^Tb\tag{3.4}$$

$\blacksquare$

注意，由于$A$不一定为方阵，即使为方阵也不一定满秩，故式(3.4)不可继续使用矩阵的标准逆元来化简，我们须使用其他的技术：

(以下内容复制自我之前的笔记，不代表目前水平)

## 矩阵的Moore–Penrose广义逆运算

https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse

### 定义

对于一个$m\times n$矩阵$A$，它的Moore–Penrose广义逆元被定义为：满足如下条件(Moore–Penrose条件)的一个$n\times m$矩阵$A^+$:

1. $AA^+A=A$
2. $A^+AA^+=A^+$
3. $(AA^+)^T=AA^+$
4. $(A^+A)^T=A^+A$

若矩阵$A$可逆，那么$A^{-1}$是其Moore–Penrose广义逆元.

我们仅需要证明其性质其一:

$$AB = ABB^+A^+AB$$

$$AB = (AB)(AB)^+(AB)$$

得到

$$ABB^+A^+AB = AB(AB)^+AB$$

故

$$(AB)^+ = B^+A^+$$

### 通过SVD计算Moore–Penrose广义逆元

由SVD运算的拆分式：

$$M = U\Sigma V^T$$

由*若矩阵$A$可逆，那么$A^{-1}$是其Moore–Penrose广义逆元*和上述证明的性质得到

$$M^+ = V\Sigma^+ U^T$$

而$\Sigma$是对角矩阵，其Moore–Penrose广义逆的算法明显是将其对角线元素取逆元(倒数).

由此，我们可以通过SVD计算出任意矩阵的Moore–Penrose广义逆元.

### Moore–Penrose广义逆元用于超定方程组求解

我们回到超定方程组求解时的倒数第二步:

$$A^Tb-A^TAx^* = 0$$

此时，我们的$A$不是列满秩矩阵，因此不可用$A^TA$的一般逆来计算其值，因此我们应用Moore–Penrose广义逆运算(此时我们可以继续化简，因为即使是非方阵也具有Moore–Penrose广义逆元):

$$x^* = (A^TA)^+A^Tb = A^+A^{T+}A^Tb = A^+b$$

如此，我们解决了当属性矩阵列不满秩时的情况. 由于计算Moore–Penrose广义逆元需要更长的时间，所以我们还是建议在设计属性值的时候避免列不满秩的情况发生.

# 线性方程组迭代法，数值解

## 向量和矩阵的极限

记$\{ x^{(k)} \}$是$\mathbb{R}^{n\times1}$上的向量序列，若：

$$\lim_{n\to\infin}\Vert x^{(k)} - x \Vert = 0$$

我们就称向量序列$\{ x^{(k)} \}$收敛至$x$.

向量收敛的充分必要条件是向量列表中的每一个元素$x_i^{(k)}$均收敛至$x_i$.

矩阵同理.

## 迭代法的一般形式

**定理3.1**：对恰定线性方程组$Ax=b$，构造与其同解的线性方程组

$$x = Mx+g\tag{3.6}$$

则$\forall x^{(0)}\in \mathbb{R}^{n\times 1}$，代入上述方程：

$$x^{(1)} = Mx^{(0)}+g$$

$$x^{(k+1)} = Mx^{(k)}+g$$

若：当$k$充分大时$x^{(k)}$收敛至某一点$x^*$，则$x^*$是方程组$Ax=b$的解.

证明：

不妨设$x^{(0)}=b$，有$x^{(1)}=(E-A)b+b$，$x^{(2)}=(E-A)^2b+(E-A)b+b$，则

$$x^{(n)} = (E-A)^nb+(E-A)^{n-1}b+...+b\tag{3.6}$$

又，矩阵的乘法运算是结合的，而加法运算是交换的，故可以对式(3.6)应用等比数列求和公式：

$$x^{(n)} = \frac{b-(E-A)^{n+1}}{E-(E-A)}=\frac{b-(E-A)^{n+1}}{A}\tag{3.7}$$

上述分数线是方便表达的记法，指"分母的逆元左乘分子".

已知上式收敛，故$\lim_{n\to\infin}x^{(n)} = A^{-1}b$

$\blacksquare$

## 常用迭代法

### Jacobi迭代法

将方程组$Ax = b$展开：

$$\left\{
\begin{aligned}
&a_{11}x_1 + a_{12}x_2 + a_{13}x_3 + ... + a_{1n}x_n=b_1\\
&a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + ... + a_{2n}x_n=b_2\\
&a_{31}x_1 + a_{32}x_2 + a_{33}x_3 + ... + a_{3n}x_n=b_3\\
&...\\
&a_{n1}x_1 + a_{n2}x_2 + a_{n3}x_3 + ... + a_{nn}x_n=b_n\\
\end{aligned}
\right.$$

移项：第$i$个方程左边只留下$x_i$：

$$\left\{
\begin{aligned}
&x_1 = (b_1-a_{12}x_2 - a_{13}x_3 - ... + a_{1n}x_n)/a_{11}\\
&x_2 = (b_2-a_{21}x_1 - a_{23}x_3 - ... + a_{2n}x_n)/a_{22}\\
&x_3 = (b_3-a_{31}x_1 - a_{32}x_2 - ... + a_{3n}x_n)/a_{33}\\
&...\\
&x_n = (b_n-a_{n1}x_1 - a_{n2}x_3 - ... + a_{n,n-1}x_{n-1})/a_{nn}\\
\end{aligned}
\right.$$

上式可以转写为$x = Mx+g$，其中

$$g = (b_1/a_{11}, b_2/a_{22}, ..., b_i/a_{ii}, ..., b_n/a_{nn})^T$$

$$M = \begin{pmatrix}
0&-a_{12}/a_{11}& ... & -a{ij}/a_{ii} &... & -a{1n} / a_{11}\\
-a_{21}/a_{22}&0& ... & -a{ij}/a_{ii} &... & -a{2n} / a_{22}\\
-a_{31}/a_{33}&-a{32}/a_{33}& ... & -a{ij}/a_{ii} &... & -a{2n} / a_{22}\\
\vdots&\vdots&-a{ij}/a_{ii}&\vdots& &\vdots\\
-a_{n1}/a_{nn}&-a{n2}/a_{nn}& ... & -a{ij}/a_{ii} &... & 0\\
\end{pmatrix}$$

易知$x = Mx+g$与$Ax=b$同解.

将上述$M$与$g$重新表示：

定义$D=diag(a_{11}, a_{22}, ..., a_{ii}, ..., a_{nn})$

则$D^{-1} = diag(a^{-1}_{11}, a^{-1}_{22}, ..., a^{-1}_{ii}, ..., a^{-1}_{nn})$

则$M = E-D^{-1}A$，$g = D^{-1}b$

### Gauss-Seidel迭代法

Jacobi迭代的运算过程可以描述如下：

$$\left\{
\begin{aligned}
&x^{(k+1)}_1 = (b_1-a_{12}x^{(k)}_2 - a_{13}x^{(k)}_3 - ... + a_{1n}x^{(k)}_n)/a_{11}\\
&x^{(k+1)}_2 = (b_2-a_{21}x^{(k)}_1 - a_{23}x^{(k)}_3 - ... + a_{2n}x^{(k)}_n)/a_{22}\\
&x^{(k+1)}_3 = (b_3-a_{31}x^{(k)}_1 - a_{32}x^{(k)}_2 - ... + a_{3n}x^{(k)}_n)/a_{33}\\
&...\\
&x^{(k+1)}_n = (b_n-a_{n1}x^{(k)}_1 - a_{n2}x^{(k)}_3 - ... + a_{n,n-1}x^{(k)}_{n-1})/a_{nn}\\
\end{aligned}
\right.$$

为节省存储空间和加快迭代进度，在运算过程中我们可以使用前步求出的结果代入后步：

$$\left\{
\begin{aligned}
&x^{(k+1)}_1 = (b_1-a_{12}x^{(k)}_2 - a_{13}x^{(k)}_3 - ... + a_{1n}x^{(k)}_n)/a_{11}\\
&x^{(k+1)}_2 = (b_2-a_{21}x^{(k+1)}_1 - a_{23}x^{(k)}_3 - ... + a_{2n}x^{(k)}_n)/a_{22}\\
&x^{(k+1)}_3 = (b_3-a_{31}x^{(k+1)}_1 - a_{32}x^{(k+1)}_2 - ... + a_{3n}x^{(k)}_n)/a_{33}\\
&...\\
&x^{(k+1)}_n = (b_n-a_{n1}x^{(k+1)}_1 - a_{n2}x^{(k+1)}_3 - ... + a_{n,n-1}x^{(k+1)}_{n-1})/a_{nn}\\
\end{aligned}
\right.$$

在实践过程中，只需要使用同一块存储单元，将求出的结果直接覆盖在旧的结果上即可.

数学上，我们将Jacobi迭代的$M$矩阵一分为二：

$$x^{(k+1)} = Lx^{(k+1)}+Ux^{(k)}+g$$

其中：$L$是下三角矩阵，$U$是上三角矩阵

$$L = \begin{pmatrix}
0&0& ... & 0 &... & 0\\
-a_{21}/a_{22}&0& ... &0 &... & 0\\
-a_{31}/a_{33}&-a{32}/a_{33}& ... & 0 &... & 0\\
\vdots&\vdots& &\vdots& &\vdots\\
-a_{n1}/a_{nn}&-a{n2}/a_{nn}& ... & -a{ij}/a_{ii} &... & 0\\
\end{pmatrix}$$

$$U = \begin{pmatrix}
0&-a_{12}/a_{11}& ... & -a{ij}/a_{ii} &... & -a{1n} / a_{11}\\
0&0& ... & -a{ij}/a_{ii} &... & -a{2n} / a_{22}\\
0&0& ... & -a{ij}/a_{ii} &... & -a{2n} / a_{22}\\
\vdots&\vdots& &\vdots& &\vdots\\
0&0& ... & 0 &... & 0\\
\end{pmatrix}$$

### 松弛迭代法

继续改进，将每一次迭代的Gauss-Seidel近似解$x^{(k+1)'}$与上一次迭代的$x^{(k)}$作加权平均：

$$x^{(k+1)} = \omega x^{(k+1)'} + (1-\omega )x^{(k)}$$

这样的方法称为松弛迭代.

其中$\omega$称为松弛系数，$\omega >1$称为高松弛，$\omega <1$称为低松弛，$\omega =1$为Gauss-Seidel法.

### 三种迭代方式的通式

观察得到，三种迭代方式的数学表达拥有共同的形式：

$$x^{(k+1)} = (L,U)(ax^{(k+1)}, bx^{(k)})^T$$

通过调整不同的$(a, b)$即可得到不同的迭代方式.

## 迭代法收敛的判断

### 谱半径

矩阵的谱指一个矩阵的特征值的集合. 矩阵的谱半径$\rho(A)$是指模最大的特征值.

由特征值的性质容易导出，$\rho(A^k)=\rho(A)^k$

关于谱半径有如下定理(证明见讲义):

1. 矩阵谱半径不大于矩阵的任意范数
2. 存在某矩阵范数$\Vert\Vert_i$，对于任意小的正数$\epsilon$，有：$\Vert A\Vert_i \leqslant \rho(A) + \epsilon$
3. 若$A\in \mathbb{R}^{n\times n}$，则$\lim_{k\to \infin}A^k=0$当且仅当$\rho(A) < 1$

由上述定义及定理，立即推得：**迭代式$x^{(k+1)} = Mx^{(k)}+g$收敛，当且仅当$\rho(M) < 1$**

有若干推论：

1. 若$M$的任意范数$\Vert M\Vert < 1$，则收敛
2. 松弛法收敛的必要条件是$\omega \in (0,2)$

### 快速判断收敛的若干结论

#### 对角占优矩阵

若一个方阵某一行的对角元之绝对值大于该行所有其余元素绝对值之和，则称这个方阵是弱对角占优的. 若所有行都满足上述条件，则称这个方阵是严格对角占优的.

例：下述方阵就是严格对角占优的：

$$\begin{pmatrix}
100&1& 2 &3 \\
1&90& 3 &6 \\
0&1& 120 &-9 \\
3&1& 2 & -203 \\
\end{pmatrix}$$

有若干结论：

1. 若方阵是严格对角占优阵或者不可约的弱对角占优阵，则Jacobi和G-S必收敛
2. 若方阵是严格对角占优阵且$\omega\in(0,1]$，松弛法收敛
3. 若方阵是正定二次型且$\omega\in(0,2)$，松弛法收敛

## 误差估计

设有迭代式$x^{(k+1)} = Mx^{(k)}+g$收敛，则误差：

$$\Vert x^{(k)} - x^* \Vert \leqslant \frac{\Vert M \Vert^k}{1 - \Vert M \Vert}\Vert x^{(1)} - x^{(0)} \Vert$$

证明不难，见讲义.

借此，若定义了误差容许范围$\epsilon$，我们可以将其代入上式求出迭代次数的范围：

$$k \geqslant \frac{\ln\frac{\epsilon(1 - \Vert M \Vert)}{\Vert x^{(1)} - x^{(0)} \Vert}}{\ln \Vert M \Vert}$$

## 对于正定二次型的迭代方法

对于正定二次型有梯度下降法和共轭梯度法，本质上是非线性方程问题，不应在此处叙述.

2021.10.23
Hautbois

---
title: 数值分析04
tags: 
  - 数值分析
---

# 矩阵特征值的计算方式

## 幂法

设置方阵$A$的各特征值$\lambda$中仅有唯一的模长最大的值，且各特征矢$u_i$线性无关，即：

$$\vert \lambda_1\vert > \vert \lambda_2 \vert \geqslant \vert \lambda_3\vert \geqslant ... \geqslant \vert \lambda_n\vert$$

设$x^{(0)}$是任意不为零的$n$维向量：

由迭代公式：

$$x^{(k+1)} = Ax^{(k)} \tag{4.1}$$

可以证明，在$k$足够大时，矩阵特征值$\lambda$是向量$x^{(k+1)}$与向量$x^k$的任意两个对应分量之商$x^{(k+1)}_i/x^{k}_i$

**证明**：

由于$u_i$是$n$个$n$维向量且线性无关，故它是$n\times1$空间中的一组基底，故无论$x^{(0)}$的取值，它总能被表示为$x^{(0)}= \sum_{i=1}^na_iu_i$，其中，$a_i$不全为0. 即$x^{(0)}$是各个特征矢的线性组合.

则由迭代公式：

$$x^{(k+1)} = Ax^{(k)} = A^{k+1}x^{(0)}=\sum_{i=1}^nA^{k+1}a_iu_i=\sum_{i=1}^n\lambda_i^{k+1}a_iu_i$$

重写上述等式：

$$x^{(k+1)}=\lambda_1^{k+1}[a_1u_1+(\frac{\lambda_2}{\lambda_1})^{k+1}a_2u_2+(\frac{\lambda_3}{\lambda_1})^{k+1}a_3u_3+...+(\frac{\lambda_n}{\lambda_1})^{k+1}a_nu_n] \tag{4.2}$$

不妨假设$\lambda_1$是各个特征值中绝对值最大的一个，则在$k$充分大时，等式(4.2)可以截断为：

$$x^{(k+1)}\approxeq\lambda_1^{k+1}a_1u_1 \tag{4.3}$$

此时，$x^{(k+1)}$可以看作是方阵$A$对应于特征值$\lambda_1$的一个特性矢. 同时，在$k$充分大时，

$$x^{(k)}\approxeq\lambda_1^{k}a_1u_1 \tag{4.4}$$

比较式子(4.3)和(4.4)，可以知道，$x^{(k+1)}$和$x^{(k)}$线性相关，且线性系数之比为$\lambda_1$

故，可以将$x^{(k+1)}$和$x^{(k)}$的任一对应分量$x_i$作比，有：

$$\lambda_1 = x^{(k+1)}/x^{(k)}\tag{4.5}$$

$\blacksquare$

但是精妙的数学逻辑应用到计算机上就会变得粗糙，由于我们需要重复计算若干多次式(4.1)，向量$x^{(k)}$中的分量可能出现溢出或舍入为0的情况. 故这时我们需要对每一次迭代出的向量$x^{(k)}$进行归一化处理：

$$ \left\{
\begin{aligned}
&y^{(k)} = \frac{x^{(k)}}{\Vert x^{(k)} \Vert_\infin}\\
&x^{(k+1)} = Ay^{(k)} \\
\end{aligned}
\right.$$

由上述原理，在实际计算中，在式子(4.5)的分量选择过程中，我们应当选择绝对值最大的分量以消除舍入误差.

### 有效性讨论

1. 若不满足"仅有一个模取得最大值"的情况，由上述推导过程易知算法仍有效
2. 特殊地，若有$\lambda_1 = -\lambda_2$时，迭代过程变为一摆动序列，则比较$k$足够大时的任意相邻三项即可得到求出$\lambda_1$和$u_1$与$u_2$
3. 易知，算法的收敛速度取决于$\vert\lambda_2\vert/\vert \lambda_1\vert$

### 迭代加速

#### 原点移位法

设$\lambda_i$是$A$的特征值，则$\lambda_i - \lambda_0$是$A-\lambda_0E$的特征值.

因此我们只需要找到合适的$\lambda_0$使得$\forall i \in \mathbb{N}^*, \vert \lambda_1 - \lambda_0\vert > \vert \lambda_i - \lambda_0\vert$且$max\frac{\vert \lambda_i - \lambda_0\vert}{\vert \lambda_1 - \lambda_0\vert}<\vert \frac{\lambda_2}{\lambda_1}\vert$，我们就可以用矩阵$A-\lambda_0E$来代替矩阵$A$，从而获得更快的收敛过程.

但实践中我们不可能先验地知晓$A$的所有特征值，故此该方法在实际上难以精确地执行.

#### Aitken方法

我们称幂法中"$k$足够大时，$x^{(k+1)}$和$x^{(k)}$成比例"为一阶收敛. 若序列$a_k$一阶收敛至$a$，即：

$$\lim_{k\to \infin}\frac{a_{k+1}-a}{a_k-a} = c \not ={0}$$

作近似：在$k$充分大时有：

$$\frac{a_{k+2}-a}{a_{k+1}-a}\approx \frac{a_{k+1}-a}{a_{k}-a}$$

解出$a$的表达式：

$$a' \approx a_k-\frac{(a_{k+1} - a_k)^2}{a_{k+2}-2a_{k+1}+a_k}$$

每次进行幂法迭代之后，我们都依据上式计算一次$\lambda'$，若$\lambda'$与上一个$\lambda'$之间的误差满足条件，则输出结果.

# 函数插值法

已知平面上若干互不相同的点$(x_0, y_0), (x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$，其关系属于某**未知**对应关系$f(x)$，如何构造一函数，使得该函数可以穿过这些点并刻画$f(x)$？

这种问题被称为插值问题，上述的函数被称为插值函数. 

## Lagrange多项式法

Lagrange多项式法使用一个$n$次多项式来拟合上述的$(n+1)$个点，且可以得出唯一的结果. 

**证明：**

对于点集$\{(x_0, y_0), (x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$，找到一$n$次多项式$f(x)=a_0+a_1x+a_2x^2+...+a_nx^n$使得$\forall x_i, f(x_i) = y_i$等价于方程组：

$$ \left\{
\begin{aligned}
& f(x_0) = y_0\\
& f(x_1) = y_1\\
& ...\\
& f(x_n) = y_n\\
\end{aligned}
\right.\tag{4.6}$$

展开之，可使其变为一个系数矩阵是范德蒙矩阵的关于多项式系数向量$A$的线性方程组，且范德蒙矩阵在$x_i$互不相同时满秩. 

多项式系数向量$A$存在且唯一.

$\blacksquare$

我们对Lagrange多项式（虽然还没有完全提出它的求解方式）进行误差估计：

设置有若干节点$(x_i, y_i), i\in [0, n]$是$[a, b]$上的$(n+1)$个互异节点，若待拟合函数在$(a,b)$上$(n+1)$阶可导，即$f(x)\in \mathbb{C}^{(n+1)}[a,b]$，若我们用Lagrange多项式$\phi(x)$拟合之，则其插值余项（截断误差）可以表述为：

$$R_n(x) = f(x) - \phi(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}\omega_{n+1}(x)\tag{4.7}$$

其中$\omega_{n+1}(x)=\Pi_{i=0}^n(x - x_i)$，$\xi \in (a,b)$

直球使用微分中值定理即可**证明：**

由定义，$R(x) = f(x) - \phi(x)$，再由Lagrange多项式的基本假设（式4.6），有$\forall i, R(x_i) = 0$.

故可以等价地将$R(x)$转写为$R(x) = K(x)\omega_{n+1}(x)$，其中$\omega_{n+1}(x)=\Pi_{i=0}^n(x - x_i)$，$K(x)$是某未知的多项式，我们需要找到它：

使用惯用的套路构造辅助函数：

$$h(t) = R(t)-K(x)\omega_{n+1}(t)$$

观察到上述的函数有$(n+2)$个零点：$x, x_0, x_1, x_2, ..., x_n$，而我们事先约定这些零点都在$f(x)$的解析域$(a,b)$上（否则Lagrange多项式将失去意义）

对自变量$t$反复应用罗尔中值定理$(n+1)$次，则存在某一点$\xi \in (a,b)$，使得：

$$h^{(n+1)}(\xi) = R^{n+1}(\xi)-K(x)(n+1)! = 0$$

故得到$K(x) = \frac{R^{n+1}(\xi)}{(n+1)!}$，$\xi \in (a,b)$

而$R(x) = f(x) - \phi(x)$，$\phi(x)$是一个$n$次多项式，在$(n+1)$次求导后变为0，故

$$K(x) = \frac{f^{n+1}(\xi)}{(n+1)!}, \xi \in (a,b)$$

$\blacksquare$

式4.7的意义在于，若记$M = \max_{(a,b)}\vert f^{(n+1)}(x) \vert$，则Lagrange插值法的截断误差$R(x) \leqslant \frac{M}{(n+1)!}\omega_{n+1}(x)$，这体现了对误差的一种控制.

### Lagrange多项式的求解

最直观地，我们可以直接通过式$(4.6)$列方程组求解，但由于涉及到$n$次方运算，我们会得到一个范氏矩阵，而它往往是病态的，故考虑使用其它的方法求解.

事实上凡是要解方程求Lagrange多项式的，都会面临病态的问题，因此我们从方程形式上来考察它：

Lagrange多项式可以认为是一组线性无关的基$1, x, x^2, x^3, ..., x^n$线性组合而成的，因此我们可以构造另外一组与其同构的基来解决这一问题. 我们构造这样的一组基底：

$${l_i(x), i\in [0,n]}, s.t:l_i(x) = \frac{\prod_{j=0, j\not ={i}}^n(x-x_j)}{\prod_{j=0, j\not ={i}}^n(x_i-x_j)}$$

这组基满足一个特殊的性质：对于点集中的若干离散$x$，当且仅当$x = x_i$时$l_i(x)$为$1$，否则为$0$，故我们可以构造插值函数如下：

$$f(x) = \sum_{i = 0}^n y_il_i(x)\tag{4.8}$$

容易得知上述的函数满足Lagrange插值的基本假设（式4.6）

但是基于4.8的Lagrange插值法具有一明显的弊端：即一旦点集发生改变，各个基均需要进行修改，故我们希望找到另外一组基，使得即使点集发生了增减，我们也只需简单地增加或删除一项即可：

## Newton插值法

课本中对Newton插值的提法太过啰嗦，故给出一种额外的提法：

我们以一种逐步构建的方式来提出Newton插值. 

1 最初，我们的点集中只有$(x_0, y_0)$，因此我们给出插值式$f_0(x) = y_0$来拟合之. 

2 随后我们加入了点$(x_1, y_1)$，此时插值公式需要额外满足$f(x_1)=y_1$，根据Newton插值的思想，我们通过为$f_0(x)$附加一项来达成此目的：（$s.t.$在数学符号中指"subject to"，"使得"）

$$f_1(x) = y_0 + R_1(x),\ s.t. f_1(x_0) = y_0, f_1(x_1) = y_1$$

故$R_1(x)$一定满足：$R_1(x_0) = 0, R_1(x_1)=y_1-y_0$

故可以提出满足上述条件的$R_1(x)$较为简单形式：$R_1(x)=b_1(x-x_0)$，代入解出

$$b_1=\frac{y_1-y_0}{x_1 - x_0}$$

此时插值公式变为

$$f_1(x) = y_0 + \frac{y_1-y_0}{x_1 - x_0}(x-x_0)$$

3 继续加入点$(x_2, y_2)$，此时插值公式需要额外满足$f(x_2)=y_2$，故仍使用上述的方式：

$$f_2(x) = y_0 + \frac{y_1-y_0}{x_1 - x_0}(x-x_0) + R_2(x)$$

同理，设置$R_2(x)=b(x-x_0)(x-x_1)$，解出

$$b_2 = \frac{\frac{y_2-y_0}{x_2-x_0}-\frac{y_1-y_0}{x_1-x_0}}{x_2-x_1}$$

这个形式不够简洁，**经过简单的数学变换之后**（这种变换蕴藏着“均差(差商)的轮换对称性”，后述），可以化为

$$b_2 = \frac{\frac{y_2-y_1}{x_2-x_1}-\frac{y_1-y_0}{x_1-x_0}}{x_2-x_0}$$

我们发现它有着某种对称性：我们定义$f[a, b] = \frac{f(a)-f(b)}{a-b}$，则$b_2$可以转写为：

$$b_2=\frac{f[x_2, x_1]-f_[x_1,x_0]}{x_2-x_0}$$

这样的形式仍符合着某种更"高级"的对称性，故我们将$b_2$记为$f[x_0, x_1, x_2]$，表示先求"较为低级的"两个$f[x_2, x_1]$和$f[x_1, x_0]$，再将其作差，与外层的两个值之差$x_2-x_0$相除. 这样，我们递归地提出了均差(差商)的概念：

1. $f[a, b] = \frac{f(a) - f(b)}{a-b}$
2. $f[a, a_1, ..., b_1, b] = \frac{f[a,...,b_1]-f[a_1,...,b]}{a-b}$，"$...$"可以为空且$a_1, b_1$可以相同(即本式支持三个及以上项目)

容易证明，均差具有轮换对称性，即只要参数的组合一致，参数的排列与均差的值无关.

故Newton插值法可以直观地表述为：

$$f^*(x) = f(x_0) + (x-x_0)f[x_0,x_1] + (x-x_0)(x-x_1)f[x_0,x_1,x_2] + ...$$

分析地描述为：

$$f^*(x) = \sum_{i=0}^n l_i(x)\tag{4.9-1}$$

$$l_i(x) = f[x_0\ to.\  x_i]\prod_{j=0}^{i-1}(x-x_0)\tag{4.9-2}$$

请务必注意上述式子中连乘符号上的$(i-1)$.

### Newton插值的截断误差

在非节点的任意点$(x, f(x))$上，Newton插值的截断误差可以表述为：

$$R(x) = f^*(x) - f(x)$$

而$f(x)$不可直接求知，因此采用一巧妙的解法：以点$(x,f(x))$为第$(n+1)$个节点构造一新的Newton插值式$f_{(n+1)}(x)$，而此式可以精确反映$f(x)$的值. 则易知其差值：

$$R(x) = f[x_0\ to.\ x_n,x]\prod_{j=0}^{n}(x-x_0)$$

即为Newton插值的截断误差. 而对于一个点系的插值式是唯一的，因此结合Lagrange插值法可以导出：

$$f[x_0\ to.\ x_n,x]\omega_{n+1}(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}\omega_{n+1}(x)$$

其中$\xi$在点集${(x_i,y_i)}$连成的开区域内部，即

$$f[x_0\ to.\ x_n,x] = \frac{f^{(n+1)}(\xi)}{(n+1)!}$$

## 等距均差和等距节点的Newton插值

如果上述过程中的节点都是等距的，即$\forall i\in[0, n-1], x_i - x_{i+1}\equiv h$，则均差可以改写为如下的形式(证明是简单的)：

$$f[x_a, x_{a+1}, ..., x_{a+n}] = \frac{\Delta^n_{a+n, a} f}{n!h^n}$$

其中$\Delta^n f$被称为$n$阶差分，根据差分与均差在内容上的一致性，我们可以递归地描述差分：

1. $\Delta^1_{a+1, a}f =f(a+1) - f(a)$
2. $\Delta^n_{a+n, a}f=\Delta^{n-1}_{a+n-1, a}f-\Delta^{n-1}_{a+n, a+1}f$

至于所谓向前差分或向后差分，我们可以通过互换下角标顺序来描述(反正阶数又不会变)，又何必创造两个不同的符号呢？数学本来是简洁明快又可爱的，为什么要弄出这么多分裂的概念来让她变得啰嗦而丑陋呢？如是，我们即可重新以差分来改写节点等距情况下的Newton插值式：

$$f^*(x) = \sum_{i=0}^n l_i(x)\tag{4.10-1}$$

$$l_i(x) = \frac{\Delta^i_{i,0}f}{i!h^i} \prod_{j=0}^{i-1}(x-x_0)\tag{4.10-2}$$

直观地表述为

$$f^*(x) = f(x_0)+\frac{\Delta_{1,0}^1f}{1!h^1}(x-x_0)+\frac{\Delta_{2,0}^2f}{2!h^2}(x-x_0)(x-x_1)+\frac{\Delta_{3,0}^3f}{3!h^3}(x-x_0)(x-x_1)(x-x_2)$$

$$...$$

而在某些情况下，$x$会使用类似于等差数列的形式表示：$x=x_0+ht$，这时我们可以使用$t$作自变量，将上述的式子再度改写：

$$f^*(x_0+ht) = \sum_{i=0}^n l_i(t)\tag{4.11-1}$$

$$l_i(t) = \frac{\Delta^i_{i,0}f}{i!h^i} \prod_{j=0}^{i-1}(x_0+th-(x_0+jh))=\Delta^i_{i,0}f\frac{\prod_{j=0}^{i-1}(t-j)}{i!}\tag{4.10-2}$$

直观地表述为

$$f^*(x_0+ht) = f(x_0) + t\Delta_{1,0}^1f + \frac{t(t-1)}{2!}\Delta_{2,0}^2f+\frac{t(t-1)(t-2)}{3!}\Delta_{3,0}^3f+...$$

这一式子被称为向前插值公式. 代入求值时应注意自变量是$t$，若输入的是$x$则需相应的线性转化.

同理，若将$x$表示为$x=x_n+ht$，也可以得出类似的结果：

$$f^*(x_n+ht) = \sum_{i=0}^n l_i(t)\tag{4.11-1}$$

$$l_i(t) = \frac{\Delta^i_{0,i}f}{i!h^i} \prod_{j=0}^{i-1}(x_0+th-(x_0+jh))=\Delta^i_{0,i}f\frac{\prod_{j=0}^{i-1}(t-j)}{i!}\tag{4.10-2}$$

直观地表述为

$$f^*(x_0+ht) = f(x_0) + t\Delta_{0,1}^1f + \frac{t(t-1)}{2!}\Delta_{0,2}^2f+\frac{t(t-1)(t-2)}{3!}\Delta_{0,3}^3f+...$$

向前和向后插值都是基于式子(4.10)的，只不过对于$x$的等差数列表示不同. 

---
title: 数值分析05
tags: 
  - 数值分析
---

由于突发恶疾住院若干天，出院之后沉迷アトリ -My Dear Moments-和萨特《存在与虚无》，故本篇记录时隔一整月.

本篇继续讨论函数插值的问题

# 函数插值法

在前述的两种插值法中，我们都构造了一个函数，使得至少在各个节点处与潜在的函数$f(x)$同值. 但如果我们想要通过插值函数$H(x)$来刻画$f(x)$更加深刻的性质，如一阶导数，那么前述的两种插值方法将难以胜任.

## Hermite插值

我们试图找到某插值函数$H(x)$使得在$(n+1)$个点的点集上，有：

$$\forall i\in[0,n]: H(x_i)=y_i,H'(x_i)=y'_i\tag{5.1}$$

直观地，在$H(x)$中蕴含了更多的信息，这样一来就可能需要更多的自由度来承载这些信息，当然我们只有$(2n+2)$个约束条件，因此天然地，Hermite插值不可以也不会超过$(2n+1)$次.

### 基函数构造法

与在Lagrange插值法中作的工作相似，我们同样希望找到一组基，使得它满足这一问题中的要求：我们将上述的问题分开处理：

首先找到某一类函数$h_i(x)$，使得对于样本点集中的点$(x_j,y_j)$有：

$$h_i(x_j) = 0, j\not =i$$

$$h_i(x_j) = 1, j=i$$

同时满足：它的一阶导数在样本点集上恒为$0$：

$$h_i'(x_j) \equiv 0$$

这样，我们即可使用$h(x)$来控制插值函数的函数值而不影响其导数.

同样地，我们试图找到$H_i(x)$，使得对于样本点集中的点$(x_j,y_j)$有：

$$H'_i(x_j) = 0, j\not =i$$

$$H'_i(x_j) = 1, j=i$$

同时，它的原值应当在样本点集上恒为$0$：

$$H_i(x_j) \equiv 0$$

这样，我们即可使用$h(x)$来控制插值函数的一阶导数而不影响其原值.

幸运的是，数学家们创造性地发现了这组基的形式：

$$\left\{
\begin{aligned}
& h_i(x) = [a+b(x-x_i)]l_i^2(x) \\
& a = 1 \\
& b = -2l_i'(x_i)
\end{aligned}
\right.
$$

$$\left\{
\begin{aligned}
& H_i(x) = cl_i^2(x) \\
& c = 1
\end{aligned}
\right.
$$

其中，$l_i(x)$是关于$x_i$的Lagrange基：

$$l_i(x) = \frac{\prod_{j=0, j\not ={i}}^n(x-x_j)}{\prod_{j=0, j\not ={i}}^n(x_i-x_j)}$$

因此，在该方法中，每一个点-导数三元组$(x_i, y_i, y'_i)$生成一项

$$y_i[1-2l'_i(x_i)(x-x_i)]l_i^2(x)+y_i'(x-x_i)l_i^2(x)$$

由此可给出Hermite插值表达式：

$$H(x) = \sum_{i=0}^n \{y_i[1-2l'_i(x_i)(x-x_i)]l_i^2(x)+y_i'(x-x_i)l_i^2(x)\}$$

Hermite插值法拥有误差表达式

$$R_n(x) = \frac{f^{(2n+2)}(\xi)}{(2n+2)!}\omega_{n+1}^2(x)$$

这样的“函数值-导数”分别控制的方法有着更广阔的应用空间，如给出点集中部分拥有导函数值而其余没有，而这种形式的问题需要构造额外的基函数(如使用原基函数则待定参数数量过多)，而构造这一基函数的过程需要较强的分析要求，故不建议使用基函数构造法. (课后作业习题9涉及到这一方法，可以试着做一下)

对于上述这种“不完全”的情况，误差表达式会有所变化，但总得来说，如果给出了$(n+1)$组原值条件，其中有$m$组限制了导数值(不妨设它们就是第$0\to(m-1)$个)，则误差可以表示为

$$R_n(x) = \frac{f^{(n+m+2)}(\xi)}{(n+1+m)!}\omega_{n+1}(x)\prod^{(m-1)}_{i=0}(x-x_i)$$

### 待定系数法

这种方法直接将$H(x)$显式地假设出来，再代入点集解方程组(5.1). 严格地来讲这种算法不能称之为方法，但在数据量较小时很好用.

### 降阶法

而面对稍多的数据时，待定系数法就不那么好用了. 因此我们试图通过某种方式来减少需要处理的自由度数：

我们先通过前述的插值法求出仅在原函数值上提供保证的插值函数：

$$N(x), s.t.\forall i\in[0,n], N(x_i) = y_i$$

明显地，Hermite插值函数与$N(x)$存在如下的关系：

$$H(x)-N(x) = P(x)\prod_{i=0}^n(x-x_i)$$

而此时，由于$H(x)$是$2n+1$阶多项式，而$N(x)$是$n$阶多项式，故$H(x)-N(x)$是$2n+1$阶多项式，故$P(n)$是$n$阶多项式.

此时再利用$H'(x_i)=y_i$对$P(n)$待定系数直球求解即可.

## 样条插值法

### 样条

对于一水平弹性木条，在若干点$\{x_0,x_1,x_2,...,x_n\}$处施加垂直于其的外力，产生弯矩$M(x)$，由Euler–Bernoulli梁方程，每两点之间的弯矩$M(x)$与材料杨氏模量$E$、产生的惯性矩$I$与产生曲线的曲率$k(x)$有如下关系：

$$M(x)=EIk(x)=EI\frac{y''}{(1+y'^2)^{1.5}}$$

一般的梁难以发生大的弯曲，故$y'$接近于$0$，有：

$$M(x)\approx EIy''$$

而每两点之间没有额外的力，故产生的弯矩是线性的，故$y''$是一分段一次函数，故可以得知$y$是一拥有连续二阶导数的分段三次函数.

### 样条插值

因此设置第$i$点和第$(i+1),i\in[0,n]$点之间的函数表达式是$\phi_i=a_i+b_ix+c_ix^2+d_ix^3$，总共有$n$段曲线，产生$4n$个自由度. 接下来数数我们有什么条件：

1. 插值条件，在$i\in[0,n]$时$\phi(x_i)=y_i$，$(n+1)$个
2. 插值条件中有$(n-1)$个被两段曲线共用，属自由度的冗余，消除$(n-1)$个自由度(书上称为函数值连续条件)
3. 一阶导数连续，在$(n-1)$个共用点上$\phi_i'(x_{i+1})=\phi_{i+1}'(x_{i+1})$，$(n-1)$个
4. 二阶导数连续，在$(n-1)$个共用点上$\phi_i''(x_{i+1})=\phi_{i+1}''(x_{i+1})$，$(n-1)$个

如此，我们拥有$(4n-2)$个条件，明显不能解决$4n$个自由度的问题.

因此我们可以通过某些人为的限定来为其附加$2$个限定条件，而最佳的方式就是将其附加在未被连续性约束的两个边界点$x_0,x_n$上.

常用的方式有：

1. 令$\phi'(x_0)=\phi'(x_n)=0$
2. 令$\phi''(x_0)=\phi''(x_n)=0$，此时与工程中使用的样条之情况(边界弯矩为$0$)符合，称为自然三次样条

我们用自然三次样条为例，指出如何求解样条：

首先假定任意第$i\in[1,n-1]$段函数均有参数列表$a_i,b_i,c_i,d_i$，我们采取较为巧妙的"自底向上"方式以降低计算难度：

在每一段上，$\phi_i(x)$的二阶导数均是线性函数，易知在每一段上：

$$\phi_i''(x)=\phi''(x_i)\frac{x_{i+1}-x}{x_{i+1}-x_i}-\phi''(x_{i+i})\frac{x_i-x}{x_{i+1}-x_i}$$

为简化表达，我们约定新符号$h_i=x_{i+1}-x_i$，使用之前在“力学背景”中提到的弯矩$M_i$来代表$\phi(x_i)$：

$$\phi_i''(x)=M_i\frac{x_{i+1}-x}{h_i}-M_{i+1}\frac{x_i-x}{h_i}$$

为利用函数值条件，对上式执行两次积分，并适当调整积分常数的位置，得到：

$$\phi_i(x)=\frac{M_i}{6h_i}(x_{i+1}-x)^3-\frac{M_{i+1}}{6h_i}(x_i-x)^3+C(x-x_i)+D(x_{i+1}-x)$$

代入条件$\phi_i(x_i)=y_i,\phi_i(x_{i+1})=y_{i+1}$，解出上述积分常数：

$$\phi_i(x)=\frac{M_i}{6h_i}(x_{i+1}-x)^3-\frac{M_{i+1}}{6h_i}(x_i-x)^3+(\frac{y_{i+1}}{h_i}-\frac{M_{i+1}h_i}{6})(x-x_i)-(\frac{y_i}{h_i}-\frac{M_ih_i}{6})(x-x_{i+1})$$

有了这一方程，我们就只需求解出各个弯矩$M_i$，即可得解.

我们还有一个未用的条件，即$(n-1)$个导数的连续性条件：

对上述方程左右求导并令$\phi_{i-1}(x_i)=\phi_i(x_i)$，有

$$h_{i-1}M_{i-1}+2(h_i+h_{i-1})M_i+h_iM_{i+1}=\frac{6}{h_i}(y_{i+1}-y_i)-\frac{6}{h_i}(y_i-y_{i-1})$$

上述方程被称为三弯矩方程，其中$i\in[1,n-1]$. 我们令它是自然样条，即$M_0=M_n=0$，得到一方程组（为了简化表达，我们记$v_i=RHS$，$u_i=2(h_i+h_{i-1})$）：

$$\begin{pmatrix}
u_1 & h_1 &     &     &     &     &  \\
h_1 & u_2 & h_2 &     &     &     &   \\
& h_2 & u_3 & h_3 &     &     &   \\
\\
&     & \ddots & \ddots & \ddots  &     &  \\
&  &  & h_{n-3} & u_{n-2}  & h_{n-2} &  \\
&  &  &  & h_{n-2}  & u_{n-1} & h_{n-1}  \\
\end{pmatrix}
\begin{pmatrix}
M_1\\
M_2\\
M_3\\
\vdots\\
M_{n-2}\\
M_{n-1}\\
\end{pmatrix}=\begin{pmatrix}
v_1\\
v_2\\
v_3\\
\vdots\\
v_{n-2}\\
v_{n-1}\\
\end{pmatrix}
$$

使用前述的"追赶法"解决之，即可得到样条插值函数.

# 函数逼近

函数逼近问题给出一目标函数$f(x)$，希望提出一(容易处理的)近似函数$\phi(x)$来代替$f(x)$，以近似刻画并简化对$f(x)$的诸多运算.

而插值问题是给出一系列点$\{(x_i, y_i)\}$，希望提出一拟合函数$\phi(x)$以近似刻画这些点所隐含的函数关系$f(x)$.

## 函数逼近法的总体过程

1. 提出近似函数$\phi(x)$的形式并预留一些待定系数$\{a_0, a_1, a_2,...\}$
2. 使用某种误差估计手段写出误差$E$关于上述待定系数$\{a_0, a_1, a_2,...\}$的关系
3. 通过最小化误差$E$求出$\{a_0, a_1, a_2,...\}$

由于其有着便于计算的性质，我们通常使用多项式函数$\phi(x)=\sum_{i=0}^na_ix^i$来进行拟合，同时使用最小二乘法来描述误差.

在某些情况下，可能会将自变量$x$映射为其它的函数，如设置原型$y=a_0+a_1e^{x}+a_2e^{2x}$逼近，则可以对逼近点列进行变换如$(e^{x_0}, y_0)$.

## 最小二乘法

“最小二乘”是一种描述误差的尺度，即使用均方误差(假使在点集$P$上考察)，其"误差泛函"表示如下：（注：仅仅是形式不同的事物不需要讨论多次，故此处的求和符号可以为离散求和，也可以为连续积分）

$$E[\phi_A(x)]=\sum_{(x,y)\in P}(f(x) - \phi(x))^2$$

我们找出某种手段来最小化这个$E$即可. 幸运的是，我们在针对“形式固定-系数待定”的多项式的讨论中不需要面对上述令人生畏的泛函，而可以将其转写为：

$$E(a_0,a_1,a_2,...,a_n) = \sum_{(x,y)\in P}[y-(\sum_{i=0}^na_ix^i)]^2$$

特别地，这种最小二乘意义下的误差是一串平方和，这意味着它必定有一个最小值，以下简单**证明**（这证明是我自己写的，虽然还挺漂亮的，但是可能有逻辑断裂可以跳过不看）

假定有一$(n+1)$个相互独立的自变量之函数：$f(a_0,a_1,a_2,...,a_n)=\sum_{i=0}^na_i^2$

易知$\forall i \in[0,n], i=0$时函数$f$取得极值，而此时其Hessian矩阵为$diag(2,2,2,2,...,2)$，明显是正定的，因此它在此处取最小值.

我们设置$A=(a_0,a_1,a_2,...,a_n)$，那么对于任一和$A$无关的$(n+1)$个向量$x_i$与标量$y_i(i\in[0,n])$，我们产生一个新的$(n+1)$维向量$(y_0+Ax_0,y_1+Ax_1,y_2+Ax_2,...,y_n+Ax_n)$，易知这个向量仅仅是将向量$A$在其空间内平移、放缩而未旋转，未实际改变该空间的拓扑性质，因此$f$在新的自变量列$(y_0+Ax_0,y_1+Ax_1,y_2+Ax_2,...,y_n+Ax_n)$下仍拥有一唯一最小值点.

而此时，$f$恰可以被写为$f(a_0,a_1,a_2,...,a_n) = \sum_{(x,y)\in P}[y-(\sum_{i=0}^na_ix^i)]^2$

$\blacksquare$

因此这提示了我们最小二乘法的解决策略：对于误差函数$E(a_0,a_1,a_2,...,a_n) = \sum_{(x,y)\in P}[y-(\sum_{i=0}^na_ix^i)]^2$，我们求：

$$\left\{
\begin{aligned}
& \frac{\partial{E}}{\partial{a_0}} = -2\sum_{(x,y)\in P}(y-\sum_{j=0}^na_jx^j) \\
& \frac{\partial{E}}{\partial{a_1}} = -2\sum_{(x,y)\in P}(y-\sum_{j=0}^na_jx^j)x \\
& ... \\
& \frac{\partial{E}}{\partial{a_k}} = -2\sum_{(x,y)\in P}(y-\sum_{j=0}^na_jx^j)x^k \\
& ... \\
& \frac{\partial{E}}{\partial{a_n}} = -2\sum_{(x,y)\in P}(y-\sum_{j=0}^na_jx^j)x^n \\
\end{aligned}
\right.
$$

用更为通俗的方式展开之：

$$\left\{
\begin{aligned}
& \frac{\partial{E}}{\partial{a_0}} = -2\sum_{(x,y)\in P}[y-(a_0+a_1x+a_2x^2+...+a_nx^n)] \\
& \frac{\partial{E}}{\partial{a_1}} = -2\sum_{(x,y)\in P}[y-(a_0+a_1x+a_2x^2+...+a_nx^n)]x \\
& ... \\
& \frac{\partial{E}}{\partial{a_k}} = -2\sum_{(x,y)\in P}[y-(a_0+a_1x+a_2x^2+...+a_nx^n)]x^k \\
& ... \\
& \frac{\partial{E}}{\partial{a_n}} = -2\sum_{(x,y)\in P}[y-(a_0+a_1x+a_2x^2+...+a_nx^n)]x^n \\
\end{aligned}
\right.
$$

令其全部为$0$以找到最小值，整理得到方程组

$$\left\{
\begin{aligned}
& \sum_{(x,y)\in P}y = \sum_{(x,y)\in P}a_0+\sum_{(x,y)\in P}xa_1+\sum_{(x,y)\in P}x^2a_2+...+\sum_{(x,y)\in P}x^na_n \\
& \sum_{(x,y)\in P}xy = \sum_{(x,y)\in P}xa_0+\sum_{(x,y)\in P}x^2a_1+\sum_{(x,y)\in P}x^3a_2+...+\sum_{(x,y)\in P}x^{n+1}a_n \\
& ... \\
& \sum_{(x,y)\in P}x^ky = \sum_{(x,y)\in P}x^{k}a_0+\sum_{(x,y)\in P}x^{k+1}a_1+\sum_{(x,y)\in P}x^{k+2}a_2+...+\sum_{(x,y)\in P}x^{k+n}a_n \\
& ... \\
& \sum_{(x,y)\in P}x^ny = \sum_{(x,y)\in P}x^na_0+\sum_{(x,y)\in P}x^{n+1}a_1+\sum_{(x,y)\in P}x^{n+2}a_2+...+\sum_{(x,y)\in P}x^{2n}a_n \\
\end{aligned}
\right.
$$

写为矩阵形式则为：

$$MA=b$$

$$M = \begin{pmatrix}
\sum_{(x,y)\in P}1&\sum_{(x,y)\in P}x&\sum_{(x,y)\in P}x^2 & ... & \sum_{(x,y)\in P}x^n\\
\sum_{(x,y)\in P}x&\sum_{(x,y)\in P}x^2&\sum_{(x,y)\in P}x^3 & ... & \sum_{(x,y)\in P}x^{n+1}\\
\vdots & \vdots& \vdots &&\vdots \\
\sum_{(x,y)\in P}x^k&\sum_{(x,y)\in P}x^{k+1}&\sum_{(x,y)\in P}x^{k+2} & ... & \sum_{(x,y)\in P}x^{k+n}\\
\vdots & \vdots& \vdots &&\vdots \\
\sum_{(x,y)\in P}x^n&\sum_{(x,y)\in P}x^{n+1}&\sum_{(x,y)\in P}x^{n+2} & ... & \sum_{(x,y)\in P}x^{2n}\\
\end{pmatrix}$$

$$b = \begin{pmatrix}
\sum_{(x,y)\in P}y\\
\sum_{(x,y)\in P}xy\\
...\\
\sum_{(x,y)\in P}x^ky\\
...\\
\sum_{(x,y)\in P}x^ny
\end{pmatrix}$$

此方程组被称为**正则方程组**. 它依据求和方式的不同，可以有两种表示形式，分别对应在离散点集上逼近和连续点集上逼近两种情况：

1. 离散点集逼近：

$$M = \begin{pmatrix}
\sum_{i=0}^m1&\sum_{i=0}^mx&\sum_{i=0}^mx^2 & ... & \sum_{i=0}^mx^n\\
\sum_{i=0}^mx&\sum_{i=0}^mx^2&\sum_{i=0}^mx^3 & ... & \sum_{i=0}^mx^{n+1}\\
\vdots & \vdots& \vdots &&\vdots \\
\sum_{i=0}^mx^k&\sum_{i=0}^mx^{k+1}&\sum_{i=0}^mx^{k+2} & ... & \sum_{i=0}^mx^{k+n}\\
\vdots & \vdots& \vdots &&\vdots \\
\sum_{i=0}^mx^n&\sum_{i=0}^mx^{n+1}&\sum_{i=0}^mx^{n+2} & ... & \sum_{i=0}^mx^{2n}\\
\end{pmatrix}$$

$$b = \begin{pmatrix}
\sum_{i=0}^my\\
\sum_{i=0}^mxy\\
...\\
\sum_{i=0}^mx^ky\\
...\\
\sum_{i=0}^mx^ny
\end{pmatrix}$$

2. 连续区间逼近

$$M = \begin{pmatrix}
\int_a^b1dx&\int_a^bxdx&\int_a^bx^2dx & ... & \int_a^bx^ndx\\
\int_a^bxdx&\int_a^bx^2dx&\int_a^bx^3dx & ... & \int_a^bx^{n+1}dx\\
\vdots & \vdots& \vdots &&\vdots \\
\int_a^bx^kdx&\int_a^bx^{k+1}dx&\int_a^bx^{k+2}dx & ... & \int_a^bx^{k+n}dx\\
\vdots & \vdots& \vdots &&\vdots \\
\int_a^bx^ndx&\int_a^bx^{n+1}dx&\int_a^bx^{n+2}dx & ... & \int_a^bx^{2n}dx\\
\end{pmatrix}$$

$$b = \begin{pmatrix}
\int_a^bf(x)dx\\
\int_a^bxf(x)dx\\
...\\
\int_a^bx^kf(x)dx\\
...\\
\int_a^bx^nf(x)dx
\end{pmatrix}$$

## 非多项式逼近，一般函数族

在上述的讨论中，我们设置逼近函数为$\phi(x)=\sum_{i=0}^na_ix^i$，即我们使用了函数族$\{1,x,x^2,...,x^n,...\}$的某一线性组合来逼近目标函数$f(x)$，且逼近点集上的各个点有着相同的"重要程度". 

我们将这个过程推广开来：我们使用一般的函数族$\{\phi_0(x), \phi_1(x), \phi_2(x),...,\phi_n(x),...\}$来刻画目标函数$f(x)$，且不同的逼近锚点$(x,y)$拥有不同的非负非全零(实际上权函数有严格的约束，但我们可以理解为非负非全零(反正不会考))的"重要性"权函数$\omega(x)$，那么，我们设置逼近函数为$\phi(x)=\sum_{i=0}^na_i\phi_i(x)$，在带权点集$P$下，其"最小二乘"尺度下的误差可以写作：

$$E(a_0,a_1,a_2,...,a_n) = \sum_{(x,y)\in P}\omega(x)[y-(\sum_{i=0}^na_i\phi_i(x))]^2$$

（推导过程简单，略）为了能使矩阵变得好看一点，我们简记$(A, B)=\sum_{(x,y)\in P}\omega(x)A(x)B(x)$则正则方程组可以写作

$$MA=b$$

$$M = \begin{pmatrix}
(\phi_0, \phi_0) & (\phi_0, \phi_1) & (\phi_0, \phi_2) & ...&(\phi_0, \phi_n)\\
(\phi_1, \phi_0) & (\phi_1, \phi_1) & (\phi_1, \phi_2) & ...&(\phi_1, \phi_n)\\
\vdots & \vdots& \vdots &&\vdots \\
(\phi_i, \phi_0) & (\phi_i, \phi_1) & (\phi_i, \phi_2) & ...&(\phi_i, \phi_n)\\
\vdots & \vdots& \vdots &&\vdots \\
(\phi_n, \phi_0) & (\phi_n, \phi_1) & (\phi_n, \phi_2) & ...&(\phi_n, \phi_n)\\
\end{pmatrix}$$

$$b = \begin{pmatrix}
(\phi_0, f)\\
(\phi_1, f)\\
...\\
(\phi_2, f)\\
...\\
(\phi_n, f)\\
\end{pmatrix}$$

为了使得上述方程恰定，矩阵$M$须满秩，这对函数族提出了要求($iff$指“当且仅当”)：

$$\sum_{i=0}^na_i\phi_i(x)=0\ \ iff.\forall i\in[0,n],a_i=0$$

这一性质被称为函数族"线性无关".

### 正交函数族

解线性方程组是痛苦的，故我们对上述方程组可以作出更进一步的幻想：“如果矩阵$M$是一个对角矩阵就再好不过了”. 这样的幻想要求函数族$\phi_i$和权函数$\omega(x)$具有这样的性质：

$$\forall i,j\in[0,n]: i\not ={j}\iff\sum_{(x,y)\in P}\omega(x)\phi_i(x)\phi_j(x)=0$$

$$\forall i\in[0,n]: \sum_{(x,y)\in P}\omega(x)\phi_i(x)\phi_i(x)=a_i>0$$

如有此性质，则称函数族$\{\phi_i\}$在域$P$上关于权函数$\omega(x)$正交. 易知一正交函数族所导出的正则方程组之系数矩阵必满秩，故正则方程组必有唯一解.

特别地，若对于任意的$i$，上述$a_i=1$，则称其为**正交-归一化函数**族. 正交-归一化函数族不仅在函数逼近中出现，在诸多领域中均有应用(如量子化学中的本征波函数)

既然正交函数族如此好用，那么是否可以将设置出的简单的非正交函数族如$\{x^i\}$转化为正交函数族呢？

#### 构造正交函数族，Gram-Schmidt正交化方法

有一线性无关函数族$\{\phi_i(x)\}$，重构一新的函数族$\{\Phi_i(x)\}$：

$$\Phi_0(x) = \phi_0(x)$$

$$\Phi_i(x) = \begin{vmatrix}
(\phi_0, \phi_0) & (\phi_0, \phi_1) & ... & (\phi_0, \phi_{i-1}) & \phi_0(x)\\
(\phi_1, \phi_0) & (\phi_1, \phi_1) & ... & (\phi_1, \phi_{i-1}) & \phi_1(x)\\
\vdots & \vdots&  &\vdots&\vdots \\
(\phi_i, \phi_0) & (\phi_i, \phi_1) & ... & (\phi_i, \phi_{i-1}) & \phi_i(x)\\
\end{vmatrix}$$

此时

$$(\Phi_i, \Phi_j)=
\left\{
\begin{aligned}
& 0,i\not ={j}\\
& \Delta_i, i=j
\end{aligned}
\right.
$$

其中

$$\Delta_i = \begin{vmatrix}
(\phi_0, \phi_0) & (\phi_0, \phi_1) & ... & (\phi_0, \phi_{i-1}) & (\phi_0, \phi_{i})\\
(\phi_1, \phi_0) & (\phi_1, \phi_1) & ... & (\phi_1, \phi_{i-1}) & (\phi_1, \phi_{i})\\
\vdots & \vdots&  &\vdots&\vdots \\
(\phi_i, \phi_0) & (\phi_i, \phi_1) & ... & (\phi_i, \phi_{i-1}) & (\phi_i, \phi_{i})\\
\end{vmatrix}$$

课本151页给出了Gram-Schmidt正交化方法的一应用实例，在153页起给出了若干常用的正交函数族.

---
title: 数值分析06
tags: 
  - 数值分析
---

本篇也比较摸，因为突然喜欢上了东京爱乐乐团(这个团的德九味儿是真的正！)和武満徹老师的管弦作品. 听了好长时间，忘记了数学.

推荐武満徹先生的交响-念白作品「系図 ‐若い人たちのための音楽詩‐」，一说20世纪严肃音乐始于斯特拉文斯基“春之祭”，终于武満徹“族谱 -为年轻的人们而作的音乐诗-”，系図的这种无调性-调性语言交织的音乐语言体系提供了相当充足的感染力和戏剧性，以支持和传达其中的充盈的情感和完整的叙事故事. 为符合年轻人们的受众定位(现在年轻人们很少能听明白严肃音乐了)，作曲家特地加上了富有日本近现代文学色彩的念白(我是国内的第一个翻译者-.-). 羡慕，只可惜没考上央音作曲系，也没交响现场可以听，只能被迫听唐山研究院难听到死的不知道基于什么通俗流行改编的上课铃.

废话说多了. 本篇讨论数值微积分.

# 数值微分

数值微分问题求一函数$f(x)$在其解析域内的某点$x_0$处的近似一阶导数$f'(x_0)$，常用的方法是用一简单插值函数来代替$f(x)$求导

## 插值法数值微分

我们在原函数$f(x)$上找到$n>1$个点，据其构造一插值函数$\phi(x)$，以$\phi'(x_0)$来代替$f'(x_0)$

**误差分析**：在点集上的$n$次插值余项可写作：

$$R_n(x) = f(x) - \phi(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}\omega_{n+1}(x)\tag{4.7}$$

其中$\omega_{n+1}(x)=\Pi_{i=0}^n(x - x_i)$，$\xi \in (a,b)$

对上式求导并将近似求导点$x=x_i$代入其中：

$$f'(x_i) - \phi'(x_i) = \frac{f^{(n+1)}(\xi)}{(n+1)!}\prod_{j=0,j\not ={i}}^n(x_i-x_j)\tag{6.1}$$

我们以$n=2,3$时的情形简要分析：

### 两点公式

$n=2$时的情况. 取插值锚点$x_0,x_1$，要求$f(x)$在$(x_0,x_1)$上至少$1$阶可导，使用直线$\phi(x) = f(x_0)+\frac{f(x_1)-f(x_0)}{x_1-x_0}(x-x_0)$代替求导，得出结果为

$$f'^*(x_0) = \frac{f(x_1)-f(x_0)}{x_1-x_0}\tag{6.2}$$

$x_1$不一定在$x_0$的左或者右，因此我们可以统一化书本上的“向前差商”和“向后差商”为上述一个式子.

**误差分析**，对$f(x)$在$x_0$处执行级数展开，有

$$f(x)=f(x_0)+f'(x_0)(x-x_0)+o(x-x_0)$$

分别代入$x=x_0,x_1$得：

$$f'^*(x_0)=\frac{f(x_1)-f(x_0)}{x_1-x_0}=f'(x_0)+o(x-x_0)$$

即，两点公式产生一$o(x-x_0)$误差. 若$f(x)$在该邻域内至少二阶可导，则展开式可拥有一Lagrange余项：

$$f'^*(x_0)=\frac{f(x_1)-f(x_0)}{x_1-x_0}=f'(x_0)+f''(\xi)(x_1-x_0),\ \xi\in(x_0,x_1)or(x_1,x_0)$$

即，产生一$f''(\xi)(x_1-x_0)$误差.

### 三点公式

三点公式之"三点"的选择可能出现不同的位置，故为了方便讨论，我们不妨假设选择的三个点距离相同，且仍记为$x_0,x_1=x_0+h,x_2=x_0+2h(h>0)$，后我们在不同的位置求导.

容易得知，上述三点的Lagrange插值多项式是

$$\phi(x)=f(x_0)\frac{(x-x_1)(x-x_2)}{2h^2}+f(x_1)\frac{(x-x_0)(x-x_2)}{-h^2}+f(x_2)\frac{(x-x_0)(x-x_1)}{2h^2}$$

对其求导有

$$\phi'(x)=f(x_0)\frac{2x-x_1-x_2}{2h^2}+f(x_1)\frac{2x-x_0-x_2}{-h^2}+f(x_2)\frac{2x-x_0-x_1}{2h^2}$$

对于不同的数值微分位置，有：

1. $x=x_0$，$\phi'(x)=\frac{1}{2h}[-3f(x_0)+4f(x_1)-f(x_2)]$
2. $x=x_1$，$\phi'(x)=\frac{1}{2h}[-f(x_0)+f(x_2)]$，中心差商公式
3. $x=x_2$，$\phi'(x)=\frac{1}{2h}[f(x_0)-4f(x_1)+3f(x_2)]$

其截断误差可以由(6.2)表示出.

除此之外，样条也可以用作估计数值微分.

# 数值积分

数值积分求一函数在其连续域$[a,b]$内的积分近似值.

## 矩形近似

正如任何哲学家或科学家在千百年来所作之艰苦卓绝的探索那样，我们首先将我们不会求解的问题抽象为简单的问题——即使这种抽象是不精确的，因此我们想到在之前提出的积分中值定理：

$$\int_b^af(x)dx=f(\xi)(a-b),\xi\in[b,a]$$

我们可以在$[a,b]$内自由选择$\xi$的值以近似估计，这一式子实际上是用一矩形来近似代替积分，因此称为矩形公式. 特殊地，若$\xi=a,b,\frac{a+b}{2}$时，上式被称为左矩形公式、右矩形公式和中矩形公式.

矩形近似本质上是用零次插值函数$\phi(x)=c$来代替$f(x)$. 更一般的讨论：

## 插值近似

我们使用对$f(x)$上的若干点进行$n$次插值，得到插值函数$\phi(x)$，以$\int_b^a\phi(x)dx$来代替$\int_b^af(x)dx$.

易知$n$次插值公式的截断误差为$\int_b^a\frac{f^{(n+1)}(\xi)}{(n+1)!}\omega_{n+1}dx$

特别地，$n=1$时，是以$x$轴、$x=a$、$x=b$、直线$(a,f(a)),(b,f(b))$围出的梯形面积代替积分值，写作：

$$\int_b^af(x)dx\approx \int_b^a\phi(x)dx=\frac{a-b}{2}[f(a)+f(b)]\tag{6.3}$$

式子6.3被称为**梯形公式**.

继续讨论：$n=2$是，是以$x$轴、$x=a$、$x=b$、抛物线$\phi(x)$围成的曲边梯形面积代替积分值，而抛物线$\phi(x)$由点$(b,f(b)),(\frac{a+b}{2},f(\frac{a+b}{2})),(a,f(a))$确定，由Lagrange插值法可以写成：

$$\phi(x)=f(b)l_0(x)+f(\frac{a+b}{2})l_1(x)+f(a)l_2(x)$$

其中$l_i(x)$是第$i$点的Lagrange基.

积分易得：

$$\int_b^a\phi(x)dx\approx\frac{1}{6}[f(b)+4f(\frac{a+b}{2})+f(a)](a-b)\tag{6.3}$$

式子6.4被称为**抛物线公式**(Simpson公式).

彻底一般化，我们用$n$次插值公式来近似，并将积分区间$[a,b]n$等分，得到$(n+1)$个等分点$x_i,i\in[0,n]$，则近似的积分式有如下形式：

$$\int_b^a\phi(x)dx\approx(a-b)\sum_{i=0}^nC_if(x_i)$$

上述公式被称为$n$阶Newton-Cotes积分公式，$C_i$是一$(n+1)$项的系数族，每一族的总和为$1$，其系数只与阶数$n$有关.

1. $n=1$时，$C=(\frac{1}{2},\frac{1}{2})$
2. $n=2$时，$C=(\frac{1}{6},\frac{4}{6},\frac{1}{6})$
3. $n=3$时，$C=(\frac{1}{8},\frac{3}{8},\frac{3}{8},\frac{1}{8})$，Simpson'$\frac{3}{8}$公式
4. $n=4$时，$C=(\frac{7}{90},\frac{32}{90},\frac{12}{90},\frac{32}{90},\frac{7}{90})$，Cotes公式

$N\leqslant7$时，Newton-Cotes积分公式数值稳定.

### 误差估计

#### 代数精确度

在上述的讨论中，我们使用插值函数$\phi(x)$来近似$f(x)$以求得积分. 不难想象，$n$次插值函数可以完全精确刻画任意$n$次的原函数$f(x)$，因此在这种情况下，由这个插值函数产生的积分估算方法$S(a,b,f)$将能精确求出任意$n$次的原函数$f(x)$的任意定积分，故我们称这种估算方法$S(a,b,f)$具有$n$次的代数精确度.

有时我们需要评价一积分估算方法$S(a,b,f)$的代数精确度，即可以令$f_i(x)={1,x,x^2,x^3,...}$，对于逐个$f_i$，判断$S(a,b,f_i)$是否严格与$\int_b^af(x)dx$相等，能够相等的最高$x$次数即为估算方法$S(a,b,f_i)$的代数精确度.

**易知，$n$阶的Newton-Cotes积分公式至少有$n$次代数精确度. 且可以证明，$2k$阶的Newton-Cotes积分公式至少有$(2k+1)$阶代数精度**(证明提示：$y=x^{2n+1}$具有中心对称性).

#### Newton-Cotes积分方法的误差估计

Newton-Cotes积分方法基于插值法，而插值多项式具有余项：

$$R_n(x) = f(x) - \phi(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}\omega_{n+1}(x)\tag{4.7}$$

其中$\omega_{n+1}(x)=\Pi_{i=0}^n(x - x_i)$，$\xi \in (a,b)$

式4.7两侧同时积分：

$$\int_b^a f(x)dx - \int_b^a\phi(x)dx=\int_b^a\frac{f^{(n+1)}(\xi)}{(n+1)!}\omega_{n+1}(x)dx$$

需要注意，此处的$f^{(n+1)}(\xi)$属与$x$相关的量，不可直接作为常数提出积分表达式外. 因此对于$n=1$的梯形公式情况，由积分中值：

$$\int_b^a\frac{f''(\xi)}{2}(x-a)(x-b)dx = \frac{f''(\eta)}{2}\int_b^a(x-a)(x-b)dx = -\frac{(a-b)^3}{12}f''(\eta),\eta\in[b,a]$$

此为梯形公式的误差

使用更为复杂的方式可以得到Simpson公式的误差：

$$R(a,b,f)=-\frac{(a-b)^5}{2880}f^{(4)}(\eta)\eta\in[b,a]$$

在对Newton-Cotes积分方法的误差估计中，我们发现：即使是最简单的梯形公式之误差，也与积分区间长度$b-a$的三次方成正比，故我们想到：是否可以通过缩减单次积分的长度，将整个区间上的积分化为若干小区间上积分之和来解决问题呢？

## 复化积分

经过上述的启发，我们将函数$f(x)$的积分域$[a,b]$**等分**为$n$个小区间，各区间长度为$h$，在每一段上分别使用插值法求积，再将其加和.

### 复化梯形公式

对于复化区间$[x_{n-1},x_n]$，梯形公式写作：

$$\int_{x_{n-1}}^{x_n}f(x)dx\approx \frac{1}{2}(x_n-x_{n-1})[f(x_n)+f(x_{n-1})]=\frac{h}{2}[f(x_n)+f(x_{n-1})]$$

则全域的积分可以表示为：

$$\int_b^af(x)dx=\frac{h}{2}[f(x_0)+f(x_n)+2\sum_{i=1}^{n-1}f(x_i)]\tag{6.4}$$

上式称为复化梯形公式. 误差分析：

在每一段上，复化梯形公式产生误差：

$$R_i = -\frac{h^3}{12}f''(\xi_i),\xi_i\in(x_{i-1},x_i)$$

则总的误差为各段误差之和：

$$R = \sum_{i=1}^n[-\frac{h^3}{12}f''(\xi_i)]=\sum_{i=1}^n[-\frac{nh^3}{12}\frac{f''(\xi_i)}{n}]$$

由离散的中值定理(均值公式)：

$$R = -\frac{h^2}{12}(a-b)f''(\eta),\eta\in(b,a)$$

### 复化Simpson公式

对于内部有一额外点$x'_n$的复化区间$[x_{n-1},x_n]$，Simpson公式写作：

$$\int_{x_{n-1}}^{x_n}f(x)dx\approx\frac{1}{6}[f(x_{n-1})+4f(x'_n)+f(x_n)](a-b)$$

故全域的积分可以写作

$$\int_b^af(x)dx=\frac{h}{2}[f(x_0)+f(x_n)+2\sum_{i=1}^{n-1}f(x_i)+4\sum_{i=1}^nf(x'_i)]\tag{6.5}$$

类似地，复化Simpson公式拥有误差

$$R = -\frac{h^4}{180}(a-b)f^{(4)}(\eta),\eta\in(b,a)$$

### 复化积分的逐次分半算法

假设在实际计算中，我们需要将误差控制至某一范围，我们又知道，区间越细分则误差越小，故我们可以逐个增多区间数$n$来搜索出满足误差要求的答案. 但容易知道，在$n$足够大时，$n$每增加$1$所导致的$h$减小之幅度很小，故我们考虑每次将$n$翻一倍.

而目前为止，每一次取到新的$n$，我们都必须重新计算一遍复化公式，这是很低效的，因此我们以梯形公式为例，考察$n$与$2n$等分的梯形公式$T_n$和$T_{2n}$的关系：

$$T_n=\frac{h}{2}[f(x_0)+f(x_n)+2\sum_{i=1}^{n-1}f(x_i)]$$

$$T_{2n}=\frac{h}{4}[f(x_0)+f(x_n)+2\sum_{i=1}^{2n-1}f(x_i)]$$

作如下计算可得：

$$T_{2n}-\frac{1}{2}T_n=\frac{h}{2}\sum_{x\in A} f(x)$$

其中$A$为所有新增点的点集.

如此可以方便求出分半后的新梯形公式值：

$$T_{2n}=\frac{1}{2}T_n+\frac{h}{2}D$$

其中$D$为所有新增点函数值之和.

逐次分半法有另一好处，即可以仅凭借估计值求得误差. 故这种误差估计被称为**后验误差估计**.

#### 逐次分半算法-后验误差估计

我们有一种直觉：将$n$翻一倍时，我们距离真实值的误差会定量变化，因此考虑$n$与$2n$等分的梯形公式$T_n$和$T_{2n}$所产生的误差（此处为方便，记真实的积分值为$I$）：

$$R_n = I - T_n=-h^2\frac{a-b}{2}f''(\eta_1),\eta_1\in(a,b)$$

$$R_{2n} = I - T_{2n}=-(\frac{h}{2})^2\frac{a-b}{2}f''(\eta_2),\eta_2\in(a,b)$$

我们回忆$f''(\eta)$是由均值公式而来，而当$n$充分大时，$f''(\eta)$可以被认为能较为精确刻画$f''(x)$在$[b,a]$上的均值，因此$f''(\eta_1)\approx f''(\eta_2)$：

$$R_n\approx4R_{2_n}$$

即：

$$\frac{I-T_n}{I-T_{2n}}\approx4$$

得到$I-T_{2n}\approx\frac{1}{3}(T_{2n}-T_n)$

如此得到了可以在实际中使用的误差估计法.

同样，Simpson公式也有后验的误差序列：

$$I-S_{2n}\approx\frac{1}{15}(S_{2n}-S_n)$$

#### 逐次分半加速，Romberg求积公式

由梯形公式的后验误差估计$I-T_{2n}\approx\frac{1}{3}(T_{2n}-T_n)$，我们可以得到$I\approx\frac{1}{3}(4T_{2n}-T_n)$

这提示我们，通过计算$\frac{1}{3}(4T_{2n}-T_n)$可能会得出更精确的解.

我们以$n=4$的情形观察：

$$T_8=\frac{a-b}{16}[y_0+y_8+2(y_1+y_2+y_3+y_4+y_5+y_6+y_7)]$$

$$T_4=\frac{a-b}{8}[y_0+y_8+2(y_2+y_4+y_6)]$$

则$I\approx\frac{1}{3}(4T_8-T_4)=\frac{a-b}{8}[y_0+y_8+4(y_1+y_3+y_5+y_7)+2(f_2+f_4+f_6)]$

注意到上式等号右侧恰为$n=8$情形下的复化Simpson公式$S_8$.

实际上，可以证明$S_{2n}=\frac{1}{3}(4T_{2n}-T_n)$，即$2n$和$n$分割的两复化梯形值可以通过线性组合转化为复化Simpson值. 而这个转化过程将误差由$O(h^2)$缩小到$O(h^4)$

同样地，有复化Cotes值：$C_{2n}=\frac{1}{15}(16S_{2n}-S_n)$，误差$O(h^8)$，复化Romberg值$R_{2n}=\frac{1}{63}(64C_{2n}-C_n)$，$O(h^{16})$. 如此可无限外推.

这一过程提示我们，可以通过组合两"较低级的"复化估计值来生成一"较高级的"的复化估计值，以缩减误差. 

## Gauss求积公式

以上，我们完成了对基于等距节点插值多项式的数值积分的讨论，我们发现，以$n$次插值多项式拟合积分，最多能提供$(n+1)$次代数精度，直觉告诉我们这个代数精度是不令人满意的，因此我们试图找到一更高代数精度是估计方法.

对于某带权函数(不带权也无所谓)的代数精确条件可以一般化描述如下：

$$\int_b^a\rho(x)f(x)dx=\sum_{i=1}^nA_if(x_i)\tag{6.6}$$

对于$n$个插值点，上式有$2n$个待定系数($n$个$A_i$，$n$个$x_i$)，因此分别取$f(x)=\{1,x,x^2,...,x^{2n-1}\}$代入其中，得到方程组(注意：这一方程组并非简单的线性方程组，写为矩阵形式仅为表示方便)(实际上还有基于算符的表示，但是理解起来不那么直观)：

$$\begin{pmatrix}
1&1&1&...&1\\
x_1&x_2&x_3&...&x_n\\
x_1^2&x_2^2&x_3^2&...&x_n^2\\
\vdots&\vdots&\vdots&&\vdots\\
x^{2n-1}_1&x^{2n-1}_2&x^{2n-1}_3&...&x^{2n-1}_n\\
\end{pmatrix}
\begin{pmatrix}
A_1\\
A_2\\
A_3\\
\vdots\\
A_n\\
\end{pmatrix}=
\begin{pmatrix}
\int_b^a\rho(x)dx\\
\int_b^a\rho(x)xdx\\
\int_b^a\rho(x)x^2dx\\
\vdots\\
\int_b^a\rho(x)x^{2n-1}dx\\
\end{pmatrix}
\tag{6.7}$$

$2n$条关系和$2n$个自由度，一定能得到一组$x_i$和$A_i$的解. 这表明$n$次多项式最高可以拥有$(2n-1)$次代数精度.

解出的这组${x_i}$称为**Gauss点**，将$x_i$和$A_i$代入(6.6)得到的公式被称为**Gauss求积公式**，Gauss求积公式的求法就是解上述的方程(6.7).

但这样的方程是非线性方程，而且体量庞大，故我们希望得到更简单的求解法，至少可以求出Gauss点集.

### 正交多项式求法

可以证明，对于估算方法$\sum_{i=0}^nA_if(x_i)$，其节点族$\{x_i\}$是Gauss点当且仅当$\omega(x)=\prod_{i=1}^n(x-x_i)$与任意$(n-1)$次多项式在$[a,b]$上关于权函数正交.

因此我们可以构造(可用正交化方法)一个与任意$(n-1)$次多项式正交的$n$次多项式$\omega(x)$，解出$\omega(x)=0$的$n$个根，即为Gauss点集.

在Gauss点集求出后，于方程(6.7)中抽取若干$n$行求出系数族$A_i$

### 特殊的Gauss求积公式

(考这一部分多少有点不人道主义)

#### Gauss-Legendre求积公式

考察在$[-1,1]$上的任意权为$1$、锚点数为$n$的积分：

$$\int_{-1}^1f(x)dx=\sum_{i=1}^nA_if(x_i)$$

1. $n=1$时，$x_1=0;A_1=2$，则$\int_{-1}^1f(x)dx\approx2f(0)$
2. $n=2$时，$x=\plusmn\frac{1}{\sqrt{3}};A_1=A_2=1$，则$\int_{-1}^1f(x)dx\approx f(-\frac{1}{\sqrt{3}})+f(\frac{1}{\sqrt{3}})$
3. $n=3$时，$x_{1,3}=\plusmn\sqrt{\frac{3}{5}},x_2=0;A_1=A_3=\frac{5}{9},A_2=\frac{8}{9}$

上述公式系列被称为Gauss-Legendre求积公式，更高次数的Gauss-Legendre求积公式参数可以查表得知.

#### Gauss-Chebyshev求积公式

考察在$[-1,1]$上的任意权为$\rho(x)=\frac{1}{\sqrt{1-x^2}}$、锚点数为$n$的积分：

$$\int_{-1}^1\frac{1}{\sqrt{1-x^2}}f(x)dx=\sum_{i=1}^nA_if(x_i)$$

其Gauss点系为$x_i=\cos\frac{2i-1}{2n}\pi$，求积系数恒为$A=\frac{\pi}{n}$

#### Gauss-Laguerre求积公式

考察在$[0,+\infin)$上的任意权为$\rho(x)=e^{-x}$、锚点数为$n$的积分，

其Gauss点系为$L(x)=e^x\frac{d^n}{dx^n}(e^{-x}x^n)$的零点，求积系数为$A_i=\frac{(n!)^2}{x_i[L_n'(x_i)]^2}$

#### Gauss－Hermite求积公式

考察在$(-\infin,+\infin)$上的任意权为$\rho(x)=e^{-x^2}$、锚点数为$n$的积分，

其Gauss点系为$H(x)=(-1)^ne^{x^2}\frac{d^n}{dx^n}(e^{-x^2})$之零点，求积系数为$A_i=\frac{2^{n+1}n!\sqrt{\pi}}{[H'_n(x_i)]^2}$

---
title: 数值分析07
tags: 
  - 数值分析
---

本篇讨论非线性方程和非线性方程组的数值解.

# 非线性方程的数值解

## 二分查找法

对于方程$f(x)=0$，其奇数重根$x_i$有如下的性质：存在$x_i$的某个邻域$(a,b)$，使得邻域内分别在零点$x_i$左和右的两点$l\in(a,x_i),r\in(x_i,b)$有$f(l)f(r)<0$，所以我们只需保证$f(l)f(r)<0$，每次将$l$或$r$替换为$\frac{l+r}{2}$，直到误差达到要求即可.

二分查找法一定收敛，但收敛速度较慢. 但二分查找法较为依赖初始值的选取，且无法处理偶数重根.

## 迭代法

### 不动点迭代

如果一函数$g(x)$满足在$x^*$处$g(x^*)=x^*$，那么我们就可以尝试使用迭代格式

$$x_{n+1}=g(x_n)\tag{7.1}$$

来找到这个$x^*$. 但这个迭代格式在某区间$[a,b]$上不一定能够收敛，于是我们试图寻找使它收敛的条件：

首先，在这个区间$[a,b]$内方程$g(x)=x$应有根，即$\phi(x)=g(x)-x$满足：$\phi(a)\phi(b)<0$

据此可得出两种互斥情况：

1. $g(a)>a;g(b)<b$
2. $g(a)<a;g(b)>b$

其次，无论$x$，这一迭代法每迭代一次所产生的误差应小于上一次产生的误差，即：

$$\vert x_{n+1}-x^*\vert < \vert x_n-x^*\vert$$

或称：

$$\vert g(x_n)-g(x^*)\vert < \vert x_n-x^*\vert$$

重写为：

$$\frac{\vert g(x_n)-g(x^*)\vert}{\vert x_n-x^*\vert} < 1$$

据此可得出$g(a)-g(b)<a-b$，与上述情况2矛盾.

因此可以得到数值分析版本的**压缩映像原理**(在泛函分析中这一原理有更准确和泛化的定义)：

**定理8.1** 若$g(x)$在区间$[a,b]$上满足条件：

1. $\forall x\in[a,b],a\leqslant g(x)\leqslant b$
2. $\exist 0<L<1,\forall x,y\in[a,b],s.t.\vert g(x)-g(y)\vert \leqslant L\vert x-y\vert$

则初值在$[a,b]$上的迭代格式$x_{n+1}=g(x_n)$收敛于$x^*=g(x^*)$

在实际中我们很少对于区间内每两点都找到一个$L$来验证上述第2条条件，故压缩映像原理有另一种提法，即对于第2条条件，我们令$y\to x$，则：

**定理8.2** 若$g(x)$在区间$[a,b]$上满足条件：

1. $\forall x\in[a,b],a\leqslant g(x)\leqslant b$
2. $\forall x\in[a,b],s.t.\vert g'(x)\vert<1$

则初值在$[a,b]$上的迭代格式$x_{n+1}=g(x_n)$收敛于$x^*=g(x^*)$

然而我们还是觉得麻烦，验证在区间$[a,b]$上处处导数小于$1$还算是麻烦，故我们又对它换了一种提法：

**定理8.3** 若$g(x)$在$x^*$的某一邻域内连续可导(注意：连续可导指的是"连续地可导"，指导函数连续，而不是"函数连续且函数可导")，而$x^*=g(x^*)$，若$g'(x^*)<1$，则一定存在某个$x^*$的邻域，使得$x_{n+1}=g(x_n)$收敛于$x^*$.

#### 迭代的收敛速度

假使一迭代序列$\{x_n\}$收敛于$x^*$，且有如下关系：

$$\lim_{n\to\infin}\frac{\vert x_{n+1}-x^*\vert}{\vert x_n-x^*\vert^r}=C\tag{7.2}$$

**则称迭代序列$\{x_n\}$是$r$阶收敛的**. 这一阶数刻画了在$x^*$附近估计误差缩小的速度，故可以刻画序列收敛于$x^*$的速度.

但式$7.2$的形式看上去就不方便使用，实际工程中更是难以凭借它来估算收敛速度，而这个$r$次方给我们一种提示：是否可以使用级数展开方法，使得这个$r$与导数联系起来？

我们对满足$g(x^*)=x^*$的$g(x)$在$x^*$处执行Taylor展开并代入$x_n$：

$$g(x_n)=g(x^*)+g'(x^*)(x_n-x^*)+...+\frac{g^{(r-1)}}{(r-1)!}(x_n-x^*)^{r-1}+\frac{g^{(r)}(\xi)}{r!}(x_n-x^*)^r$$

即：

$$x_{n+1}-x^*=g'(x^*)(x_n-x^*)+...+\frac{g^{(r-1)}}{(r-1)!}(x_n-x^*)^{r-1}+\frac{g^{(r)}(\xi)}{r!}(x_n-x^*)^r=a(x_n-x^*)^r$$

而基$\{1,x_n-x^*,(x_n-x^*)^2,...\}$线性无关，因此可以得出：

$$g'(x^*)=g''(x^*)=...=g^{(r-1)}(x^*)=0\not ={g^{(r)}(x^*)}\tag{7.3}$$

式子(7.2)与(7.3)完全等价，因此可以使用式子(7.3)来计算收敛阶数：**第一个在解处非零的导数是几阶，收敛阶数就是几阶.**

### 函数值迭代

#### 简单迭代法

故基于压缩映像原理之扩展(定理8.3)，我们可以将方程$f(x)=0$经过同解转化，化为能够收敛的$g(x)=x$的形式来迭代求解. 这种转化不仅仅是两边同时加一个$x$，可能还需要基于其导函数对其调整，使其根周围的导数小于$1$，故在此之前，可以先使用二分查找法缩小根的范围.

简单迭代法收敛较慢，且大部分情况下是一阶收敛(线性收敛)，不能满意.

#### Steffensen加速

在$n$充分大时，有关系：

$$\frac{x_n-x^*}{x_{n+1}-x^*}\approx\frac{x_{n+1}-x^*}{x_{n+2}-x^*}$$

解出

$$x^*=x_n-\frac{(x_{n+1}-x_n)^2}{x_{n+2}-2x_{n+1}+x_n}$$

我们令$x_{n+3}=x^*$，即

$$x_{n+3}=x_n-\frac{(x_{n+1}-x_n)^2}{x_{n+2}-2x_{n+1}+x_n}\tag{7.4}$$

即得Steffensen加速法原理. 它至少是2阶收敛的.

在Steffensen加速的运算过程中，我们首先使用一般迭代法求出$x_1,x_2,x_3$，后直接持续使用式子$7.4$直到进入误差允许范围.

### 插值迭代

插值迭代的主要思想是用线性方程或简单的多项式方程近似非线性方程，逐步逼近其解.

#### Newton迭代法

Newton迭代法使用$f(x)$在$x_n$处的切线与$x$轴的交点作$x_{n+1}$. 即：

$$x_{n+1}=g(x_n)=x_n-\frac{f(x_n)}{f'(x_n)}\tag{7.5}$$

Newton法同样满足$g(x^*)=x^*$

**收敛速度：**易求得$g'(x^*)=0$，故Newton法至少$2$阶收敛.

但Newton法对重根是线性收敛的，因此有如下改进手段：

1. $x_{n+1}=x_n-r\frac{f(x_n)}{f'(x_n)}$，$r$为根的重数
2. 使用同解方程$\phi(x)=\frac{f(x)}{f'(x)}=0$的根代替$f(x)$的根

#### 弦截法

弦截法基于Newton法，使用差商$\Delta f(x)/\Delta x$来代替式(7.5)中的导数：

$$x_{n+1}=x_n-\frac{f(x_n)(x_n-x_{n-1})}{f(x_n)-f(x_{n-1})}\tag{7.6}$$

它需要两个初值$x_0,x_1$，一般取确定有解区间的两端点.

#### muller法(抛物线法)

抛物线法是三点的弦截法. 使用过$x_n,x_{n-1},x_{n-2}$三点的抛物线$\phi(x)$的，在$x_{n-2}\to x_{n}$方向上的零点作$x_{n+1}$. 

# 非线性方程组的数值解

对于非线性方程组：

$$\left\{
\begin{aligned}
&f_1(x_1,x_2,x_3,...,x_n) = 0 \\
&f_2(x_1,x_2,x_3,...,x_n) = 0 \\
&f_3(x_1,x_2,x_3,...,x_n) = 0 \\
&...\\
&f_n(x_1,x_2,x_3,...,x_n) = 0
\end{aligned}
\right.
\tag{7.7}$$

有两种解法：其一是将其在某一点线性展开为超平面，以这个切平面来近似这个方程；其二是将其转化为同解的、具有最小值$0$的非线性函数，再通过梯度下降求出其最值.

## Newton-Raphson法，线性化求解

不难得出，对于$f_i(x_1,x_2,x_3,...,x_n)=0$，以其在点$p=(x_{p1},x_{p2},x_{p3},...,x_{pn})$处的切平面代替之可有：

$$f(x_1,...,x_n)=f_i(p)+(\frac{\partial f_i}{\partial x_1})_p(x_1-x_{p1})+(\frac{\partial f_i}{\partial x_2})_p(x_2-x_{p2})+...+(\frac{\partial f_i}{\partial x_n})_p(x_n-x_{pn})$$

为了方便求解，我们将所有的方程都在同一点$p$处展开，故我们可以使用方程组：

$$\left\{
\begin{aligned}
&f_1(p)+(\frac{\partial f_1}{\partial x_1})_p(x_1-x_{p1})+(\frac{\partial f_1}{\partial x_2})_p(x_2-x_{p2})+...+(\frac{\partial f_1}{\partial x_n})_p(x_n-x_{pn}) = 0 \\
&f_2(p)+(\frac{\partial f_2}{\partial x_1})_p(x_1-x_{p1})+(\frac{\partial f_2}{\partial x_2})_p(x_2-x_{p2})+...+(\frac{\partial f_2}{\partial x_n})_p(x_n-x_{pn}) = 0 \\
&f_3(p)+(\frac{\partial f_3}{\partial x_1})_p(x_1-x_{p1})+(\frac{\partial f_3}{\partial x_2})_p(x_2-x_{p2})+...+(\frac{\partial f_3}{\partial x_n})_p(x_n-x_{pn}) = 0 \\
&...\\
&f_n(p)+(\frac{\partial f_n}{\partial x_1})_p(x_1-x_{p1})+(\frac{\partial f_n}{\partial x_2})_p(x_2-x_{p2})+...+(\frac{\partial f_n}{\partial x_n})_p(x_n-x_{pn}) = 0
\end{aligned}
\right.
$$

来代替上述的方程组. 用矩阵的形式表示则为：

$$\begin{pmatrix}
\frac{\partial f_1}{\partial x_1}&\frac{\partial f_1}{\partial x_2}&\frac{\partial f_1}{\partial x_3}&...&\frac{\partial f_1}{\partial x_n}\\
\frac{\partial f_2}{\partial x_1}&\frac{\partial f_2}{\partial x_2}&\frac{\partial f_2}{\partial x_3}&...&\frac{\partial f_2}{\partial x_n}\\
\frac{\partial f_3}{\partial x_1}&\frac{\partial f_3}{\partial x_2}&\frac{\partial f_3}{\partial x_3}&...&\frac{\partial f_3}{\partial x_n}\\
\vdots&\vdots&\vdots&&\vdots\\
\frac{\partial f_n}{\partial x_1}&\frac{\partial f_n}{\partial x_2}&\frac{\partial f_n}{\partial x_3}&...&\frac{\partial f_n}{\partial x_n}\\
\end{pmatrix}
\begin{pmatrix}
x_1-x_{p1}\\
x_2-x_{p2}\\
x_3-x_{p3}\\
\vdots\\
x_n-x_{pn}\\
\end{pmatrix}=
\begin{pmatrix}
f_1(x_0,x_1,...,x_n)\\
f_2(x_0,x_1,...,x_n)\\
f_3(x_0,x_1,...,x_n)\\
\vdots\\
f_n(x_0,x_1,...,x_n)\\
\end{pmatrix}
\tag{7.8}$$

第一项偏导数矩阵为数学分析学中Jacobi矩阵.

将上述式子变形：

$$\begin{pmatrix}
\frac{\partial f_1}{\partial x_1}&\frac{\partial f_1}{\partial x_2}&\frac{\partial f_1}{\partial x_3}&...&\frac{\partial f_1}{\partial x_n}\\
\frac{\partial f_2}{\partial x_1}&\frac{\partial f_2}{\partial x_2}&\frac{\partial f_2}{\partial x_3}&...&\frac{\partial f_2}{\partial x_n}\\
\frac{\partial f_3}{\partial x_1}&\frac{\partial f_3}{\partial x_2}&\frac{\partial f_3}{\partial x_3}&...&\frac{\partial f_3}{\partial x_n}\\
\vdots&\vdots&\vdots&&\vdots\\
\frac{\partial f_n}{\partial x_1}&\frac{\partial f_n}{\partial x_2}&\frac{\partial f_n}{\partial x_3}&...&\frac{\partial f_n}{\partial x_n}\\
\end{pmatrix}
(\begin{pmatrix}
x_1\\
x_2\\
x_3\\
\vdots\\
x_n\\
\end{pmatrix}-
\begin{pmatrix}
x_{p1}\\
x_{p2}\\
x_{p3}\\
\vdots\\
x_{pn}\\
\end{pmatrix})=
\begin{pmatrix}
f_1(x_0,x_1,...,x_n)\\
f_2(x_0,x_1,...,x_n)\\
f_3(x_0,x_1,...,x_n)\\
\vdots\\
f_n(x_0,x_1,...,x_n)\\
\end{pmatrix}
\tag{7.8'}$$

简记为$J(x-x_0)=F$，则可以直接构建迭代格式

$$x_{n}=x_{n-1}J^{-1}F\tag{7.9}$$

## 梯度下降法求解

易知，上述的非线性方程组(7.7)与方程：

$$\sum_{i=1}^n f^2(x_1,x_2,x_3,...,x_n)=0$$

同解(平方和为零当且仅当各项均为零)

故正如我们在炼丹炉中所作的工作那样，我们可以设置损失函数$E=\sum_{i=1}^n f^2(x_1,x_2,x_3,...,x_n)=0$，对其执行梯度下降算法求出解.

不会考. 因为人类很难求这种东西的梯度.

---
title: 数值分析08
tags: 
  - 数值分析
---

本来不太想看第九章的（

本章讨论一阶常微分方程的数值解.

# 一阶常微分方程的数值解

对于一个附带了初值条件(否则将无法定义$y$的截距)的一阶常微分方程：

$$\left\{
\begin{aligned}
&\frac{dy}{dx}=f(x,y) \\
&y(a)=a_0
\end{aligned}
\right.
\tag{8.1}$$

求函数$y$在若干点$x_0,x_1,...,x_n$的函数值$y_0,y_1,...,y_n$

我们假定方程右侧的多项式$f(x,y)$是连续的，且关于$y$满足比连续更为高级的条件Lipschitz连续条件(收缩映射)：

$$\vert f(x,a)-f(x,b)\vert\leqslant L\vert a-b\vert$$

这样，$y$存在单解(具体的证明可以参看常微分方程和实分析)

## 一阶近似法

既然我们只需要在离散的点集上进行求算，故上述方程便允许一定程度上的信息损失，而这种近似操作的首选目标便是微分或积分算子：

### 导数的差商近似

我们使用"向前的"差商来近似方程8.1中的导数项，有

$$\frac{y_{i+1}-y_i}{x_{i+1}-x_i}=f(x_i,y_i)\tag{8.2}$$

故由式子(8.2)初值$y_0=a_0$可迭代求出所有的$y_i$

本质上是一种基于Taylor公式的线性近似，称为Euler方法

### 积分的数值积分近似

上述方程还可以写作：

$$dy=f(x,y)dx$$

两侧积分：

$$y_{i+1}=\int_{x_i}^{x_{i+1}}f(x,y)dx + y_i$$

再以矩形公式近似上述积分值：

$$y_{i+1}=({x_{i+1}}-{x_i})f(x_i,y_i) + y_i$$

故由式子(8.2)初值$y_0=a_0$可迭代求出所有的$y_i$

## 高阶近似法

### Euler方法校正：梯形近似

上述对于微积分的近似始终只基于单点函数值来预测，这导致了精度的下降，因此我们尝试使用两点的函数值来预测，即使用梯形公式给出一迭代公式(为方便讨论，不妨设待求节点是距离为$h$的等距点)：

$$y_{n+1}=y_n+\frac{h}{2}[f(x_n,y_n)+f(x_{n+1},y_{n+1})]\tag{8.3}$$

直观地，式8.3无法直接拿来使用，因为等式的右侧也出现了$y_{n+1}$，而在待求$y_{n+1}$的时候我们明显无法先验地知晓$y_{n+1}$的值.

故我们先行一次普通Euler法，将其解集记为$\vec{y}$，再将它于式子8.3中执行$1$次即得改进的解.