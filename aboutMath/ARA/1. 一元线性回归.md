# 一元线性回归模型

样本观测值：$(x_1,y_1), (x_2,y_2),\cdots,(x_n,y_n)$

样本回归模型：
$$
\begin{aligned}
y_i=\beta_0+\beta_1x_i+\varepsilon_i\\
\begin{cases}
E(\varepsilon_i)=0\\
var(\varepsilon_i)=\sigma^2
\end{cases}
\end{aligned}
$$

对上式两边分别求数学期望和方差得

回归方程 $E(y_i)=\beta_0+\beta_1x_i,\ var(y_i)=\sigma^2$

**经验回归方程** $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$

# 参数$\beta_0,\beta_1$的估计

## 普通最小二乘估计
(Ordinary Least Square Estimation， 简记为OLSE)

### 推导

==important==

最小二乘法就是寻找参数$β_0,β_1$的估计值使离差平方和达到极小

$$
\begin{aligned}
\min S_{\epsilon^2}=&\sum(y_i-\hat{y}_i)^2\\
\\
Q(\hat{\beta}_0,\hat{\beta}_1)=&\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)^2\\
=&\min\limits_{\beta_0,\beta_1}\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)^2
\end{aligned}
$$

这里，称$\hat{y}_i=\hat{\beta}_0-\hat{\beta}_1x_i$为$y_i$的**回归拟合值**，简称为回归值或拟合值；称$e_i=y_i-\hat{y}_i$为$y_i$的**残差**

要求残差平方和最小，可以使偏导为0：
$$
\begin{cases}
\frac{ \partial Q }{ \partial \beta_0}\bigg|_{\beta_0 = \hat{\beta}_0}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)=0\\
\frac{ \partial Q }{ \partial \beta_1}\bigg|_{\beta_1 = \hat{\beta}_1}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)x_i=0
\end{cases}
$$

可以得出
$$
\begin{cases}
\sum\limits_{i=1}^ne_i=0\\
\sum\limits_{i=1}^ne_ix_i=0
\end{cases}
$$

整理后，得正规方程组
$$
\begin{cases}
n\hat{\beta}_0+(\sum\limits_{i=1}^nx_i)\hat{\beta_1}=\sum\limits_{i=1}^ny_i\\
(\sum\limits_{i=1}^nx_i)\hat{\beta}_0+(\sum\limits_{i=1}^nx_i^2)\hat{\beta_1}=\sum\limits_{i=1}^nx_iy_i
\end{cases}
$$

则最后可以得出OLSE：
$$
\begin{cases}
\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}\\
\hat{\beta}_1=\frac{\sum\limits_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}
\end{cases}
$$

为了方便表示，这里可以记$\begin{aligned}L_{xx}&=\sum\limits_{i=1}^n(x_i-\bar{x})^2=\sum\limits_{i=1}^nx_i^2-n(\bar{x})^2\\ L_{xy}&=\sum\limits_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})=\sum\limits_{i=1}^nx_iy_i-n\bar{x}\bar{y}\end{aligned}$

那么就有：
$$
\begin{cases}
\hat{\beta_0}=\bar{y}-\hat{\beta}_1\bar{x}\\
\hat{\beta_1}=L_{xy}／L_{xx}
\end{cases}
$$

### 性质

#### 线性

易知 $\hat{\beta}_1=\frac{\sum\limits_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}$ 可以等价表示为 $\hat{\beta}_1=\frac{\sum\limits_{i=1}^n(x_i-\bar{x})y_i}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}$
则有：
$$
\begin{aligned}
\hat{\beta}_1&=\frac{\sum\limits_{i=1}^n(x_i-\bar{x})y_i}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}\\
\\
&=\sum\limits_{i=1}^n\frac{x_i-\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}y_i
\end{aligned}
$$

这里 $\sum\limits_{i=1}^n\frac{x_i-\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}$ 是常数，所以$\hat{\beta}_1$是$y_i$的线性组合；同理，可以证明$\hat{\beta}_0$是$y_i$的线性组合：
$$
\begin{aligned}
\hat{\beta}_0&=\bar{y}-\hat{\beta}_1\bar{x}\\
&=\frac{\sum\limits_{i=1}^ny_i}{n}-\sum\limits_{i=1}^n\frac{x_i-\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}y_i\bar{x}\\
&=\sum\limits_{i=1}^n\left(\frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}\right)y_i
\end{aligned}
$$

这里的$\sum\limits_{i=1}^n\left(\frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}\right)$是常数

#### 无偏性

指估计值的期望等于被估计参数的真实值

> [!TIP] 前提：
> $$
> \begin{aligned}
> &E(y_i)=\beta_0+\beta_1x_i\\
> \\
> &\sum(x_i-\bar{x})=0\\
> \\
> &\sum(x_i-\bar{x})x_i=\sum(x_i-\bar{x})^2
> \end{aligned}
> $$

由$\hat{\beta}_1$的式子可以得
$$
\begin{aligned}
E(\hat{\beta}_1)=&\sum\limits_{i=1}^n\frac{x_i-\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}E(y_i)\\
=&\sum\limits_{i=1}^n\frac{x_i-\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}(\beta_0+\beta_1x_i)\\
=&\beta_0\frac{\sum\limits_{i=1}^n(x_i-\bar{x})}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}+\beta_1\frac{\sum\limits_{i=1}^n(x_i-\bar{x})x_i}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}\\
=&\beta_0\frac{0}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}+\beta_1\frac{\sum\limits_{i=1}^n(x_i-\bar{x})^2}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}\\
=&\beta_1
\end{aligned}
$$

同理，可证$\hat{\beta}_0$是$\beta_0$的无偏估计

#### 有效性

!!! 感觉没必要写推导过程，过几天再来看看，不行删了

> 一个估计量是无偏的，只揭示了估计量优良性的一个方面。我们通常还关心估计量本身的波动情况，这就需要进一步研究它的方差。

> [!TIP] 前提：
> 方差性质：$var(aX+bY)=a^2var(X)+b^2var(Y)$
> 
> $$
> y_i=\beta_0+\beta_1x_i+\varepsilon_i
> $$
> $$
> var(y_i)=var(\varepsilon_i)=\sigma^2
> $$
> $\hat{\beta}_0$在线性推导中有$\hat{\beta}_0=\sum\limits_{i=1}^n\left(\frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}\right)y_i$

那么，有

$$
\begin{aligned}
var(\hat{\beta}_1)&=\sum\limits_{i=1}^n\left[\frac{x_i-\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}\right]^2var(y_i)\\
&=\frac{\sum\limits_{i=1}^n(x_i-\bar{x})^2}{\left[\sum\limits_{j=1}^n(x_j-\bar{x})^2\right]^2}\sigma^2\\
&=\frac{\sigma^2}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}
\end{aligned}
$$

与
$$
\begin{aligned}
var(\hat{\beta}_0)&=var\left(\sum\limits_{i=1}^n\left(\frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}\right)y_i\right)\\
&=\sigma^2\cdot\sum\limits_{i=1}^n\left(\frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}\right)^2\\
&=\left[\frac{1}{n}+\frac{(\bar{x})^2}{\sum(x_j-\bar{x})^2}\right]\sigma^2
\end{aligned}
$$

综上，有$\hat{\beta}_0,\hat{\beta}_1$的正态分布
$$
\begin{aligned}
\hat{\beta}_0&\sim N(\beta_0,(\frac{1}{n}+\frac{(\bar{x})^2}{L_{xx}})\sigma^2)\\
\hat{\beta}_1&\sim N(\beta_1,\frac{\sigma^2}{L_{xx}})
\end{aligned}
$$

还可以得到$\hat{\beta}_0,\hat{\beta}_1$的协方差：
$$
cov(\hat{\beta}_0,\hat{\beta}_1)=-\frac{\bar{x}}{L_{xx}}\sigma^2
$$
它说明，在$\bar{x}=0$时，$\hat{\beta}_0$与$\hat{\beta}_1$不相关，在正态假定下两者相应独立；在$\bar{x}\ne0$时，不独立。它揭示了回归系数间的关系。

#### 最佳线性无偏估计
(Best Linear Unbiased Estimator, BLUE)

$$
\begin{cases}
E(\varepsilon_i)=0,\quad i=1,2,\cdots,n\\
cov(\varepsilon_i,\varepsilon_j)=\begin{cases}
\sigma^2,\quad i=j\\
0,\quad i\ne j
\end{cases}
\quad i,j=1,2,\cdots,n
\end{cases}
$$

以上条件（Gauss~Markov条件）下可以证明$\hat{\beta}_0,\hat{\beta}_1$分别是$\beta_0,\beta_1$的最佳线性无偏估计(BLUE)，也称为最小方差线性无偏估计。BLUE 即指在$\beta_0,\beta_1$的一切线性无偏估计中，它们的方差最小。

## 最大似然估计
(Maximum Likelihood Estimation, MLE)

最大似然法就是用已知的样本标记结果，反推最大概率能使结果出现的模型参数。

设(环境)参数$\theta$与结果$x$，有$p(x|\theta)$：已知参数，推导结果；相反的，有似然函数$L(\theta|x)$已知结果，反推参数。

所以，最大似然估计应在一切$\theta$中选取使随机样本$(X_1,X_2,\cdots,X_n)$落在点$(x_1,x_2,\cdots,x_n)$附近点概率最大的$\hat{\theta}$为未知参数$\theta$真值的估计值，即选取$\theta$满足
$$
L(\hat{\theta};x_1,x_2,\cdots,x_n)=\max_{\theta}L(\theta;x_1,x_2,\cdots,x_n)
$$

这里的话，函数是用$x_i$来求$\theta$

#### 对于一元线性回归的最大似然估计

在假设$\varepsilon_i\sim N(0,\sigma^2)$时，$y_i$有服从如下正态分布：
$$
y_i\sim N(\beta_0+\beta_1x_i,\sigma^2)
$$

$y_i$的密度分布为
$$
f_i(y_i)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2\sigma^2}[y_i-(\beta_0+\beta_1x_i)]^2\right\},\qquad i=1,2,\cdots,n
$$

那么有$y_1,y_2,\cdots,y_n$的似然函数
$$
\begin{aligned}
L(\beta_0,\beta_1,\sigma^2)&=\prod\limits_{i=1}^nf_i(y_i)\\
&=(2\pi\sigma^2)^{\frac{n}{2}}\exp\left\{-\frac{1}{2\sigma^2}[y_i-(\beta_0+\beta_1x_i)]^2\right\}
\end{aligned}
$$

因为$L$的极大化和$\ln(L)$的极大化是等价的，所以取对数似然函数为
$$
\ln(L)=-\frac{n}{2}\ln(2\pi\sigma^2)-\frac{1}{2\sigma^2}[y_i-(\beta_0+\beta_1x_i)]^2
$$

那么，要求上式的极大值，等价于求$\sum\limits_{i=1}^n[y_i-(\beta_0+\beta_1x_i)]^2$的极小值，到这里就与OLSE相同
$$
\begin{cases}
\frac{\partial\ln(L)}{\partial\beta_0}=\frac{\partial \sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)^2}{\partial\beta_0}=\frac{ \partial Q(\beta_0,\beta_1) }{ \partial \beta_0}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)=0\\
\frac{\partial\ln(L)}{\partial\beta_1}=\frac{ \partial Q(\beta_0,\beta_1) }{ \partial \beta_1}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)x_i=0
\end{cases}
$$

此外，由最大似然估计还可以得到$\sigma^2$的估计值，令
$$
\frac{\partial\ln(L)}{\partial\sigma^2}=0
$$

有
$$
\frac{n}{2\pi}=\frac{\sum\limits_{i=1}^n[y_i-(\beta_0+\beta_1x_i)]^2}{\sigma^2}
$$

最后可以得到$\sigma^2$的有偏估计值
$$
\begin{aligned}
\hat{\sigma}^2&=\frac{1}{n}\sum\limits_{i=1}^n[y_i-(\beta_0+\beta_1x_i)]^2\\
&=\frac{1}{n}\sum\limits_{i=1}^ne_i^2
\end{aligned}
$$

实际应用中，常用无偏估计量
$$
\hat{\sigma}^2=\frac{1}{n-2}\sum\limits_{i=1}^n[y_i-(\beta_0+\beta_1x_i)]^2
$$

# 回归方程的显著性检验

当我们找到了一个经验回归方程后，还不能马上用于实际，仍然需要进行检验。在对回归方程进行检验时，通常需要做正态性假设$\varepsilon \sim N(0,\sigma^2)$。

检验有原假设：
$$
H_{0}: \beta_{1}=0
$$

与对立假设：
$$
H_{1}: \beta_{1} \ne 0
$$

回归系数的显著性检验就是检验自变量$x$对因变量$y$的影响是否显著。若是原假设成立，就说明$x$与$y$没有线性关系，无显著性影响；也就是说要检验$x$与$y$是否有线性关系，就要拒绝原假设。

##  t 检验

为了检验显著性，可以先构造一个$t$统计量
$$
\begin{align}
t&=\frac{最小二乘估计值}{标准差的样本估计值}\\ \\
&=\frac{\hat{\beta}_{1}}{\sqrt{ \hat{\sigma}^{2}/L_{xx} }}=\frac{\hat{\beta}_{1}\sqrt{ L_{xx} }}{\hat{\sigma}}
\end{align}
$$

需要注意的是，式中的$\hat{\sigma}^{2}$是$\sigma^{2}$的无偏估计；这里称$\hat{\sigma}$为回归标准差。

若要原假设$H_{0}: \beta_{1}=0$成立，所构造的$t$统计量服从自由度为$n-2$的$t$分布，给定显著性水平$\alpha$，双侧检验的临界值为$t_{\frac{\alpha}{2}}$（一连串的很头疼，意思就是要查表）。

也就是说，当$\lvert t\rvert \ge t_{\frac{\alpha}{2}}$时，拒绝原假设$H_{0}: \beta_{1}=0$，认为$\beta_{1}$显著不为零，$x$与$y$有线性关系，因变量$y$对自变量$x$的一元线性回归成立；相反，当$\lvert t \rvert <t_{\frac{\alpha}{2}}$时，不拒绝原假设，认为$\beta_{1}$为零，一元线性回归不成立。

上面这段，就是说要先求出$t$统计量，再查表找出$t_{\frac{\alpha}{2}}$，最后进行比较。这个方法需要查表，比较麻烦，所以我们可以使用计算$P$值的方法来判断是否需要拒绝原假设：
$$
P(\lvert t \rvert > \lvert t值 \rvert )=P值
$$
$P值$的意义是得到的结果是巧合的概率，越小，就说明越不可能是巧合。

根据$t$分布的性质已知$|t值|$越大， $P值$越小； $|t值|$越小，$P值$越大。因此，对于给定的显著性水平$\alpha$，当$P值<\alpha$时，拒绝原假设。

## F 检验

$F$检验是根据平方和分解式，直接从回归效果检验回归方程的显著性。

平方和分解式：
$$
\sum\limits_{i=1}^{n}(y_{i}-\bar{y})^{2}=\sum\limits_{i=1}^{n}(\hat{y}_{i}-\bar{y})^{2}-\sum\limits_{i=1}^{n}(y_{i}-\bar{y}_{i})^{2}
$$
其中：
- 总离差平方和：$SST(Sum\ of\ Squares\ for\ Total)=\sum\limits_{i=1}^{n}(y_{i}-\bar{y})^{2}=L_{yy}$
- 回归平方和：$SSR(Sum\ of\ Squares\ for\ Regression)=\sum\limits_{i=1}^{n}(\hat{y}_{i}-\bar{y})^{2}$
- 残差平方和：$SSE(Sum\ of\ Squares\ for\ Error)=\sum\limits_{i=1}^{n}(y_{i}-\bar{y}_{i})^{2}$

![SST=SSR+SSE](https://365datascience.com/resources/blog/2018-11-image8-5-1024x495.jpg)

可以阅读该网站：[Sum of Squares: SST, SSR, SSE | 365 Data Science](https://365datascience.com/tutorials/statistics-tutorials/sum-squares/)以全面理解。

SST反映了因变量的波动程度（不确定性），将SST拆成SSR和SSE，可以发现SSR由自变量的波动引起，而SSE则由自变量之外的因素引起。所以**回归平方和SSR越大，回归的效果越好**，据此可以构造$F$统计量：
$$
F=\frac{SSR/1}{SSE/(n-2)}
$$

当原假设成立时，$F$统计量服从自由度为$(1,n-2)$的$F$分布($F\sim F(1,n-2)$)。当$F值 \ge F_{\alpha}$时，拒绝原假设，说明回归方程显著。

当然，也可以根据$P$值做检验，具体检验过程可以放在方差分析表中进行：

| 方差来源 | 自由度 | 平方和 | 均方      | F 值                        | P 值           |
| -------- | ------ | ------ | --------- | --------------------------- | -------------- |
| 回归     | 1      | SSR    | $SSR/1$   | $F=\frac{SSR/1}{SSE/(n-2)}$ | $P(F>F值)=P值$ |
| 残差     | $n-2$  | SSE    | $SSE/n-2$ |                             |                |
| 总和     | $n-1$  | SST    |           |                             |                |

^f6f9b4

## 相关系数的显著性检验

因为一元线性回归方程讨论的是变量$x$与变量$y$间的线性关系，那么也可以用变量$x$与$y$间的相关系数来检验回归方程的显著性。

那么，我们有$x$与$y$的简单相关系数，简称相关系数：
$$
\begin{align}
r & = \frac{\sum\limits_{i=1}^n (x_{i}-\bar{x})(y_{i}-\bar{y})}{\sqrt{ \sum\limits_{i=1}^n (x_{i}-\bar{x})^{2}\sum\limits_{i=1}^n (y_{i}-\bar{y})^{2} }}\\ \\
 & =\frac{L_{xy}}{\sqrt{ L_{xx}L_{yy} }}=\hat{\beta}_{1} \sqrt{ \frac{L_{xx}}{L_{yy}} }
\end{align}
$$

相关系数表示$x$与$y$线性关系的密切程度，取值范围为$\lvert r \rvert\le 1$。

这里可以发现，几种检验具有相通之处，如相关系数的检验也可以用统计量：
$$
t=\frac{\sqrt{ n-2 }r}{\sqrt{ 1-r^{2} }}\quad \sim t(n-2)
$$

又如，有拟合优度（决定系数）：
$$
R^{2}=\frac{SSR}{SST}=r^{2}
$$

所以，可以进行三种检验的关系的验证：见[验证三种检验的关系](1.%20一元线性回归.md#验证三种检验的关系)

最后，记总体相关系数为$\rho$，样本相关系数$r$是$\rho$的估计值

两变量间相关程度的强弱分为以下几个等级： 
- 当$|\rho|\ge0.8$时，视为高度相关；
- 当$0.5\le|\rho|＜0.8$时，视为中度相关；
- 当$0.3\le|\rho|＜0.5$时，视为低度相关；
- 当$|\rho|＜0.3$时，表明两个变量之间的相关程度极弱，在实际应用中可视为不相关。

## 验证三种检验的关系

> [!ATTANTION] 适用于一元线性回归，不一定适用于多远

> [!FAQ]- 验证$t=\frac{\hat{\beta}_{1}\sqrt{ L_{xx} }}{\hat{\sigma}}=\frac{\sqrt{ n-2 }r}{\sqrt{ 1-r^{2} }}$
> pass
> pass


> [!FAQ]- 验证$F=\frac{SSR/1}{SSE/(n-2)}=\frac{\hat{\beta}_{1}^{2}\cdot L_{xx}}{\hat{\sigma}^{2}}=t$
> pass
> pass

todo：看看视频：[t2.8](https://www.bilibili.com/video/BV1Yq4y1m7us?t=3791.5)

## 使用R进行计算

### 使用R读取数据

#### 手输

```R
> x<-c(3.4,1.8,4.6,2.3,3.1,5.5) #输入一个数组 
> X<-matrix(x,nrow=2,ncol=3) #输入一个矩阵
```

#### 读入文件

使用 `read.table(file,head= ,sep="delimiter")`函数 读取文件中的数据，其中file可以是.txt文件或.csv文件；.csv文件也可以用`read.csv()`函数读取。

### 对课本中的例2.1做回归分析

建立数据：
```R
x<-c(3.4,1.8,4.6,2.3,3.1,5.5,0.7,3.0,2.6,4.3,2.1,1.1,6.1,4.8,3.8) #生成数值向量 x 并赋予距消防站距离的数据
y<-c(26.2,17.8,31.3,23.1,27.5,36.0,14.1,22.3,19.6,31.3,24.0,17.3, 43.2,36.4,26.1) #生成数值向量 y 并赋予火灾损失的数据
```

#### 建立线性回归方程

(可以理解为t检验)

```R
#以 y 为因变量 x 为自变量建立回归方程， 默认回归方程包含截距项，如果是 lm(y~x-1)，则不包含截距项
lm2.1<-lm(y~x)

#输出回归分析的结果
summary(lm2.1)
```

输出：

```
Call:
lm(formula = y ~ x)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.4682 -1.4705 -0.1311  1.7915  3.3915 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  10.2779     1.4203   7.237 6.59e-06 ***
x             4.9193     0.3927  12.525 1.25e-08 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.316 on 13 degrees of freedom
Multiple R-squared:  0.9235,	Adjusted R-squared:  0.9176 
F-statistic: 156.9 on 1 and 13 DF,  p-value: 1.248e-08
```

其中：
- `(Intercept)`表示截距
- `Estimate`列表示回归系数的估计值$\hat{\beta}_{0}=10.278, \hat{\beta}_{1}=4.919$
- `Std. Error`列表示回归系数的标准差：$\sqrt{ var(\hat{\beta}_{0}) }=1.420,\sqrt{ var(\hat{\beta}_{1})=0.393 }$
- `t value`列表示回归系数显著性检验的$t$值$t=12.525$
- `Pr(>|t|)`列表示$P$值
- `Residual standard error`表示标准残差误$\hat{\sigma}$，可以在后面标准化残差计算中用到
- `F-statistic`表示$F$统计量的取值
- `p-value`也表示$P$值：$1.248e-08$

由回归系数显著性检验的$P$值$1.25e-08<<0.05$可知， 应该拒绝$\beta_{1}=0$的原假设，认为回归系数显著。

另外，由输出结果中$F$值$=156.9$，对应的$P$值为$1.248e-08 <<0.05$可知，应该拒绝原假设，认为回归方程是显著的。

#### 输出方差分析表

(可以理解为F检验)

上面的结果只给出了F值，没有给出方差分析表，可以使用`anova()`得到方差分析表

```R
> anova(lm2.1)
```

```
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(>F)    
x          1 841.77  841.77  156.89 1.248e-08 ***
Residuals 13  69.75    5.37                      
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

可以与前面F检验的[方差分析表](1.%20一元线性回归.md#^f6f9b4)所对应

不过，需要注意的是，R输出的方差分析表没有最后一行（即SST行），也就是说在上面的用例中$SSR=841.77,SSE=69.75$

#### 计算相关系数并检验其显著性

(可以理解为相关系数的显著性检验)

```R
cor(x,y,method="pearson") #计算相关系数
```

这里的方法`method`可以选`pearson, kendall, spearman`，默认为`pearson`，所以计算的是pearson相关系数。得到的相关系数为$0.961$。

检验相关系数的显著性使用：

```R
#相关系数的显著性检验
cor.test(x,y,alternative="two.sided",method="pearson",conf.level=0.95)
```

`alternative`可选`two.sided, less, greater`，默认值`two.sided`

输出：
```
Pearson's product-moment correlation

data:  x and y
t = 12.525, df = 13, p-value = 1.248e-08
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.8837722 0.9872459
sample estimates:
      cor 
0.9609777
```

显而易见，第三行中`p-value`为$P$值，最后的`cor 0.9609777`为样本相关系数。

# 残差分析

## 残差概念

残差$e_{i}$是误差项$\varepsilon_{i}$的估计值

残差：$e_{i}=y_{i}-\hat{y}_{i}=y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}$

误差项：$\varepsilon_{i}=y_{i}-\beta_{0}-\beta_{1}x_{i}$

R中计算残差：

```R
e<-resid(lm2.1, digits = 5) #将残差赋值给e，保留小数点后5位
e #显示e的值
```

## 残差的性质

性质1：$E(e_{i})=0$

证明：
$$
\begin{align}
E(e_{i}) & = E(y_{i})-E(\hat{y}_{i})\\ &=(\beta_{0}+\beta_{1}x_{i})-E(\hat{\beta}_{0}+\hat{\beta}_{1}x_{i})\\ &=0
\end{align}
$$

性质2：
$$
var(e_{i})=\left[ 1-\frac{1}{n}-\frac{(x_{i}-\bar{x})^{2}}{L_{xx}} \right] \sigma^{2} = (1-h_{ii})\sigma^{2}
$$

这里$h_{ii}=\frac{1}{n}+\frac{(x_{i}-\bar{x})^{2}}{L_{xx}}$被称为杠杆值

性质3：残差满足约束条件:
$$
\begin{align}
 & \sum\limits_{i=1}^n e_{i}=0\\ \\
 & \sum\limits_{i=1}^n x_{i}e_{i}=0
\end{align}
$$

这表明残差$e_{1},e_{2},\cdots,e_{n}$是相关的，不是独立的。

## 改进的残差

标准化残差
$$
ZRE_{i}=\frac{e_{i}}{\hat{\sigma}}
$$

学生化残差
$$
SRE_{i}=\frac{e_{i}}{\hat{\sigma}\sqrt{ 1-h_{ii} }}
$$

计算标准化残差和学生化残差的代码为：

```R
ZRE<-e/2.316 #计算标准化残差
SRE<-rstandard(lm2.1) #计算学生化残差
```

# 回归系数的区间估计

由$\hat{\beta}_{1}\sim N\left( \beta_{1},\frac{\sigma^{2}}{L_{xx}} \right)$可得
$$
t=\frac{\hat{\beta}_{1}-\beta_{1}}{\sqrt{ \hat{\sigma}^{2}/L_{xx} }}=\frac{(\hat{\beta}_{1-\beta_{1}})\sqrt{ L_{xx} }}{\hat{\sigma}}\sim t(n-2)
$$

故
$$
P\left( \left\lvert  \frac{(\hat{\beta}_{1}-\beta_{1})\sqrt{ L_{xx} }}{\hat{\sigma}}  \right\rvert <t_{\frac{\alpha}{2}}(n-2) \right) = 1-\alpha
$$

等价于
$$
P\left( \hat{\beta}_{1}-t_{\frac{\alpha}{2}}\frac{\hat{\sigma}}{L_{{xx}}}<\beta_{1}<\hat{\beta}_{1}+t_{\frac{\alpha}{1}} \frac{\hat{\sigma}}{\sqrt{ L_{xx} }} \right) =1-\alpha
$$

既得$\beta_{1}$的置信度为$1-\alpha$的置信区间为
$$
\left( \hat{\beta}_{1} - t_{\frac{\alpha}{2}} \frac{\hat{\sigma}}{\sqrt{ L_{xx} }},\hat{\beta}_{1}+t_{\frac{\alpha}{2}} \frac{\hat{\sigma}}{\sqrt{ L_{xx} }} \right) 
$$

求置信度为95%的置信区间的代码：
```R
confint(lm2.1, level=0.95)# level为置信度，0.95为默认值可以不加
```

# 预测

（应该只考代码的解读）

```R
new<-data.frame(x = 3.5) #以数据框的格式存储新值3.5 

#计算预测值及预测区间并赋给 ypred，level = 0.95 为默认值
ypred<-predict(lm2.1,new,interval = "prediction",level = 0.95)

#计算预测值及置信区间并赋给 yconf，level = 0.95 为默认值
yconf<-predict(lm2.1,new,interval = "confidence",level = 0.95) 
```

输出结果：
（利用作业2.14的数据）
```
> new<-data.frame(x=3.5)
> ypred<-predict(lm2.14,new,interval = "prediction",level = 0.95)
> ypred
   fit      lwr      upr
1 23.5 2.171277 44.82872
> yconf
   fit      lwr      upr
1 23.5 14.35912 32.64088
```

由以上输出结果可知，点估计值$\hat{y}_{0}$以及置信水平为95%的置信区间为
- 点估计值 $\hat{y}_{0}:23.5$
- 单个新值 $(2.171,44.83)$
- 平均值 $E(y_{0})=(14.359,32.641)$
