# 一元线性回归模型

样本观测值：$(x_1,y_1), (x_2,y_2),\cdots,(x_n,y_n)$

样本回归模型：
$$
\begin{aligned}
y_i=\beta_0+\beta_1x_i+\varepsilon_i\\
\begin{cases}
E(\varepsilon_i)=0\\
var(\varepsilon_i)=\sigma^2
\end{cases}
\end{aligned}
$$

对上式两边分别求数学期望和方差得

回归方程 $E(y_i)=\beta_0+\beta_1x_i,\ var(y_i)=\sigma^2$

**经验回归方程** $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$

# 参数$\beta_0,\beta_1$的估计

## 普通最小二乘估计
(Ordinary Least Square Estimation， 简记为OLSE)

### 推导

==important==

最小二乘法就是寻找参数$β_0,β_1$的估计值使离差平方和达到极小

$$
\begin{aligned}
\min S_{\epsilon^2}=&\sum(y_i-\hat{y}_i)^2\\
\\
Q(\hat{\beta}_0,\hat{\beta}_1)=&\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)^2\\
=&\min\limits_{\beta_0,\beta_1}\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)^2
\end{aligned}
$$

这里，称$\hat{y}_i=\hat{\beta}_0-\hat{\beta}_1x_i$为$y_i$的**回归拟合值**，简称为回归值或拟合值；称$e_i=y_i-\hat{y}_i$为$y_i$的**残差**

要求残差平方和最小，可以使偏导为0：
$$
\begin{cases}
\frac{ \partial Q }{ \partial \beta_0}\bigg|_{\beta_0 = \hat{\beta}_0}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)=0\\
\frac{ \partial Q }{ \partial \beta_1}\bigg|_{\beta_1 = \hat{\beta}_1}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)x_i=0
\end{cases}
$$

可以得出
$$
\begin{cases}
\sum\limits_{i=1}^ne_i=0\\
\sum\limits_{i=1}^ne_ix_i=0
\end{cases}
$$

整理后，得正规方程组
$$
\begin{cases}
n\hat{\beta}_0+(\sum\limits_{i=1}^nx_i)\hat{\beta_1}=\sum\limits_{i=1}^ny_i\\
(\sum\limits_{i=1}^nx_i)\hat{\beta}_0+(\sum\limits_{i=1}^nx_i^2)\hat{\beta_1}=\sum\limits_{i=1}^nx_iy_i
\end{cases}
$$

则最后可以得出OLSE：
$$
\begin{cases}
\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}\\
\hat{\beta}_1=\frac{\sum\limits_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}
\end{cases}
$$

为了方便表示，这里可以记$\begin{aligned}L_{xx}&=\sum\limits_{i=1}^n(x_i-\bar{x})^2=\sum\limits_{i=1}^nx_i^2-n(\bar{x})^2\\ L_{xy}&=\sum\limits_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})=\sum\limits_{i=1}^nx_iy_i-n\bar{x}\bar{y}\end{aligned}$

那么就有：
$$
\begin{cases}
\hat{\beta_0}=\bar{y}-\hat{\beta}_1\bar{x}\\
\hat{\beta_1}=L_{xy}／L_{xx}
\end{cases}
$$

### 性质

#### 线性

易知 $\hat{\beta}_1=\frac{\sum\limits_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}$ 可以等价表示为 $\hat{\beta}_1=\frac{\sum\limits_{i=1}^n(x_i-\bar{x})y_i}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}$
则有：
$$
\begin{aligned}
\hat{\beta}_1&=\frac{\sum\limits_{i=1}^n(x_i-\bar{x})y_i}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}\\
\\
&=\sum\limits_{i=1}^n\frac{x_i-\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}y_i
\end{aligned}
$$

这里 $\sum\limits_{i=1}^n\frac{x_i-\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}$ 是常数，所以$\hat{\beta}_1$是$y_i$的线性组合；同理，可以证明$\hat{\beta}_0$是$y_i$的线性组合：
$$
\begin{aligned}
\hat{\beta}_0&=\bar{y}-\hat{\beta}_1\bar{x}\\
&=\frac{\sum\limits_{i=1}^ny_i}{n}-\sum\limits_{i=1}^n\frac{x_i-\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}y_i\bar{x}\\
&=\sum\limits_{i=1}^n\left(\frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}\right)y_i
\end{aligned}
$$

这里的$\sum\limits_{i=1}^n\left(\frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}\right)$是常数

#### 无偏性

指估计值的期望等于被估计参数的真实值

> [!TIP] 前提：
> $$
> \begin{aligned}
> &E(y_i)=\beta_0+\beta_1x_i\\
> \\
> &\sum(x_i-\bar{x})=0\\
> \\
> &\sum(x_i-\bar{x})x_i=\sum(x_i-\bar{x})^2
> \end{aligned}
> $$

由$\hat{\beta}_1$的式子可以得
$$
\begin{aligned}
E(\hat{\beta}_1)=&\sum\limits_{i=1}^n\frac{x_i-\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}E(y_i)\\
=&\sum\limits_{i=1}^n\frac{x_i-\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}(\beta_0+\beta_1x_i)\\
=&\beta_0\frac{\sum\limits_{i=1}^n(x_i-\bar{x})}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}+\beta_1\frac{\sum\limits_{i=1}^n(x_i-\bar{x})x_i}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}\\
=&\beta_0\frac{0}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}+\beta_1\frac{\sum\limits_{i=1}^n(x_i-\bar{x})^2}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}\\
=&\beta_1
\end{aligned}
$$

同理，可证$\hat{\beta}_0$是$\beta_0$的无偏估计

#### 有效性

!!! 感觉没必要写推导过程，过几天再来看看，不行删了

> 一个估计量是无偏的，只揭示了估计量优良性的一个方面。我们通常还关心估计量本身的波动情况，这就需要进一步研究它的方差。

> [!TIP] 前提：
> 方差性质：$var(aX+bY)=a^2var(X)+b^2var(Y)$
> 
> $$
> y_i=\beta_0+\beta_1x_i+\varepsilon_i
> $$
> $$
> var(y_i)=var(\varepsilon_i)=\sigma^2
> $$
> $\hat{\beta}_0$在线性推导中有$\hat{\beta}_0=\sum\limits_{i=1}^n\left(\frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}\right)y_i$

那么，有

$$
\begin{aligned}
var(\hat{\beta}_1)&=\sum\limits_{i=1}^n\left[\frac{x_i-\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}\right]^2var(y_i)\\
&=\frac{\sum\limits_{i=1}^n(x_i-\bar{x})^2}{\left[\sum\limits_{j=1}^n(x_j-\bar{x})^2\right]^2}\sigma^2\\
&=\frac{\sigma^2}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}
\end{aligned}
$$

与
$$
\begin{aligned}
var(\hat{\beta}_0)&=var\left(\sum\limits_{i=1}^n\left(\frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}\right)y_i\right)\\
&=\sigma^2\cdot\sum\limits_{i=1}^n\left(\frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}{\sum\limits_{j=1}^n(x_j-\bar{x})^2}\right)^2\\
&=\left[\frac{1}{n}+\frac{(\bar{x})^2}{\sum(x_j-\bar{x})^2}\right]\sigma^2
\end{aligned}
$$

综上，有$\hat{\beta}_0,\hat{\beta}_1$的正态分布
$$
\begin{aligned}
\hat{\beta}_0&\sim N(\beta_0,(\frac{1}{n}+\frac{(\bar{x})^2}{L_{xx}})\sigma^2)\\
\hat{\beta}_1&\sim N(\beta_1,\frac{\sigma^2}{L_{xx}})
\end{aligned}
$$

还可以得到$\hat{\beta}_0,\hat{\beta}_1$的协方差：
$$
cov(\hat{\beta}_0,\hat{\beta}_1)=-\frac{\bar{x}}{L_{xx}}\sigma^2
$$
它说明，在$\bar{x}=0$时，$\hat{\beta}_0$与$\hat{\beta}_1$不相关，在正态假定下两者相应独立；在$\bar{x}\ne0$时，不独立。它揭示了回归系数间的关系。

#### 最佳线性无偏估计
(Best Linear Unbiased Estimator, BLUE)

$$
\begin{cases}
E(\varepsilon_i)=0,\quad i=1,2,\cdots,n\\
cov(\varepsilon_i,\varepsilon_j)=\begin{cases}
\sigma^2,\quad i=j\\
0,\quad i\ne j
\end{cases}
\quad i,j=1,2,\cdots,n
\end{cases}
$$

以上条件（Gauss~Markov条件）下可以证明$\hat{\beta}_0,\hat{\beta}_1$分别是$\beta_0,\beta_1$的最佳线性无偏估计(BLUE)，也称为最小方差线性无偏估计。BLUE 即指在$\beta_0,\beta_1$的一切线性无偏估计中，它们的方差最小。

## 最大似然估计
(Maximum Likelihood Estimation, MLE)

最大似然法就是用已知的样本标记结果，反推最大概率能使结果出现的模型参数。

设(环境)参数$\theta$与结果$x$，有$p(x|\theta)$：已知参数，推导结果；相反的，有似然函数$L(\theta|x)$已知结果，反推参数。

所以，最大似然估计应在一切$\theta$中选取使随机样本$(X_1,X_2,\cdots,X_n)$落在点$(x_1,x_2,\cdots,x_n)$附近点概率最大的$\hat{\theta}$为未知参数$\theta$真值的估计值，即选取$\theta$满足
$$
L(\hat{\theta};x_1,x_2,\cdots,x_n)=\max_{\theta}L(\theta;x_1,x_2,\cdots,x_n)
$$

这里的话，函数是用$x_i$来求$\theta$

#### 对于一元线性回归的最大似然估计

在假设$\varepsilon_i\sim N(0,\sigma^2)$时，$y_i$有服从如下正态分布：
$$
y_i\sim N(\beta_0+\beta_1x_i,\sigma^2)
$$

$y_i$的密度分布为
$$
f_i(y_i)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2\sigma^2}[y_i-(\beta_0+\beta_1x_i)]^2\right\},\qquad i=1,2,\cdots,n
$$

那么有$y_1,y_2,\cdots,y_n$的似然函数
$$
\begin{aligned}
L(\beta_0,\beta_1,\sigma^2)&=\prod\limits_{i=1}^nf_i(y_i)\\
&=(2\pi\sigma^2)^{\frac{n}{2}}\exp\left\{-\frac{1}{2\sigma^2}[y_i-(\beta_0+\beta_1x_i)]^2\right\}
\end{aligned}
$$

因为$L$的极大化和$\ln(L)$的极大化是等价的，所以取对数似然函数为
$$
\ln(L)=-\frac{n}{2}\ln(2\pi\sigma^2)-\frac{1}{2\sigma^2}[y_i-(\beta_0+\beta_1x_i)]^2
$$

那么，要求上式的极大值，等价于求$\sum\limits_{i=1}^n[y_i-(\beta_0+\beta_1x_i)]^2$的极小值，到这里就与OLSE相同
$$
\begin{cases}
\frac{\partial\ln(L)}{\partial\beta_0}=\frac{\partial \sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)^2}{\partial\beta_0}=\frac{ \partial Q(\beta_0,\beta_1) }{ \partial \beta_0}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)=0\\
\frac{\partial\ln(L)}{\partial\beta_1}=\frac{ \partial Q(\beta_0,\beta_1) }{ \partial \beta_1}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)x_i=0
\end{cases}
$$

此外，由最大似然估计还可以得到$\sigma^2$的估计值，令
$$
\frac{\partial\ln(L)}{\partial\sigma^2}=0
$$

有
$$
\frac{n}{2\pi}=\frac{\sum\limits_{i=1}^n[y_i-(\beta_0+\beta_1x_i)]^2}{\sigma^2}
$$

最后可以得到$\sigma^2$的有偏估计值
$$
\begin{aligned}
\hat{\sigma}^2&=\frac{1}{n}\sum\limits_{i=1}^n[y_i-(\beta_0+\beta_1x_i)]^2\\
&=\frac{1}{n}\sum\limits_{i=1}^ne_i^2
\end{aligned}
$$

实际应用中，常用无偏估计量
$$
\hat{\sigma}^2=\frac{1}{n-2}\sum\limits_{i=1}^n[y_i-(\beta_0+\beta_1x_i)]^2
$$

