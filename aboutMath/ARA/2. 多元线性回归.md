# 多元线性回归模型

## 一般形式

$$
y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\cdots+\beta_{p}x_{p}+\varepsilon
$$
$$
\begin{cases}
E(\varepsilon)=0 \\
var(\varepsilon)=\sigma^{2}
\end{cases}
$$

对$n$组观测数据$(x_{i1},x_{i2},\dots,x_{ip};y_{i}),i=1,2,\dots n$，线性回归模型表示为
$$
\begin{cases}
y_{1}=\beta_{0}+\beta_{1}x_{11}+\beta_{2}x_{12}+\cdots+\beta_{p}x_{1p}+\varepsilon_{1} \\
y_{2}=\beta_{0}+\beta_{1}x_{21}+\beta_{2}x_{22}+\cdots+\beta_{p}x_{2p}+\varepsilon_{2} \\
\quad\cdots \\
y_{n}=\beta_{0}+\beta_{1}x_{n1}+\beta_{2}x_{n2}+\cdots+\beta_{p}x_{np}+\varepsilon_{n}
\end{cases}
$$
对于上式可以写成
$$
\boldsymbol{y=X\beta+\varepsilon}
$$

^a5061c

其中，
$$
\boldsymbol{y}=\begin{pmatrix}y_{1} \\ y_{2}\\ \vdots\\ y_{n}\end{pmatrix}\quad
\boldsymbol{X}=\begin{pmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1p} \\
1 & x_{21} & x_{22} & \cdots & x_{2p}  \\ 
\vdots & \vdots & \vdots & \vdots & \vdots & \\
1 & x_{n1} & x_{n2} & \cdots & x_{np}
\end{pmatrix}_{n\times(p+1)}\quad
\boldsymbol{\beta}=\begin{pmatrix}\beta_{0}\\ \beta_{1}\\ \vdots\\ \beta_{p}\end{pmatrix}\quad
\boldsymbol{\varepsilon}=\begin{pmatrix}\varepsilon_{1}\\ \varepsilon_{2}\\ \vdots\\ \varepsilon_{n}\end{pmatrix}
$$

$\boldsymbol{X}$为设计矩阵

 ## 基本假设

1. 解释变量$x_{1}+x_{2}+\dots+x_{p}$是确定性变量，不是随机变量，而且要求$rank(\boldsymbol{X})=p+1<n$；说明设计矩阵中的列自变量列间不相关，样本量的个数应大于解释变量的个数，$\boldsymbol{X}$为满秩矩阵。
2. 随机误差项均值为$0$且等方差，即[高斯-马尔柯夫(Gauss-Markov)](1.%20一元线性回归.md#最佳线性无偏估计)条件。
3. 正态分布的假定条件为：
    $$
    \begin{cases}
\varepsilon_{i}\sim N(0,\sigma^{2}),i=1,2,\dots,n \\
\varepsilon_{1},\varepsilon_{2},\dots,\varepsilon_{n}相互独立
\end{cases}
    $$
    对于[模型式](#^a5061c)，该条件可表示为
    $$
    \boldsymbol{\varepsilon} \sim N(\boldsymbol{0},\sigma^{2}\boldsymbol{I}_{n})
    $$
    此时
    $$
    \boldsymbol{y}\sim N(\boldsymbol{X\beta},\sigma^{2}\boldsymbol{I}_{n})
    $$

## 回归系数的解释

对含有 $p$ 个自变量的多元线性回归而言，每个回归系数 $β_i$ 表示在回归方程中其他自变量保持不变的情况下，自变量 $x_i$ 每增加一个单位时因变量 $y$ 的平均增加幅度。 因此也把多元线性回归的回归系数称为偏回归系数(partial regression coefficient)
如$y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\varepsilon,E(y)=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}$
在$x_{2}$保持不变时，有$\frac{\partial E(y)}{\partial x_{1}}=\beta_{1}$
在$x_{1}$保持不变时，有$\frac{\partial E(y)}{\partial x_{2}}=\beta_{2}$

# 回归系数的估计

## 最小二乘估计

最小二乘估计要寻找$\hat{\beta}_{0},\hat{\beta}_{1},\hat{\beta}_{2},\dots,\hat{\beta}_{p}$，使得离差平方和
$$
\begin{align}
Q(\hat{\beta}_{0},\hat{\beta}_{1},\hat{\beta}_{2},\dots\hat{\beta}_{p})&=\sum\limits_{i=1}^n(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i1}-\hat{\beta}_{2}x_{i2}-\dots-\hat{\beta}_{p}x_{ip})^{2} \\
&=\min_{\beta_{0},\beta_{1},\beta_{2},\dots,\beta_{p}}=\sum\limits_{i=1}^n(y_{i}-\beta_{0}-\beta_{1}x_{i1}-\beta_{2}x_{i2}-\dots-\beta_{p}x_{ip})
\end{align}
$$

根据微积分中求极值的原理，有
$$
\begin{cases}
\frac{ \partial Q }{ \partial \beta_0}\bigg|_{\beta_0 = \hat{\beta}_0}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\hat{\beta}_{2}x_{i2}-\dots-\hat{\beta}_{p}x_{ip})=0\\
\frac{ \partial Q }{ \partial \beta_1}\bigg|_{\beta_1 = \hat{\beta}_1}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\hat{\beta}_{2}x_{i2}-\dots-\hat{\beta}_{p}x_{ip})x_{i1}=0 \\
\frac{ \partial Q }{ \partial \beta_1}\bigg|_{\beta_2 = \hat{\beta}_2}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\hat{\beta}_{2}x_{i2}-\dots-\hat{\beta}_{p}x_{ip})x_{i2}=0 \\
\dots\dots \\
\frac{ \partial Q }{ \partial \beta_1}\bigg|_{\beta_p = \hat{\beta}_p}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\hat{\beta}_{2}x_{i2}-\dots-\hat{\beta}_{p}x_{ip})x_{ip}=0
\end{cases}
$$

^c4b4e5

整理后得到正规方程(**注意！这里的$\boldsymbol{X}^{\prime}$就是$\boldsymbol{X}^T$**)：
$$
\begin{align}
\boldsymbol{X^{\prime}(y-X \hat{\beta})=0} \\
\boldsymbol{X^{\prime}X \hat{\beta}=X^{\prime}y}
\end{align}
$$
$(\boldsymbol{X^{\prime}X})^{-1}$存在时，即得到回归参数的最小二乘估计：
$$
\boldsymbol{\hat{\beta}}=(\boldsymbol{X^{\prime}X})^{-1}\boldsymbol{X^{\prime}y}
$$
那么称
$$
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_1x_{1}+\hat{\beta}_{2}x_{2}+\dots+\hat{\beta}_{p}x_{p}
$$

^2b6d45

为经验回归方程

> $(\boldsymbol{X^{\prime}X})^{-1}\Rightarrow \lvert \boldsymbol{X^{\prime}X} \rvert\ne0\Rightarrow rank(\boldsymbol{X^{\prime}X})=p+1\Rightarrow rank(X)\ge p+1\Rightarrow X_{n\times (p+1)}\Rightarrow n\ge p+1$

## 回归值和残差

### 回归值（与帽子矩阵）

$\hat{y}=\hat{\beta}_{0}+\hat{\beta}_1x_{i1}+\hat{\beta}_{2}x_{i2}+\dots+\hat{\beta}_{p}x_{ip}$为观测值$y_{i}$的回归（拟合）值；有$\boldsymbol{\hat{y}}=\boldsymbol{X \hat{\beta}}=(\hat{y_{1}},\hat{y_{2}},\cdots,\hat{y_{n}})^{\prime}$为因变量向量$\boldsymbol{y}=(y_{1},y_{2},\cdots,y_{n})^{\prime}$的回归值。由$\boldsymbol{\hat{\beta}}=\boldsymbol{X}(\boldsymbol{X^{\prime}X})^{-1}\boldsymbol{X^{\prime}y}$可得
$$
\boldsymbol{\hat{y}}=\boldsymbol{X\hat{\beta}}=\boldsymbol{X}(\boldsymbol{X^{\prime}X})^{-1}\boldsymbol{X^{\prime}y}
$$

从上式可以看出，矩阵$\boldsymbol{X}(\boldsymbol{X^{\prime}X})^{-1}\boldsymbol{X^{\prime}}$的作用是把因变量向量$\boldsymbol{y}$变为拟合值向量$\boldsymbol{\hat{y}}$，从形式上看是给$\boldsymbol{y}$带上了帽子，因此形象地称其为**帽子矩阵**，记为$\boldsymbol{H}$，于是有$\boldsymbol{\hat{y}}=\boldsymbol{Hy}$。

**帽子矩阵**：$\boldsymbol{H}=\boldsymbol{X}(\boldsymbol{X^{\prime}X})^{-1}\boldsymbol{X^{\prime}}$
	1. 是$n$阶对称矩阵
	2. 是幂等矩阵，即$\boldsymbol{H}=\boldsymbol{H}^{2}$
	3. 主对角线元素记为$h_{ii}$([见性质2](1.%20一元线性回归.md#残差的性质))，易得帽子矩阵$\boldsymbol{H}$的迹为$tr(\boldsymbol{H})=\sum\limits_{i=1}^nh_{ii}=p+1$

### 残差

1. $y_{i}$的残差：$e_{i}=y_{i}-\hat{y}_{i}$
2. （回归）残差向量：$\boldsymbol{e}=\boldsymbol{y}-\boldsymbol{\hat{y}}=(\boldsymbol{I}-\boldsymbol{H})\boldsymbol{y}$（将$\boldsymbol{\hat{y}}=\boldsymbol{Hy}$带入）
3. 残差向量的协方差阵：
	  $$
	  \begin{align}
    D(\boldsymbol{e}) & =cov(\boldsymbol{e},\boldsymbol{e}) \\
 & =cov((\boldsymbol{I}-\boldsymbol{H})\boldsymbol{y},(\boldsymbol{I}-\boldsymbol{H})\boldsymbol{y}) \\
 & =(\boldsymbol{I}-\boldsymbol{H})cov(\boldsymbol{y},\boldsymbol{y})(\boldsymbol{I}-\boldsymbol{H})^{\prime} \\
 & =\sigma^{2}(\boldsymbol{I}-\boldsymbol{H})
    \end{align}
	  $$
    于是有$D(e_{i})=(1-h_{ii})\sigma^{2},i=1,2,\cdots,n$

根据[该式](#^c4b4e5)可知，残差满足关系式：

$$
\begin{cases}
\sum e_{i}=0 \\
\sum e_{i}x_{i1}=0 \\
\vdots \\
\sum e_{i}x_{ip}=0
\end{cases}
$$

就是说，残差的平均值为0，残差对每个只变量的加权平均为0。上式可以用矩阵表示为$\boldsymbol{X}^{\prime}\boldsymbol{e}=\boldsymbol{0}$

$E\left( \sum\limits_{i=1}^ne_{i}^{2} \right)=\sum\limits_{i=1}^nD(e_{i})=(n-p-1)\sigma^{2}$误差项方差$\sigma^{2}$的无偏估计为

$$
\begin{align}
\hat{\sigma}^{2}&=\frac{1}{n-p-1}SSE=\frac{1}{n-p-1}(\boldsymbol{e}^{\prime}\boldsymbol{e}) \\
&=\frac{1}{n-p-1}\sum\limits_{i=1}^ne_{i}^{2}
\end{align}
$$

## 最大似然估计

由[模型式](#^a5061c)可得

$$
\boldsymbol{y}=\boldsymbol{X\beta}+\boldsymbol{\varepsilon}
$$
$$
\boldsymbol{\varepsilon}\sim N(\boldsymbol{0},\sigma^{2}\boldsymbol{I}_{n})
$$

那么$\boldsymbol{y}$的概念分布为

$$
\boldsymbol{y}\sim N(\boldsymbol{X\beta},\sigma^{2}\boldsymbol{I}_{n})
$$

可以得到似然函数

$$
L=(2\pi)^{-n/2}(\sigma^{2})^{-n/2}\exp\left( -\frac{1}{1\sigma^{2}}(\boldsymbol{y}-\boldsymbol{X\beta})^{\prime}(\boldsymbol{y}-\boldsymbol{X\beta}) \right)
$$

其中的未知参数就是$\boldsymbol{\beta}$和$\sigma^{2}$，最大似然估计就是要选似然函数$L$达到最大的$\hat{\boldsymbol{\beta}}$和$\hat{\sigma}^{2}$，要使$L$达到最大，似然函数两边需同时取对数，得到对数似然函数

$$
\ln L=-\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^{2})-\frac{1}{2\sigma^{2}}(\boldsymbol{y}-\boldsymbol{X\beta})^{\prime}(\boldsymbol{y}-\boldsymbol{X\beta})
$$

在上式中仅在最后一项中含有$\boldsymbol{\beta}$，显然使上式达到最大，等价于使下式达到最小

$$
(\boldsymbol{y}-\boldsymbol{X\beta})^{\prime}(\boldsymbol{y}-\boldsymbol{X\beta})
$$

这个时候，又与[OLSE](#最小二乘估计)一样。故在正态假定下，$\boldsymbol{\beta}$的最大似然估计与OLSE完全相同，即

$$\hat{\boldsymbol{\beta}}=(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}\boldsymbol{y}$$

误差项方差$\sigma^{2}$的最大似然估计为

$$
\hat{\sigma}^{2}_{L}=\frac{1}{n}SSE=\frac{1}{n(\boldsymbol{e}^{\prime}\boldsymbol{e})}
$$
# 估计量的性质

性质一：$\hat{\boldsymbol{\beta}}$是随机向量$\boldsymbol{y}$的一个**线性**变换

$$
\hat{\boldsymbol{\beta}}=(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}\boldsymbol{y}
$$

性质二：$\hat{\boldsymbol{\beta}}$是$\boldsymbol{\beta}$的**无偏**估计

$$
\begin{align}
E(\hat{\boldsymbol{\beta}}) & =E((\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}\boldsymbol{y}) \\
 & =(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}E(\boldsymbol{y}) \\
 & =(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}E(\boldsymbol{X\beta}+\boldsymbol{\varepsilon}) \\
 & =(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}\boldsymbol{X\beta} \\
 & =\boldsymbol{\beta}
\end{align}
$$

性质三：　$D(\hat{\boldsymbol{\beta}})=\sigma^{2}(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}$ （协方差矩阵）

$$
\begin{align}
D(\hat{\beta}) & =cov(\hat{\boldsymbol{\beta}},\hat{\boldsymbol{\beta}}) \\
 & =cov((\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}\boldsymbol{y},(\boldsymbol{X}^{\prime}\boldsymbol{X})\boldsymbol{X}^{\prime}y) \\
 & =(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}cov(\boldsymbol{y},\boldsymbol{y})((\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime})^{\prime} \\
 & =(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}\sigma^{2}\boldsymbol{X}^{\prime}\boldsymbol{X}(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1} \\
 & =\sigma^{2}(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}
\end{align}
$$
当$p=1$时即一元线性回归的情况

性质四：高斯-马尔柯夫([G-M](1.%20一元线性回归.md#^GaussM))定理

假定$E(\boldsymbol{y})=\boldsymbol{X\beta},D(\boldsymbol{y})=\sigma^{2}\boldsymbol{I}_{n}$时，$\boldsymbol{\beta}$的任一线性函数$\boldsymbol{c}^{\prime}\boldsymbol{\beta}$的最小方差线性无偏估计(Best Linear Unbiased Estimator, BLUE)为$\boldsymbol{c}^{\prime}\hat{\boldsymbol{\beta}}$（$\boldsymbol{c}$是任一$p+1$维常数向量，$\hat{\boldsymbol{\beta}}$是$\boldsymbol{\beta}$的最小二乘估计）

对于该性质，需要注意：（其中前三个性质易证明，5、6性质也适用于一元线性回归）
1. 取常数向量$\boldsymbol{c}$的第$j(j=1,2,\cdots,p)$个分量为$1$，其余分量为$0$，这时G-M定理表明最小二乘估计$\hat{\beta}_{j}$是$\beta_{j}$的最小方差线性无偏估计
2. 可能存在$y_{1},y_{2},\cdots,y_{n}$的非线性函数，作为$\boldsymbol{c^{\prime}\beta}$的无偏估计，比最小二乘估计$\boldsymbol{c^{\prime}\hat{\beta}}$的方差更小
3. 可能存在$\boldsymbol{c^{\prime}\beta}$的有偏估计，在某种意义（如均方误差最小）上比最小二乘估计$\boldsymbol{c^{\prime}\hat{\beta}}$更好
4. 在正态假定下， $\boldsymbol{c^{\prime}\hat{\beta}}$是$\boldsymbol{c^{\prime}\beta}$的最小方差无偏估计。 也就是说，既不可能存在$y_{1},y_{2},\cdots,y_{n}$的非线性函数，也不可能存在$y_{1},y_{2},\cdots,y_{n}$的其它线性函数，作为$\boldsymbol{c^{\prime}\beta}$的无偏估计，比最小二乘估计$\boldsymbol{c^{\prime}\hat{\beta}}$的方差更小

性质五：$cov(\hat{\boldsymbol{\beta}},\boldsymbol{e})=0$（说明$\hat{\boldsymbol{\beta}}$与$\boldsymbol{e}$不相关，等价于$\hat{\boldsymbol{\beta}}$与$\boldsymbol{e}$独立，从而$\hat{\boldsymbol{\beta}}$与$SSE=\boldsymbol{e^{\prime}e}$）

性质六：当$\boldsymbol{y}\sim N(\boldsymbol{X\beta},\sigma^{2}\boldsymbol{I}_{n})$时，则 
	1. $\hat{\boldsymbol{\beta}}\sim N(\boldsymbol{\beta},\sigma^{2}(\boldsymbol{X^{\prime}X})^{-1})$
	2. $SSE/\sigma^{2}\sim \chi^{2}(n-p-1)$
^01e93c
# 回归方程的显著性检验

## F检验

原假设：$H_{0}:\beta_{1}=\beta_{2}=\cdots=\beta_{p}=0$

利用离差平方和的分解式$SST = SSR + SSE$建立对$H_{0}$进行检验的F统计量

$$
F=\frac{SSR/p}{SSE/(n-p-1)}
$$

在正态假设下，当$H_{0}$成立时$F\sim F(p,n-p-1)$F服从自由度为$(n-p-1)$的F分布

不过 F 检验只检验整个方程的显著性，所以在 t 检验中有剔除无关变量方法的提及。

方差分析表

| 方差来源 | 自由度 | 平方和 | 均方      | F 值                        | P 值           |
| -------- | ------ | ------ | --------- | --------------------------- | -------------- |
| 回归     | 1      | SSR    | $SSR/p$   | $F=\frac{SSR/p}{SSE/(n-p-1)}$ | $P(F>F值)=P值$ |
| 残差     | $n-p-1$  | SSE    | $SSE/(n-p-1)$ |                             |                |
| 总和     | $n-1$  | SST    |           |                             |                |

与[一元线性回归方差分析表](1.%20一元线性回归.md#^f6f9b4)对比可以发现，当$p=1$时就是一元的方差分析表

> 对于线性回归的方差分析， R 语言中不仅可使用函数`anova()`得到方差分析表，还可以使用函数`Anova()`，而在使 用函数`Anova()`前需要安装包 `car` 并加载该包
> 
> ```R
> install.packages("car")
> library(car)
> Anova(lm, type="III")
> ```

## t检验

原假设$H_{0j}:\beta_{j}=0,j=1,2,\cdots,p$

依据[性质六](#^01e93c)构造 t 统计量

$$
t_{j}=\frac{\hat{\beta}_{j}}{\sqrt{ c_{jj} }\hat{\sigma}}
$$

其中
$$
\begin{align}
c_{ij}=(\boldsymbol{X^{\prime}}X)^{-1},\quad i,j=1,2,\cdots,p \\
\hat{\sigma}=\sqrt{ \frac{1}{n-p-1}\sum\limits_{i=1}^ne^{2}_{i} }=\sqrt{ \frac{1}{n-p-1}\sum\limits_{i=1}^n(y_{i}-\hat{y}_{i})^{2} }
\end{align}
$$

在实际应用中，某些自变量对因变量的影响不显著（$P>\alpha$），因而多元回归中并不是包含在回归方程中的自变量越多越好，该问题会在第5章有更详细的讨论，书中仅介绍一种剔除多余变量的方法：

### 后退法

当t检验结果显示有多个自变量对因变量无显著影响时，每次剔除对应$|t值|$最小（P值最大）的一个变量（原则上每次只剔除一个变量），然后再对新的回归方程进行检验，直至保留的变量都对因变量有显著影响，否则需要继续剔除变量。

一般利用下面的代码进行计算

```R
lm3.1_drop4<-lm(y~x1+x2+x3+x5+x6+x7+x8+x9,data=data3.1)# y 对除x4外的变量作回归
# 或
lm3.1_drop4<-update(lm3.1,.~.-x4)# 在前面已经建立了lm3.1
```

具体操作可见书本p72

### 偏F统计量

> 在一元线性回归中，回归系数显著性的 t 检验与回归方程显著性的 F 检验是等价的，而在多元线性回归中，这两种检验是不等价的。
> - F 检验显著，说明$y$对自变量 $x_{1},x_{2},\cdots,x_{p}$ 整体的线性回归效果是显著的，但不等于$y$对每个自变量$x_{j}$的回归效果都显著。
> - 某个或某几个$x_{j}$的系数不显著，回归方程显著性的F检验仍有可能是显著的。

从另一个角度考虑自变量$x_{j}$的显著性：$y$对自变量$x_{1},x_{2},\cdots,x_{p}$线性回归的残差平方和为$SSE$，回归平方和为$SSR$，在剔除$x_{j}$后，用$y$对其余的$p-1$个自变量做回归，记所得的残差平方和为$SSE_{(j)}$，回归平方和为$SSR_{(j)}$， 则自变量$x_{j}$对回归的贡献为$\Delta SSR_{(j)}=SSR-SSR_{(j)}$，称为$x_{j}$的偏回归平方和。由此构造偏F统计量

$$
F_{j}=\frac{\Delta SSR_{(j)}/1}{SSE/(n-p-1)}
$$

当原假设$H_{0j}:\beta_{j}=0$成立时，$F_{j}\sim F(1,n-p-1)$且$F_{j}=t_{j}^{2}$

## 回归系数的置信区间

由$t=\frac{\hat{\beta}_{j}-\beta_{j}}{\sqrt{ c_{jj} }\hat{\sigma}}\sim t(n-p-1)$可得$\beta_{j}$的置信度为$1-\alpha$的置信区间为

$$
\left( \hat{\beta}_{j}-t_{\frac{\alpha}{2}}\sqrt{ c_{jj} }\hat{\sigma},\hat{\beta}_{j}+t_{\frac{\alpha}{2}}\sqrt{ c_{jj} }\hat{\sigma} \right)
$$

利用`confint()`函数可以计算置信区间

## 拟合优度

决定系数：$R^{2}=\frac{SSR}{SST}=1-\frac{SSE}{SST}$

$y$关于$x_{1},x_{2},\cdots,x_{p}$的样本复相关系数：

$$
R=\sqrt{ R^{2} }=\sqrt{ \frac{SSR}{SST} }
$$

在两个变量的简单相关系数中，相关系数有正负之分，而复相关系数表示的是因变量y与全体自变量之间的线性关系，因而都取正号

# 中心化和标准化

## 中心化

经验回归方程：![经验回归方程](#^2b6d45)
经过样本中心$(\bar{x}_{1},\bar{x}_{2},\cdots,\bar{x}_{p};\bar{y})$，将坐标原点移至样本中心，即做坐标变换

$$
x^{\prime}_{ij}=x_{ij}-\bar{x}_{j},\quad i=1,2,\cdots,n;j=1,2,\cdots,p
$$
$$
y_{i}^{\prime}=y_{i}-\bar{y},\quad i=1,2,\cdots,n
$$
那么，上面的经验方程就转变为

$$
\hat{y}^{\prime}=\hat{\beta}_{1}x^{\prime}_{1}+\hat{\beta}_{2}x^{\prime}_{2}+\cdots+\hat{\beta}_{p}x^{\prime}_{p}  
$$

在手工计算求解线性回归方程时，通常先对数据中心化，求出中心化经验方程（即上式），再由
$$
\hat{\beta}_{0}=\bar{y}-\hat{\beta}_{1}\bar{x}_{1}-\hat{\beta}_{2}\bar{x}_{2}-\cdots-\hat{\beta}_{p}\bar{x}_{p}
$$
求出常数项估计值$\hat{\beta}_{0}$

## 标准化回归系数

标本数据的标准化公式为
$$
\begin{align}
x^*_{ij}=\frac{x_{ij}-\bar{x}_{j}}{\hat{\sigma}_{x_{j}}}, \quad i=1,2,\cdots,n;\quad j=1,2,\cdots,p \\
y^*_{i}=\frac{y_{i}-\bar{y}}{\hat{\sigma}_{y}},\quad i=1,2,\cdots,n
\end{align}
$$

式中$\hat{\sigma}_{x_{j}}$和$\hat{\sigma}_{y}$分别是自变量$x_{j}$的样本标准差和因变量$y$的样本标准差

利用最小二乘法得到标准化的回归方程$y^*=\hat{\beta}_{1}^*x_{1}^*+\hat{\beta}_{2}^*x_{2}^*+\cdots+\hat{\beta}_{p}^*x_{p}^*$

标准化包括了中心化，因而标准化的回归常数项为0。于是，易验证标准化回归系数与最小二乘回归系数间存在关系式

$$
\hat{\beta}^*_{j}=\frac{\sqrt{ L_{jj} }}{\sqrt{ L_{yy} }}\hat{\beta}_{j},\quad j=1,2,\cdots,p
$$

需要注意的是，当自变量的单位不同时普通最小二乘估计的回归系数不具有可比性。

# 相关阵与偏相关系数

## 样本相关阵

由样本观测值$x_{i1},x_{i2},\cdots,x_{ip}(i=1,2,\cdots,n)$，分别计算$x_{i}$与$x_{j}$间的简单相关系数$r_{ij}$，得自变量样本相关阵（对称矩阵）

$$
\boldsymbol{r}=\begin{bmatrix}
1 & r_{12} & \cdots & r_{1p} \\
r_{21} & 1 & \cdots & r_{2p} \\
\cdots & \cdots &  & \cdots \\
r_{p1} & r_{p2} & \cdots & 1
\end{bmatrix}
$$

记$\boldsymbol{X}^*=(x_{ij}^*)_{n\times p}$表示中心标准化的设计阵，则相关阵可表示为
$$
\boldsymbol{r}=\frac{(\boldsymbol{X}^*)^{\prime}X^*}{n-1} 
$$

进一步求出$y$与每个自变量$x_{i}$的相关系数$r_{yi}$，得到增广的样本相关阵为
$$
\tilde{\boldsymbol{r}}=\begin{bmatrix}
1 & r_{y1} & r_{y2} & \cdots & r_{yp} \\
r_{1y} & 1 & r_{12} & \cdots & r_{1p} \\
r_{2y} & y_{21} & 1 & \cdots & r_{2p} \\
\cdots & \cdots & \cdots &  & \cdots \\
r_{py} & r_{p1} & r_{p2} & \cdots & 1
\end{bmatrix}
$$
使用`cor(Z)`函数可以直接计算增广样本相关矩阵，其中$Z=(\boldsymbol{y},\boldsymbol{X})$，$\boldsymbol{y}$为因变量的样本值，$\boldsymbol{X}$为设计矩阵。

## 偏决定系数（偏判定系数）

> 当其他变量被固定后，给定的任两个变量之间的相关系数，叫偏相关系数。偏相关系数可以度量$p+1$个变量$y,x_1 ,x_{2} ,x_{p}$之中任意两个变量的线性相关程度，而这种相关程度是在固定其余$p-1$个变量的影响下的线性相关。
> 复决定系数$R^{2}$测量回归中一组自变量$x_{1},x_{2},\cdots,x_{p}$使因变量$y$的变差的相对减少量。相应地，偏决定系数测量在回归方程中已包含若干个自变量时，再引入某一个新的自变量，$y$的剩余变差的相对减少量，它衡量某自变量对$y$的变差减少的边际贡献。在讲偏相关系数前，首先引入偏决定系数。

### 1. 两个自变量的偏决定系数

 二元线性回归模型为
 $$
y_{i}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\varepsilon_{i},\quad i=1,2,\cdots,n
$$

模型中已含有$x_{2}$时再加入$x_{2}$使$y$的剩余变差的相对减小量(即模型中已含有$x_{2}$时，$y$与$x_{1}$的偏判定系数)为：
$$
r^{2}_{y1;2}=\frac{SSE(x_{2})-SSE(x_{1},x_{2})}{SSE(x_{2})}
$$
其中，$SSE(x_{2})$是模型中只含有自变量$x_{2}$时$y$的残差平方和，$SSE(x_{1},x_{2})$是模型中同时含有自变量$x_{1}$和$x_{2}$时$y$的残差平方和。

### 2. 一般情况

在模型中已含有$x_{2},\cdots,x_{p}$时，$y$与$x_{1}$的偏决定系数为：
$$
r^{2}_{y1;2,\cdots,p}=\frac{SSE(x_{2},\cdots,x_{p})-SSE(x_{1},x_{2},\cdots,x_{p})}{SSE(x_{2},\cdots,x_{p})}
$$

## 偏相关系数

> 偏判定系数的平方根称为偏相关系数，其符号与相应的回归系数的符号相同。

R 中计算偏相关系数首先需要计算[相关系数矩阵 r](#样本相关阵)，然后下载安装 corpcor 包，并使用该包中的函数 cor2pcor(r)计算偏相关系数阵

```R
r<-cor(data3.2)# 计算相关系数阵

install.packages("corpcor")# 安装 corpcor 包
library(corpcor)# 加载 corpcor 包

pcor3.2<-cor2pcor(r)# 由相关系数阵计算偏相关系数阵
```