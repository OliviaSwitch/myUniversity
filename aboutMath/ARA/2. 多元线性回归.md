# 多元线性回归模型

## 一般形式

$$
y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\cdots+\beta_{p}x_{p}+\varepsilon
$$
$$
\begin{cases}
E(\varepsilon)=0 \\
var(\varepsilon)=\sigma^{2}
\end{cases}
$$

对$n$组观测数据$(x_{i1},x_{i2},\dots,x_{ip};y_{i}),i=1,2,\dots n$，线性回归模型表示为
$$
\begin{cases}
y_{1}=\beta_{0}+\beta_{1}x_{11}+\beta_{2}x_{12}+\cdots+\beta_{p}x_{1p}+\varepsilon_{1} \\
y_{2}=\beta_{0}+\beta_{1}x_{21}+\beta_{2}x_{22}+\cdots+\beta_{p}x_{2p}+\varepsilon_{2} \\
\quad\cdots \\
y_{n}=\beta_{0}+\beta_{1}x_{n1}+\beta_{2}x_{n2}+\cdots+\beta_{p}x_{np}+\varepsilon_{n}
\end{cases}
$$
对于上式可以写成
$$
\boldsymbol{y=X\beta+\varepsilon}
$$

^a5061c

其中，
$$
\boldsymbol{y}=\begin{pmatrix}y_{1} \\ y_{2}\\ \vdots\\ y_{n}\end{pmatrix}\quad
\boldsymbol{X}=\begin{pmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1p} \\
1 & x_{21} & x_{22} & \cdots & x_{2p}  \\ 
\vdots & \vdots & \vdots & \vdots & \vdots & \\
1 & x_{n1} & x_{n2} & \cdots & x_{np}
\end{pmatrix}_{n\times(p+1)}\quad
\boldsymbol{\beta}=\begin{pmatrix}\beta_{0}\\ \beta_{1}\\ \vdots\\ \beta_{p}\end{pmatrix}\quad
\boldsymbol{\varepsilon}=\begin{pmatrix}\varepsilon_{1}\\ \varepsilon_{2}\\ \vdots\\ \varepsilon_{n}\end{pmatrix}
$$

$\boldsymbol{X}$为设计矩阵

 ## 基本假设

1. 解释变量$x_{1}+x_{2}+\dots+x_{p}$是确定性变量，不是随机变量，而且要求$rank(\boldsymbol{X})=p+1<n$；说明设计矩阵中的列自变量列间不相关，样本量的个数应大于解释变量的个数，$\boldsymbol{X}$为满秩矩阵。
2. 随机误差项均值为$0$且等方差，即[高斯-马尔柯夫(Gauss-Markov)](1.%20一元线性回归.md#最佳线性无偏估计)条件。
3. 正态分布的假定条件为：
    $$
    \begin{cases}
\varepsilon_{i}\sim N(0,\sigma^{2}),i=1,2,\dots,n \\
\varepsilon_{1},\varepsilon_{2},\dots,\varepsilon_{n}相互独立
\end{cases}
    $$
    对于[模型式](#^a5061c)，该条件可表示为
    $$
\boldsymbol{\varepsilon} \sim N(\boldsymbol{0},\sigma^{2}\boldsymbol{I}_{n})
$$
    此时
    $$
\boldsymbol{y}\sim N(\boldsymbol{X\beta},\sigma^{2}\boldsymbol{I}_{n})
$$

## 回归系数的解释

对含有 $p$ 个自变量的多元线性回归而言，每个回归系数 $β_i$ 表示在回归方程中其他自变量保持不变的情况下，自变量 $x_i$ 每增加一个单位时因变量 $y$ 的平均增加幅度。 因此也把多元线性回归的回归系数称为偏回归系数(partial regression coefficient)
如$y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\varepsilon,E(y)=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}$
在$x_{2}$保持不变时，有$\frac{\partial E(y)}{\partial x_{1}}=\beta_{1}$
在$x_{1}$保持不变时，有$\frac{\partial E(y)}{\partial x_{2}}=\beta_{2}$

# 回归系数的估计

## 最小二乘估计

最小二乘估计要寻找$\hat{\beta}_{0},\hat{\beta}_{1},\hat{\beta}_{2},\dots,\hat{\beta}_{p}$，使得离差平方和
$$
\begin{align}
Q(\hat{\beta}_{0},\hat{\beta}_{1},\hat{\beta}_{2},\dots\hat{\beta}_{p})&=\sum\limits_{i=1}^n(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i1}-\hat{\beta}_{2}x_{i2}-\dots-\hat{\beta}_{p}x_{ip})^{2} \\
&=\min_{\beta_{0},\beta_{1},\beta_{2},\dots,\beta_{p}}=\sum\limits_{i=1}^n(y_{i}-\beta_{0}-\beta_{1}x_{i1}-\beta_{2}x_{i2}-\dots-\beta_{p}x_{ip})
\end{align}
$$

根据微积分中求极值的原理，有
$$
\begin{cases}
\frac{ \partial Q }{ \partial \beta_0}\bigg|_{\beta_0 = \hat{\beta}_0}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\hat{\beta}_{2}x_{i2}-\dots-\hat{\beta}_{p}x_{ip})=0\\
\frac{ \partial Q }{ \partial \beta_1}\bigg|_{\beta_1 = \hat{\beta}_1}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\hat{\beta}_{2}x_{i2}-\dots-\hat{\beta}_{p}x_{ip})x_{i1}=0 \\
\frac{ \partial Q }{ \partial \beta_1}\bigg|_{\beta_2 = \hat{\beta}_2}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\hat{\beta}_{2}x_{i2}-\dots-\hat{\beta}_{p}x_{ip})x_{i2}=0 \\
\dots\dots \\
\frac{ \partial Q }{ \partial \beta_1}\bigg|_{\beta_p = \hat{\beta}_p}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\hat{\beta}_{2}x_{i2}-\dots-\hat{\beta}_{p}x_{ip})x_{ip}=0
\end{cases}
$$

^c4b4e5

整理后得到正规方程(**注意！这里的$\boldsymbol{X}^{\prime}$就是$\boldsymbol{X}^T$**)：
$$
\begin{align}
\boldsymbol{X^{\prime}(y-X \hat{\beta})=0} \\
\boldsymbol{X^{\prime}X \hat{\beta}=X^{\prime}y}
\end{align}
$$
$(\boldsymbol{X^{\prime}X})^{-1}$存在时，即得到回归参数的最小二乘估计：
$$
\boldsymbol{\hat{\beta}}=(\boldsymbol{X^{\prime}X})^{-1}\boldsymbol{X^{\prime}y}
$$
那么称
$$
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_1x_{i1}+\hat{\beta}_{2}x_{i2}+\dots+\hat{\beta}_{p}x_{ip}
$$
为经验回归方程

> $(\boldsymbol{X^{\prime}X})^{-1}\Rightarrow \lvert \boldsymbol{X^{\prime}X} \rvert\ne0\Rightarrow rank(\boldsymbol{X^{\prime}X})=p+1\Rightarrow rank(X)\ge p+1\Rightarrow X_{n\times (p+1)}\Rightarrow n\ge p+1$

## 回归值和残差

### 回归值（与帽子矩阵）

$\hat{y}=\hat{\beta}_{0}+\hat{\beta}_1x_{i1}+\hat{\beta}_{2}x_{i2}+\dots+\hat{\beta}_{p}x_{ip}$为观测值$y_{i}$的回归（拟合）值；有$\boldsymbol{\hat{y}}=\boldsymbol{X \hat{\beta}}=(\hat{y_{1}},\hat{y_{2}},\cdots,\hat{y_{n}})^{\prime}$为因变量向量$\boldsymbol{y}=(y_{1},y_{2},\cdots,y_{n})^{\prime}$的回归值。由$\boldsymbol{\hat{\beta}}=\boldsymbol{X}(\boldsymbol{X^{\prime}X})^{-1}\boldsymbol{X^{\prime}y}$可得
$$
\boldsymbol{\hat{y}}=\boldsymbol{X\hat{\beta}}=\boldsymbol{X}(\boldsymbol{X^{\prime}X})^{-1}\boldsymbol{X^{\prime}y}
$$

从上式可以看出，矩阵$\boldsymbol{X}(\boldsymbol{X^{\prime}X})^{-1}\boldsymbol{X^{\prime}}$的作用是把因变量向量$\boldsymbol{y}$变为拟合值向量$\boldsymbol{\hat{y}}$，从形式上看是给$\boldsymbol{y}$带上了帽子，因此形象地称其为**帽子矩阵**，记为$\boldsymbol{H}$，于是有$\boldsymbol{\hat{y}}=\boldsymbol{Hy}$。

**帽子矩阵**：$\boldsymbol{H}=\boldsymbol{X}(\boldsymbol{X^{\prime}X})^{-1}\boldsymbol{X^{\prime}}$
	1. 是$n$阶对称矩阵
	2. 是幂等矩阵，即$\boldsymbol{H}=\boldsymbol{H}^{2}$
	3. 主对角线元素记为$h_{ii}$([见性质2](1.%20一元线性回归.md#残差的性质))，易得帽子矩阵$\boldsymbol{H}$的迹为$tr(\boldsymbol{H})=\sum\limits_{i=1}^nh_{ii}=p+1$

### 残差

1. $y_{i}$的残差：$e_{i}=y_{i}-\hat{y}_{i}$
2. （回归）残差向量：$\boldsymbol{e}=\boldsymbol{y}-\boldsymbol{\hat{y}}=(\boldsymbol{I}-\boldsymbol{H})\boldsymbol{y}$（将$\boldsymbol{\hat{y}}=\boldsymbol{Hy}$带入）
3. 残差向量的协方差阵：
	  $$
	  \begin{align}
    D(\boldsymbol{e}) & =cov(\boldsymbol{e},\boldsymbol{e}) \\
 & =cov((\boldsymbol{I}-\boldsymbol{H})\boldsymbol{y},(\boldsymbol{I}-\boldsymbol{H})\boldsymbol{y}) \\
 & =(\boldsymbol{I}-\boldsymbol{H})cov(\boldsymbol{y},\boldsymbol{y})(\boldsymbol{I}-\boldsymbol{H})^{\prime} \\
 & =\sigma^{2}(\boldsymbol{I}-\boldsymbol{H})
    \end{align}
	  $$
    
      于是有$D(e_{i})=(1-h_{ii})\sigma^{2},i=1,2,\cdots,n$

根据[该式](#^c4b4e5)可知，残差满足关系式：

$$
\begin{cases}
\sum e_{i}=0 \\
\sum e_{i}x_{i1}=0 \\
\vdots \\
\sum e_{i}x_{ip}=0
\end{cases}
$$

就是说，残差的平均值为0，残差对每个只变量的加权平均为0。上式可以用矩阵表示为$\boldsymbol{X}^{\prime}\boldsymbol{e}=\boldsymbol{0}$

$E\left( \sum\limits_{i=1}^ne_{i}^{2} \right)=\sum\limits_{i=1}^nD(e_{i})=(n-p-1)\sigma^{2}$误差项方差$\sigma^{2}$的无偏估计为

$$
\begin{align}
\hat{\sigma}^{2}&=\frac{1}{n-p-1}SSE=\frac{1}{n-p-1}(\boldsymbol{e}^{\prime}\boldsymbol{e}) \\
&=\frac{1}{n-p-1}\sum\limits_{i=1}^ne_{i}^{2}
\end{align}
$$

## 最大似然估计

由[模型式](#^a5061c)可得

$$
\boldsymbol{y}=\boldsymbol{X\beta}+\boldsymbol{\varepsilon}
$$
$$
\boldsymbol{\varepsilon}\sim N(\boldsymbol{0},\sigma^{2}\boldsymbol{I}_{n})
$$

那么$\boldsymbol{y}$的概念分布为

$$
\boldsymbol{y}\sim N(\boldsymbol{X\beta},\sigma^{2}\boldsymbol{I}_{n})
$$

可以得到似然函数

$$
L=(2\pi)^{-n/2}(\sigma^{2})^{-n/2}\exp\left( -\frac{1}{1\sigma^{2}}(\boldsymbol{y}-\boldsymbol{X\beta})^{\prime}(\boldsymbol{y}-\boldsymbol{X\beta}) \right)
$$

其中的未知参数就是$\boldsymbol{\beta}$和$\sigma^{2}$，最大似然估计就是要选似然函数$L$达到最大的$\hat{\boldsymbol{\beta}}$和$\hat{\sigma}^{2}$，要使$L$达到最大，似然函数两边需同时取对数，得到对数似然函数

$$
\ln L=-\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^{2})-\frac{1}{2\sigma^{2}}(\boldsymbol{y}-\boldsymbol{X\beta})^{\prime}(\boldsymbol{y}-\boldsymbol{X\beta})
$$

在上式中仅在最后一项中含有$\boldsymbol{\beta}$，显然使上式达到最大，等价于使下式达到最小

$$
(\boldsymbol{y}-\boldsymbol{X\beta})^{\prime}(\boldsymbol{y}-\boldsymbol{X\beta})
$$

这个时候，又与[OLSE](#最小二乘估计)一样。故在正态假定下，$\boldsymbol{\beta}$的最大似然估计与OLSE完全相同，即

$$\hat{\boldsymbol{\beta}}=(\boldsymbol{X}^{\prime}\boldsymbol{X})^(-1)\boldsymbol{X}^{\prime}\boldsymbol{y}$$

误差项方差$\sigma^{2}$的最大似然估计为

$$
\hat{\sigma}^{2}_{L}=\frac{1}{n}SSE=\frac{1}{n(\boldsymbol{e}^{\prime}\boldsymbol{e})}
$$
# 估计量的性质

性质一：$\hat{\boldsymbol{\beta}}$是随机向量$\boldsymbol{y}$的一个**线性**变换

$$
\hat{\boldsymbol{\beta}}=(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}\boldsymbol{y}
$$

性质二：$\hat{\boldsymbol{\beta}}$是$\boldsymbol{\beta}$的**无偏**估计

$$
\begin{align}
E(\hat{\boldsymbol{\beta}}) & =E((\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}\boldsymbol{y}) \\
 & =(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}E(\boldsymbol{y}) \\
 & =(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}E(\boldsymbol{X\beta}+\boldsymbol{\varepsilon}) \\
 & =(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}\boldsymbol{X\beta} \\
 & =\boldsymbol{\beta}
\end{align}
$$

性质三：　$D(\hat{\boldsymbol{\beta}})=\sigma^{2}(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}$

$$
\begin{align}
D(\hat{\beta}) & =cov(\hat{\boldsymbol{\beta}},\hat{\boldsymbol{\beta}}) \\
 & =cov((\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}\boldsymbol{y},(\boldsymbol{X}^{\prime}\boldsymbol{X})\boldsymbol{X}^{\prime}y) \\
 & =(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}cov(\boldsymbol{y},\boldsymbol{y})((\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime})^{\prime} \\
 & =(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}\boldsymbol{X}^{\prime}\sigma^{2}\boldsymbol{X}^{\prime}\boldsymbol{X}(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1} \\
 & =\sigma^{2}(\boldsymbol{X}^{\prime}\boldsymbol{X})^{-1}
\end{align}
$$
当$p=1$时即一元线性回归的情况

性质四：高斯-马尔柯夫([G-M](1.%20一元线性回归.md#^067e89))定理

假定$E(\boldsymbol{y})=\boldsymbol{X\beta},D(\boldsymbol{y})=\sigma^{2}\boldsymbol{I}_{n}$时，$\boldsymbol{\beta}$的任一线性函数$\boldsymbol{c}^{\prime}\boldsymbol{\beta}$的最小方差线性无偏估计(Best Linear Unbiased Estimator, BLUE)为$\boldsymbol{c}^{\prime}\hat{\boldsymbol{\beta}}$（$\boldsymbol{c}$是任一$p+1$维常数向量，$\hat{\boldsymbol{\beta}}$是$\boldsymbol{\beta}$的最小二乘估计）

对于该性质，需要注意：（其中前三个性质易证明，5、6性质也适用于一元线性回归）
1. 取常数向量$\boldsymbol{c}$的第$j(j=1,2,\cdots,p)$个分量为$1$，其余分量为$0$，这时G-M定理表明最小二乘估计$\hat{\beta}_{j}$是$\beta_{j}$的最小方差线性无偏估计
2. 可能存在$y_{1},y_{2},\cdots,y_{n}$的非线性函数，作为$\boldsymbol{c^{\prime}\beta}$的无偏估计，比最小二乘估计$\boldsymbol{c^{\prime}\hat{\beta}}$的方差更小
3. 可能存在$\boldsymbol{c^{\prime}\beta}$的有偏估计，在某种意义（如均方误差最小）上比最小二乘估计$\boldsymbol{c^{\prime}\hat{\beta}}$更好
4. 在正态假定下， $\boldsymbol{c^{\prime}\hat{\beta}}$是$\boldsymbol{c^{\prime}\beta}$的最小方差无偏估计。 也就是说，既不可能存在$y_{1},y_{2},\cdots,y_{n}$的非线性函数，也不可能存在$y_{1},y_{2},\cdots,y_{n}$的其它线性函数，作为$\boldsymbol{c^{\prime}\beta}$的无偏估计，比最小二乘估计$\boldsymbol{c^{\prime}\hat{\beta}}$的方差更小
5. $cov(\hat{\boldsymbol{\beta}},\boldsymbol{e})=0$（说明$\hat{\boldsymbol{\beta}}$与$\boldsymbol{e}$不相关，等价于$\hat{\boldsymbol{\beta}}$与$\boldsymbol{e}$独立，从而$\hat{\boldsymbol{\beta}}$与$SSE=\boldsymbol{e^{\prime}e}$）
6. 当$\boldsymbol{y}\sim N(\boldsymbol{X\beta},\sigma^{2}\boldsymbol{I}_{n})$时，则
	1. $\hat{\boldsymbol{\beta}}\sim N(\boldsymbol{\beta},\sigma^{2}(\boldsymbol{X^{\prime}X})^{-1})$
	2. $SSE/\sigma^{2}\sim \chi^{2}(n-p-1)$

# 回归方程的显著性检验

# 中心化和标准化

# 相关阵与偏相关系数
