# 多元线性回归模型

## 一般形式

$$
y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\cdots+\beta_{p}x_{p}+\varepsilon
$$
$$
\begin{cases}
E(\varepsilon)=0 \\
var(\varepsilon)=\sigma^{2}
\end{cases}
$$

对$n$组观测数据$(x_{i1},x_{i2},\dots,x_{ip};y_{i}),i=1,2,\dots n$，线性回归模型表示为
$$
\begin{cases}
y_{1}=\beta_{0}+\beta_{1}x_{11}+\beta_{2}x_{12}+\cdots+\beta_{p}x_{1p}+\varepsilon_{1} \\
y_{2}=\beta_{0}+\beta_{1}x_{21}+\beta_{2}x_{22}+\cdots+\beta_{p}x_{2p}+\varepsilon_{2} \\
\quad\cdots \\
y_{n}=\beta_{0}+\beta_{1}x_{n1}+\beta_{2}x_{n2}+\cdots+\beta_{p}x_{np}+\varepsilon_{n}
\end{cases}
$$
对于上式可以写成
$$
\boldsymbol{y=X\beta+\varepsilon}
$$

^a5061c

其中，
$$
\boldsymbol{y}=\begin{pmatrix}y_{1} \\ y_{2}\\ \vdots\\ y_{n}\end{pmatrix}\quad
\boldsymbol{X}=\begin{pmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1p} \\
1 & x_{21} & x_{22} & \cdots & x_{2p}  \\ 
\vdots & \vdots & \vdots & \vdots & \vdots & \\
1 & x_{n1} & x_{n2} & \cdots & x_{np}
\end{pmatrix}_{n\times(p+1)}\quad
\boldsymbol{\beta}=\begin{pmatrix}\beta_{0}\\ \beta_{1}\\ \vdots\\ \beta_{p}\end{pmatrix}\quad
\boldsymbol{\varepsilon}=\begin{pmatrix}\varepsilon_{1}\\ \varepsilon_{2}\\ \vdots\\ \varepsilon_{n}\end{pmatrix}
$$

$\boldsymbol{X}$为设计矩阵

## 基本假设

1. 解释变量$x_{1}+x_{2}+\dots+x_{p}$是确定性变量，不是随机变量，而且要求$rank(\boldsymbol{X})=p+1<n$；说明设计矩阵中的列自变量列间不相关，样本量的个数应大于解释变量的个数，$\boldsymbol{X}$为满秩矩阵。
2. 随机误差项均值为$0$且等方差，即[高斯-马尔柯夫(Gauss-Markov)](1.%20一元线性回归.md#最佳线性无偏估计)条件。
3. 正态分布的假定条件为：
    $$
    \begin{cases}
\varepsilon_{i}\sim N(0,\sigma^{2}),i=1,2,\dots,n \\
\varepsilon_{1},\varepsilon_{2},\dots,\varepsilon_{n}相互独立
\end{cases}
    $$
    对于[模型式](2.%20多元线性回归.md#^a5061c)，该条件可表示为
    $$
\boldsymbol{\varepsilon} \sim N(\boldsymbol{0},\sigma^{2}\boldsymbol{I}_{n})
$$
    此时
    $$
\boldsymbol{y}\sim N(\boldsymbol{X\beta},\sigma^{2}\boldsymbol{I}_{n})
$$

## 回归系数的解释

对含有 $p$ 个自变量的多元线性回归而言，每个回归系数 $β_i$ 表示在回归方程中其他自变量保持不变的情况下，自变量 $x_i$ 每增加一个单位时因变量 $y$ 的平均增加幅度。 因此也把多元线性回归的回归系数称为偏回归系数(partial regression coefficient)
如$y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\varepsilon,E(y)=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}$
在$x_{2}$保持不变时，有$\frac{\partial E(y)}{\partial x_{1}}=\beta_{1}$
在$x_{1}$保持不变时，有$\frac{\partial E(y)}{\partial x_{2}}=\beta_{2}$

# 回归系数的估计

## 最小二乘估计

最小二乘估计要寻找$\hat{\beta}_{0},\hat{\beta}_{1},\hat{\beta}_{2},\dots,\hat{\beta}_{p}$，使得离差平方和
$$
\begin{align}
Q(\hat{\beta}_{0},\hat{\beta}_{1},\hat{\beta}_{2},\dots\hat{\beta}_{p})&=\sum\limits_{i=1}^n(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i1}-\hat{\beta}_{2}x_{i2}-\dots-\hat{\beta}_{p}x_{ip})^{2} \\
&=\min_{\beta_{0},\beta_{1},\beta_{2},\dots,\beta_{p}}=\sum\limits_{i=1}^n(y_{i}-\beta_{0}-\beta_{1}x_{i1}-\beta_{2}x_{i2}-\dots-\beta_{p}x_{ip})
\end{align}
$$

根据微积分中求极值的原理，有
$$
\begin{cases}
\frac{ \partial Q }{ \partial \beta_0}\bigg|_{\beta_0 = \hat{\beta}_0}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\hat{\beta}_{2}x_{i2}-\dots-\hat{\beta}_{p}x_{ip})=0\\
\frac{ \partial Q }{ \partial \beta_1}\bigg|_{\beta_1 = \hat{\beta}_1}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\hat{\beta}_{2}x_{i2}-\dots-\hat{\beta}_{p}x_{ip})x_{i1}=0 \\
\frac{ \partial Q }{ \partial \beta_1}\bigg|_{\beta_2 = \hat{\beta}_2}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\hat{\beta}_{2}x_{i2}-\dots-\hat{\beta}_{p}x_{ip})x_{i2}=0 \\
\dots\dots \\
\frac{ \partial Q }{ \partial \beta_1}\bigg|_{\beta_p = \hat{\beta}_p}=-2\sum\limits_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\hat{\beta}_{2}x_{i2}-\dots-\hat{\beta}_{p}x_{ip})x_{ip}=0
\end{cases}
$$
整理后得到正规方程(**注意！这里的$\boldsymbol{X}^{\prime}$就是$\boldsymbol{X}^T$**)：
$$
\begin{align}
\boldsymbol{X^{\prime}(y-X \hat{\beta})=0} \\
\boldsymbol{X^{\prime}X \hat{\beta}=X^{\prime}y}
\end{align}
$$
$(\boldsymbol{X^{\prime}X})^{-1}$存在时，即得到回归参数的最小二乘估计：
$$
\boldsymbol{\hat{\beta}}=(\boldsymbol{X^{\prime}X})^{-1}\boldsymbol{X^{\prime}y}
$$
(这里有一个重要的矩阵：**帽子矩阵** $\boldsymbol{H}=\boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T$)
那么称
$$
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_1x_{i1}+\hat{\beta}_{2}x_{i2}+\dots+\hat{\beta}_{p}x_{ip}
$$
为经验回归方程

> $(\boldsymbol{X^{\prime}X})^{-1}\Rightarrow \lvert \boldsymbol{X^{\prime}X} \rvert\ne0\Rightarrow rank(\boldsymbol{X^{\prime}X})=p+1\Rightarrow rank(X)\ge p+1\Rightarrow X_{n\times (p+1)}\Rightarrow n\ge p+1$

## 回归值和残差

$\hat{y}=\hat{\beta}_{0}+\hat{\beta}_1x_{i1}+\hat{\beta}_{2}x_{i2}+\dots+\hat{\beta}_{p}x_{ip}$为观测值$y_{i}$的回归（拟合）值；有$\boldsymbol{\hat{y}}=\boldsymbol{X \hat{\beta}}=(\hat{y_{1}},\hat{y_{2}},\cdots,\hat{y_{n}})^{\prime}$为因变量向量$\boldsymbol{y}=(y_{1},y_{2},\cdots,y_{n})^{\prime}$的回归值。由$\boldsymbol{\hat{\beta}}=\boldsymbol{X\hat{\beta}}=\boldsymbol{X}(\boldsymbol{X^{\prime}X})^{-1}\boldsymbol{X^{-1}y}$可得
$$
\boldsymbol{\hat{y}}=\boldsymbol{X\hat{\beta}}
$$

## 最大似然估计


# 回归方程的显著性检验

# 中心化和标准化

# 相关阵与偏相关系数
